{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Underworld 2 is a python-friendly version of the Underworld code which provides a programmable and flexible front end to all the functionality of the code running in a parallel HPC environment. This gives signficant advantages to the user, with access to the power of python libraries for setup of complex problems, analysis at runtime, problem steering, and multi physics coupling. While Underworld2 embraces Jupyter Notebooks as the preferred modelling environment, only standard python is required. The Underworld2 development team is based in Melbourne, Australia at the University of Melbourne and at Monash University led by Louis Moresi. We would like to acknowledge AuScope Simulation, Analysis and Modelling for providing long term funding which has made the project possible. Additional funding for specific improvements and additional functionality has come from the Australian Research Council (http://www.arc.gov.au). The python toolkit was funded by the NeCTAR eresearch_tools program. Underworld was originally developed in collaboration with the Victorian Partnership for Advanced Computing. Lastest stable release (master branch) Development branch - Getting Underworld2 The Underworld2 docker container is the recommended method of installation for Windows, Mac OSX and Linux. You will need to install Docker on your system. The Underworld docker container is available through the docker hub: https://hub.docker.com/r/underworldcode/underworld2/ Please check our blog page for a writeup on using dockers: http://www.underworldcode.org/pages/Blog/ If you need to compile Underworld2 (in particular for HPC usage), please refer to COMPILE.md API documentation is available at http://underworld2.readthedocs.io/ Underworld Docker Usage Most new users may wish to use the Kitematic GUI to download and run Underworld. Simply search for 'underworldcode/underworld2' within Kitematic, and then click 'CREATE' to launch a container. You will eventually wish to modify your container settings (again through Kitematic) to enable local folder volume mapping, which will allow you to access your local drives within your container. For Linux users, and those who prefer the command line, the following minimal command should be sufficient to access the Underworld2 Jupyter Notebook examples: docker run -p 8888 :8888 underworldcode/underworld2 Privacy Note that basic usage metrics are dispatched when you use Underworld. To opt out, set the UW_NO_USAGE_METRICS environment variable. See PRIVACY.md for full details. Bedtime reading Moresi, L., Dufour, F., and Muhlhaus, H.B., 2002, Mantle convection modeling with viscoelastic/brittle lithosphere: Numerical methodology and plate tectonic modeling: Pure And Applied Geophysics, v. 159, no. 10, p. 2335\u20132356, doi: 10.1007/s00024-002-8738-3. Moresi, L., Dufour, F., and Muhlhaus, H.B., 2003, A Lagrangian integration point finite element method for large deformation modeling of viscoelastic geomaterials: Journal of Computational Physics, v. 184, no. 2, p. 476\u2013497. Moresi, L., Quenette, S., Lemiale, V., M\u00e9riaux, C., Appelbe, W., M\u00fchlhaus, 2007, Computational approaches to studying non-linear dynamics of the crust and mantle: Phys. Earth Planet. Inter, v. 163, p. 69\u201382, doi: 10.1016/j.pepi.2007.06.009.","title":"Introduction"},{"location":"index.html#getting-underworld2","text":"The Underworld2 docker container is the recommended method of installation for Windows, Mac OSX and Linux. You will need to install Docker on your system. The Underworld docker container is available through the docker hub: https://hub.docker.com/r/underworldcode/underworld2/ Please check our blog page for a writeup on using dockers: http://www.underworldcode.org/pages/Blog/ If you need to compile Underworld2 (in particular for HPC usage), please refer to COMPILE.md API documentation is available at http://underworld2.readthedocs.io/","title":"Getting Underworld2"},{"location":"index.html#underworld-docker-usage","text":"Most new users may wish to use the Kitematic GUI to download and run Underworld. Simply search for 'underworldcode/underworld2' within Kitematic, and then click 'CREATE' to launch a container. You will eventually wish to modify your container settings (again through Kitematic) to enable local folder volume mapping, which will allow you to access your local drives within your container. For Linux users, and those who prefer the command line, the following minimal command should be sufficient to access the Underworld2 Jupyter Notebook examples: docker run -p 8888 :8888 underworldcode/underworld2","title":"Underworld Docker Usage"},{"location":"index.html#privacy","text":"Note that basic usage metrics are dispatched when you use Underworld. To opt out, set the UW_NO_USAGE_METRICS environment variable. See PRIVACY.md for full details.","title":"Privacy"},{"location":"index.html#bedtime-reading","text":"Moresi, L., Dufour, F., and Muhlhaus, H.B., 2002, Mantle convection modeling with viscoelastic/brittle lithosphere: Numerical methodology and plate tectonic modeling: Pure And Applied Geophysics, v. 159, no. 10, p. 2335\u20132356, doi: 10.1007/s00024-002-8738-3. Moresi, L., Dufour, F., and Muhlhaus, H.B., 2003, A Lagrangian integration point finite element method for large deformation modeling of viscoelastic geomaterials: Journal of Computational Physics, v. 184, no. 2, p. 476\u2013497. Moresi, L., Quenette, S., Lemiale, V., M\u00e9riaux, C., Appelbe, W., M\u00fchlhaus, 2007, Computational approaches to studying non-linear dynamics of the crust and mantle: Phys. Earth Planet. Inter, v. 163, p. 69\u201382, doi: 10.1016/j.pepi.2007.06.009.","title":"Bedtime reading"},{"location":"Instructions.html","text":"This is the non-trivial part of the instruction manual ... using this template to make your content available to other people Getting started Ultimately you probably want to fork this project , build a new docker image with your own software stack, and replace my content with something more relevant. The docker website has many tutorials on learning how to become fluent with images / containers and you will have to visit there to grab a copy of the docker binaries. As a first step, though, you can use the docker VOLUMEs to over-ride my content and also to capture the results of rebuilding the site and editing notebooks. When the container is running (in kitematic) it looks like this: Clicking on the preview brings up the web content in the default browser (by default this content !). The volumes within the container that can be used to exchange data are listed below the preview. You can connect both these to local directories in the settings and use the running container to test / build / debug your content. I will leave you to figure out how to do that from the Kitematic documentation ! You can start by pointing the /demonstration/Content VOLUME at a local copy of the original content grabbed from github. Background We use the jupyter notebook system as the webserver. This is necessary because we want to mix live content with static content but docker means we don't really know our IP address or port - we have to make links entirely within the one server. This is not a big deal but does constrain how we structure the website and links. The site is built using mkdocs which combines a pile of markdown files plus stylesheets and produces an elegant, entirely static website. The plus side is that you need only think about markdown and not stylesheets but, of course there is a layer of additional logic and more to learn. My Content ! Jekyll will take any markdown files it finds in the /demonstration/Content directory and render them into html files in the /demonstration/_site/Content directory. There must be some metadata provided at the top of these files (as in the examples). There is more data in the _config.yml file which is used to set paths, choose links for the navigation buttons, choose a logo image etc etc. Learn from the examples and don't change anything until it is obvious what it does ! Any file or directory that starts with a _ is either special or ignored by jekyll. Once you figure out how to change those files, you won't need to use this project at all ! There must be an index.md file which will be the landing page for the website and the target of the Home button. There can be any number of other pages. If you want to make links to those pages from the notebooks or from the jekyll markdown files, then you need to make those links point to the html version of the file. For example, if you create a file /demonstration/Content/MyFiles/test.md then you can write a link to this in markdown as [my test file](/files/Content/MyFiles/test.html) . The jupyter notebook server makes a distinction between files which are linked as /files/Content which are rendered directly by the webserver, and files which are linked /notebooks/Content which are rendered by the live notebook system. If the file is actually a notebook, then it is obvious why you should link /notebooks/file.ipynb but if it is an html file, for example, then files/file.html will show you a preview whereas /notebooks/file.html will open the source in an editor. How to build it The repository contains all the scripts and style information to build a website from the content which it finds in the Content directory (actually /demonstration/Content ). All the directories with names that start with _ are part of that magic. To build the website run this command in the container's command line (in Kitematic, the execute button will start a shell for you) You will need to view that site using the jupyter notebooks rather than a standard server or directly from the filesystem as the paths are only valid within that environment. Where to put it All of the material that creates the web pages and notebooks should live in the Content directory. Paths are mapped from the Content directory to the _site directory in an obvious way (this can be subverted if you add permalink metadata to a file) which means that links can be written in a predictable manner. Notebooks live in Content/Notebooks so that, by default, the navigation bar knows where to find them. The Dockerfile The unix environment you are working in depends on the setup of your Docker machine. This is determined from the Dockerfile in the home directory. The FROM command at the start of this file is the base image that is downloaded as the starting point. This contains all the dependencies you need to run live demonstrations (including the notebooks server, the jekyll system and so on). The Dockerfile in this repository builds upon the python installation and adds the web-building tools and the content. You will need to customise this !","title":"Instructions - how to add your own content"},{"location":"Instructions.html#getting-started","text":"Ultimately you probably want to fork this project , build a new docker image with your own software stack, and replace my content with something more relevant. The docker website has many tutorials on learning how to become fluent with images / containers and you will have to visit there to grab a copy of the docker binaries. As a first step, though, you can use the docker VOLUMEs to over-ride my content and also to capture the results of rebuilding the site and editing notebooks. When the container is running (in kitematic) it looks like this: Clicking on the preview brings up the web content in the default browser (by default this content !). The volumes within the container that can be used to exchange data are listed below the preview. You can connect both these to local directories in the settings and use the running container to test / build / debug your content. I will leave you to figure out how to do that from the Kitematic documentation ! You can start by pointing the /demonstration/Content VOLUME at a local copy of the original content grabbed from github.","title":"Getting started"},{"location":"Instructions.html#background","text":"We use the jupyter notebook system as the webserver. This is necessary because we want to mix live content with static content but docker means we don't really know our IP address or port - we have to make links entirely within the one server. This is not a big deal but does constrain how we structure the website and links. The site is built using mkdocs which combines a pile of markdown files plus stylesheets and produces an elegant, entirely static website. The plus side is that you need only think about markdown and not stylesheets but, of course there is a layer of additional logic and more to learn.","title":"Background"},{"location":"Instructions.html#my-content","text":"Jekyll will take any markdown files it finds in the /demonstration/Content directory and render them into html files in the /demonstration/_site/Content directory. There must be some metadata provided at the top of these files (as in the examples). There is more data in the _config.yml file which is used to set paths, choose links for the navigation buttons, choose a logo image etc etc. Learn from the examples and don't change anything until it is obvious what it does ! Any file or directory that starts with a _ is either special or ignored by jekyll. Once you figure out how to change those files, you won't need to use this project at all ! There must be an index.md file which will be the landing page for the website and the target of the Home button. There can be any number of other pages. If you want to make links to those pages from the notebooks or from the jekyll markdown files, then you need to make those links point to the html version of the file. For example, if you create a file /demonstration/Content/MyFiles/test.md then you can write a link to this in markdown as [my test file](/files/Content/MyFiles/test.html) . The jupyter notebook server makes a distinction between files which are linked as /files/Content which are rendered directly by the webserver, and files which are linked /notebooks/Content which are rendered by the live notebook system. If the file is actually a notebook, then it is obvious why you should link /notebooks/file.ipynb but if it is an html file, for example, then files/file.html will show you a preview whereas /notebooks/file.html will open the source in an editor.","title":"My Content !"},{"location":"Instructions.html#how-to-build-it","text":"The repository contains all the scripts and style information to build a website from the content which it finds in the Content directory (actually /demonstration/Content ). All the directories with names that start with _ are part of that magic. To build the website run this command in the container's command line (in Kitematic, the execute button will start a shell for you) You will need to view that site using the jupyter notebooks rather than a standard server or directly from the filesystem as the paths are only valid within that environment.","title":"How to build it"},{"location":"Instructions.html#where-to-put-it","text":"All of the material that creates the web pages and notebooks should live in the Content directory. Paths are mapped from the Content directory to the _site directory in an obvious way (this can be subverted if you add permalink metadata to a file) which means that links can be written in a predictable manner. Notebooks live in Content/Notebooks so that, by default, the navigation bar knows where to find them.","title":"Where to put it"},{"location":"Instructions.html#the-dockerfile","text":"The unix environment you are working in depends on the setup of your Docker machine. This is determined from the Dockerfile in the home directory. The FROM command at the start of this file is the base image that is downloaded as the starting point. This contains all the dependencies you need to run live demonstrations (including the notebooks server, the jekyll system and so on). The Dockerfile in this repository builds upon the python installation and adds the web-building tools and the content. You will need to customise this !","title":"The Dockerfile"},{"location":"Map.html","text":"Not everything interesting is found in the menu bar ! Pages Home - (also accessible through the Jupyter logo in notebooks) Index - this site map ! Format - describes the markdown dialect, maths, code highlights and links Notebooks Start - a sample notebook explaining the format iPython Maps Example - an example that actually does something Other Jupyter - the file browser / editor Terminal - a terminal running on the backend machine Source Code File browser (and editor !) Licence External Github Source Louis Moresi's Home Page Louis Moresi's email","title":"Site Index"},{"location":"Map.html#pages","text":"Home - (also accessible through the Jupyter logo in notebooks) Index - this site map ! Format - describes the markdown dialect, maths, code highlights and links","title":"Pages"},{"location":"Map.html#notebooks","text":"Start - a sample notebook explaining the format iPython Maps Example - an example that actually does something","title":"Notebooks"},{"location":"Map.html#other","text":"Jupyter - the file browser / editor Terminal - a terminal running on the backend machine","title":"Other"},{"location":"Map.html#source-code","text":"File browser (and editor !) Licence","title":"Source Code"},{"location":"Map.html#external","text":"Github Source Louis Moresi's Home Page Louis Moresi's email","title":"External"},{"location":"Geodynamics/Introduction.html","text":"Computational Geodynamics Notes Chapter 1 - Introduction Chapter 2 - Mathematics Chapter 3 - Convection Chapter 4 - Other Solutions","title":"Introduction"},{"location":"Geodynamics/Introduction.html#computational-geodynamics-notes","text":"Chapter 1 - Introduction Chapter 2 - Mathematics Chapter 3 - Convection Chapter 4 - Other Solutions","title":"Computational Geodynamics Notes"},{"location":"Geodynamics/TheoreticalBackground/index.html","text":"Table of Contents Chapter 1 - Introduction Chapter 2 - Mathematics Chapter 3 - Convection Chapter 4 - Other Solutions","title":"Table of Contents"},{"location":"Geodynamics/TheoreticalBackground/index.html#table-of-contents","text":"Chapter 1 - Introduction Chapter 2 - Mathematics Chapter 3 - Convection Chapter 4 - Other Solutions","title":"Table of Contents"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-1.html","text":"Computational Geodynamics: Introduction Athanasius Kircher, Mundus subterraneus (1664/65): Systema Ideale PYROPHYLACIORUM Subterraneorum, quorum montes Vulcanii, veluti spiracula quaedam existant Introduction Our goal is to develop an quantitative understanding of the dynamic processes within the Earth and sister planets as I have sketched in the Figure to the right. These dynamic processes are largely driven by the internal heat of the planet escaping to the surface through whatever mechanisms are available. Some of the heat is left over from the original formation of the planet, and the rest originates in the decay of radioactive elements. In the Earth's early history and elsewhere in the solar system, tidal heating, chemical segregation, and impacts have all played a role in supplying the interior heat budget. The figure below is a schematic of the Earth's interior and something similar for Venus and Mars would be, on the face of it, much simpler. A schematic of the interior of the Earth showing some of the global scale processes we seek to understand quantitatively in this course This is because the dynamics of the Earth is completely dominated by Plate Tectonics - a unique manifestation of interior heat loss as far as we are aware. Part of our task is to understand why plate tectonics is a possible outcome of a hot planet, but also why it is not the only possible outcome. If we can also understand how the different modes are selected for planets of different size, composition, and heat budget, then we have a powerful way to predict the geological behaviour of extrasolar planets. Plate tectonics creates a number of very efficient cycling mechanisms which link the interior of the Earth and the Atmosphere and Oceans; it may prove to be an essential ingredient for the kind of friendly world we expect to be needed to nurture (intelligent) life. Modeling Global scale geodynamics is a discipline where we cannot do controlled experiments on the basic processes we are studying. We rely on observing the Earth and the other terrestrial planets and moons and looking for multiple manifestations of the same processes under different conditions to give us control on certain parameters. While it is not possible to do experiments at the planetary scale over geological time, it is possible to perform experiments at a physically manageable size and, by careful scaling, to generalise the results to geologically relevant space and time-scales. If these processes of interest can be understood through a mathematical description, then the equations are automatically applicable at geological time and space scales provided the assumptions which go into developing the mathematical model are still valid. We will frequently be talking about \"modeling\" - people mean many different things by this and all of the following fall under the general concept of modeling: - Laboratory based physical models which can be scaled to give meaningful, quantitative insight into deformation at geological scales. - The building of mathematical descriptions of the world and their use to approximate physical \"reality\". - Computational solution of these descriptions (where needed) and the concept of a numerical model (we deal with the extensively later) - How to go about constructing a model - How to go about using a model (these are quite different things !). - Exploring parameter variation to understand the dominant effects. Mathematical background These notes contains material at different levels. There is broadly descriptive content which is intended to introduce the subject and lead up to the more advanced mathematical content. It should be possible to follow these notes without detailed knowledge of how the mathematical results are obtained, but it is expected that you can use the results in exercises for to solve real problems in geophysics. Familiarity with vectors and tensors notation together with the index notation is assumed for understanding the equations and I interchange the different notations where the result is clearer or more memorable. On the assumption that these things are disconcerting (at best) the first few times, I will write everything out in full -- at least for the Cartesian case. In other geometries, vector notation generally still holds, but the definition of the operators can be very different, and it is always worth checking before using them. Reading Material G. F. Davies. Dynamic Earth. Cambridge University Press, New York, 1999. J. Grotzinger, T. H. Jordan, F. Press, and R. Siever. Understanding Earth. W. H. Freeman & Co, 5 edition, 2006. link . L. D. Landau and E. M. Lifshitz. Fluid Mechanics, volume 6 of Course of Theoretical Physics. Pergamon Press, 1959. G. Schubert, D. L. Turcotte, and P. Olson. Mantle Convection in Earth and Planets. Cambridge University Press, UK, 2001. D.L. Turcotte and G. Schubert. Geodynamics. John Wiley and Sons, New York, 1982.","title":"Computational Geodynamics: Introduction"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-1.html#computational-geodynamics-introduction","text":"Athanasius Kircher, Mundus subterraneus (1664/65): Systema Ideale PYROPHYLACIORUM Subterraneorum, quorum montes Vulcanii, veluti spiracula quaedam existant","title":"Computational Geodynamics: Introduction"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-1.html#introduction","text":"Our goal is to develop an quantitative understanding of the dynamic processes within the Earth and sister planets as I have sketched in the Figure to the right. These dynamic processes are largely driven by the internal heat of the planet escaping to the surface through whatever mechanisms are available. Some of the heat is left over from the original formation of the planet, and the rest originates in the decay of radioactive elements. In the Earth's early history and elsewhere in the solar system, tidal heating, chemical segregation, and impacts have all played a role in supplying the interior heat budget. The figure below is a schematic of the Earth's interior and something similar for Venus and Mars would be, on the face of it, much simpler. A schematic of the interior of the Earth showing some of the global scale processes we seek to understand quantitatively in this course This is because the dynamics of the Earth is completely dominated by Plate Tectonics - a unique manifestation of interior heat loss as far as we are aware. Part of our task is to understand why plate tectonics is a possible outcome of a hot planet, but also why it is not the only possible outcome. If we can also understand how the different modes are selected for planets of different size, composition, and heat budget, then we have a powerful way to predict the geological behaviour of extrasolar planets. Plate tectonics creates a number of very efficient cycling mechanisms which link the interior of the Earth and the Atmosphere and Oceans; it may prove to be an essential ingredient for the kind of friendly world we expect to be needed to nurture (intelligent) life.","title":"Introduction"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-1.html#modeling","text":"Global scale geodynamics is a discipline where we cannot do controlled experiments on the basic processes we are studying. We rely on observing the Earth and the other terrestrial planets and moons and looking for multiple manifestations of the same processes under different conditions to give us control on certain parameters. While it is not possible to do experiments at the planetary scale over geological time, it is possible to perform experiments at a physically manageable size and, by careful scaling, to generalise the results to geologically relevant space and time-scales. If these processes of interest can be understood through a mathematical description, then the equations are automatically applicable at geological time and space scales provided the assumptions which go into developing the mathematical model are still valid. We will frequently be talking about \"modeling\" - people mean many different things by this and all of the following fall under the general concept of modeling: - Laboratory based physical models which can be scaled to give meaningful, quantitative insight into deformation at geological scales. - The building of mathematical descriptions of the world and their use to approximate physical \"reality\". - Computational solution of these descriptions (where needed) and the concept of a numerical model (we deal with the extensively later) - How to go about constructing a model - How to go about using a model (these are quite different things !). - Exploring parameter variation to understand the dominant effects.","title":"Modeling"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-1.html#mathematical-background","text":"These notes contains material at different levels. There is broadly descriptive content which is intended to introduce the subject and lead up to the more advanced mathematical content. It should be possible to follow these notes without detailed knowledge of how the mathematical results are obtained, but it is expected that you can use the results in exercises for to solve real problems in geophysics. Familiarity with vectors and tensors notation together with the index notation is assumed for understanding the equations and I interchange the different notations where the result is clearer or more memorable. On the assumption that these things are disconcerting (at best) the first few times, I will write everything out in full -- at least for the Cartesian case. In other geometries, vector notation generally still holds, but the definition of the operators can be very different, and it is always worth checking before using them.","title":"Mathematical background"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-1.html#reading-material","text":"G. F. Davies. Dynamic Earth. Cambridge University Press, New York, 1999. J. Grotzinger, T. H. Jordan, F. Press, and R. Siever. Understanding Earth. W. H. Freeman & Co, 5 edition, 2006. link . L. D. Landau and E. M. Lifshitz. Fluid Mechanics, volume 6 of Course of Theoretical Physics. Pergamon Press, 1959. G. Schubert, D. L. Turcotte, and P. Olson. Mantle Convection in Earth and Planets. Cambridge University Press, UK, 2001. D.L. Turcotte and G. Schubert. Geodynamics. John Wiley and Sons, New York, 1982.","title":"Reading Material"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html","text":"Conservation Laws \\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\] Let us start by deriving the equations of motion, energy balance and so on through a conservation principle. This will give a useful insight into the different forms of the equations which we will later encounter. A general conservation law does not distinguish the quantity which is being conserved -- it is a mathematical identity. Consider a quantity \\( \\phi \\) / unit mass which is carried around by a fluid. We can draw an arbitrary volume, \\( \\Omega \\) to contain some amount of this fluid at a given time. We label the surface of the volume \\( \\Omega \\) as \\( \\Gamma \\), and define an outward surface normal vector \\( \\dGamma \\) which is normal to the tangent plane of an infinitesimal element of the surface and has a magnitude equal to the area of this element. {: width=\"50%\"} Arbitrary fluid volume within which the fluid properties are to be conserved We also define a source/sink term, \\(H\\) / unit mass which generates/consumes the quantity \\( \\phi \\), and a flux term, \\(\\mathbf{F}\\) which occurs across the surface when the fluid is stationary (e.g. this might represent diffusion of \\( \\phi \\)). The rate of change of \\( \\phi \\) is given by combining the contribution due to the source term, the stationary flux term, and the effect of motion of the fluid. \\begin{equation} \\frac{d}{dt} \\int _ {\\Omega} \\rho \\phi d\\Omega = - \\int _ {\\Gamma} \\mathbf{F} \\cdot \\dGamma + \\int _ {\\Omega} \\rho H d\\Omega - \\int _ {\\Gamma} \\rho \\phi \\mathbf{v} \\cdot \\dGamma \\label{eq:1} \\end{equation} where the final term is the change due to the fluid carrying \\( \\phi \\) through the volume. Fluxes are positive outward (by our definition of \\( \\dGamma \\)), so a positive flux reduces \\( \\phi \\) within \\( d\\Omega \\) and negative signs are needed for these terms. We can generally use Gauss' theorem to write surface integrals as volume integrals: \\begin{equation} \\nonumber \\int _ {\\Gamma} \\phi \\mathbf{u} \\cdot \\dGamma = \\int _ {\\Omega} \\nabla \\cdot (\\phi \\mathbf{u}) d\\Omega \\end{equation} The test surface, \\( \\Gamma \\), and volume, \\( \\Omega \\) are assumed to be_fixed in the lab reference frame_ so that the order of integration and differentiation can be interchanged \\begin{equation} \\nonumber \\frac{d}{dt} \\int_\\Omega \\rho \\phi d\\Omega = \\int _ \\Omega \\frac{\\partial \\rho \\phi}{\\partial t} d \\Omega \\end{equation} Allowing us to write \\begin{equation} \\nonumber \\int_{\\Gamma} \\mathbf{F} \\cdot \\dGamma + \\int _ {\\Gamma} \\rho \\phi \\mathbf{v} \\cdot \\dGamma = \\int _ {\\Omega} \\nabla \\cdot (\\mathbf{F} + \\rho \\phi \\mathbf{v}) d\\Omega \\end{equation} so that we rewrite the general conservation equation as \\begin{equation} \\int_{\\Omega} \\left[ \\frac{d \\rho \\phi}{dt} + \\nabla \\cdot (\\mathbf{F} + \\rho \\phi \\mathbf{v}) -\\rho H \\right] d\\Omega=0 \\label{eq:cons1}) \\end{equation} We can now appeal to the fact that this conservation law holds regardless of our particular choice of volume \\( \\Omega \\) and, therefore, the integral above can only be zero for arbitrary \\( \\Omega \\) if the enclosed term is zero everywhere, i.e. \\begin{equation} \\frac{d \\rho \\phi}{dt} + \\nabla \\cdot (\\mathbf{F} + \\rho \\phi \\mathbf{v}) -\\rho H =0 \\label{eq:cons2} \\end{equation} This is a general conservation rule for any property \\( \\phi \\) of a moving fluid. We now consider various quantities that may be transported and develop specific conservation laws. Conservation of mass In this case\\( \\phi = 1 \\) (since \\(\\int_\\Omega \\rho d\\Omega \\rightarrow \\mbox{mass}\\)), \\(\\mathbf{F} = H = 0\\). Thus, equation (\\ref{eq:cons2}) gives \\begin{equation} \\Red{\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot \\rho \\mathbf{v} = 0} \\label{eq:masscons} \\end{equation} Conservation of (heat) energy The thermal energy / unit mass is \\(C_p T\\) and the conductive heat flux \\(\\mathbf{F}\\) is given by \\( \\mathbf{F} = -k \\nabla T\\), where \\(k\\) is the thermal conductivity. Then the general conservation law of \\( (\\ref{eq:cons2}) \\) reduces to \\begin{equation} \\nonumber \\frac{\\partial (\\rho C_p T)}{\\partial t} + \\nabla \\cdot \\left(-k \\nabla T + \\rho C_p T \\mathbf{v}\\right) - \\rho H = 0 \\end{equation} Rearranging with some foresight gives the following: \\begin{equation} \\nonumber \\frac{C_p T}{\\rho} \\left[ \\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot \\rho \\mathbf{v} \\right] + \\frac{\\partial C_p T}{\\partial t} + \\mathbf{v} \\cdot \\nabla C_p T = \\frac{1}{\\rho} \\nabla \\cdot k \\nabla T + H \\end{equation} Where the term in square brackets is simply the statement of mass conservation which vanishes identically. The constant density assumption is just to simplify the discussion at this point and will have to be revisited later in the context of thermal convection where density changes drive the flow. If the heat capacity, \\(C_p\\) and thermal conductivity, \\(k \\) are constants, then the conservation equation becomes \\begin{equation} \\nonumber \\Red{ \\left( \\frac{\\partial T}{\\partial t} + \\mathbf{v} \\cdot \\nabla T \\right)= \\kappa \\nabla ^ 2 T + \\frac{H}{C _ p} } \\label{eq:energycons} \\end{equation} where \\(\\kappa\\) is the thermal diffusivity, \\(\\kappa = k/\\rho C_p\\). This equation is linear provided that the velocity field is specified and is independent of \\(T\\). Clearly this is generally not true because mantle circulation is driven by thermal buoyancy. A new bit of notation has been defined in the process of this derivation. The meaning of the \\(\\mathbf{v} \\cdot \\nabla\\) operator is \\begin{equation} \\nonumber \\mathbf{v} \\cdot \\nabla \\equiv v _ j \\frac{\\partial}{\\partial x _ j} \\end{equation} For later reference, this is how it looks: \\begin{equation} \\nonumber (\\mathbf{v} \\cdot \\nabla) T = v _ 1 \\frac{\\partial T}{\\partial x _ 1} + v _ 2 \\frac{\\partial T}{\\partial x _ 2} + v _ 3 \\frac{\\partial T}{\\partial x _ 3} \\end{equation} The Laplacian, \\(\\nabla ^ 2\\), is this expression (for scalar \\(T\\)) \\begin{equation} \\nonumber \\nabla ^ 2 T \\equiv \\frac{\\partial ^ 2 T}{\\partial x _ 1 ^ 2} + \\frac{\\partial ^ 2 T}{\\partial x _ 2 ^ 2} + \\frac{\\partial ^ 2 T}{\\partial x _ 3 ^ 2} \\end{equation} Conservation of momentum Momentum is a vector quantity, so the form of (\\ref{eq:cons1}) is slightly different. The source term in a momentum equation represents a force, and the surface flux term represents a stress. \\begin{equation} \\frac{d}{dt}\\int _ {\\Omega} \\rho \\mathbf{v} d\\Omega = - \\int _ {\\Omega} \\rho g \\hat{\\mathbf{z}} d\\Omega + \\int _ {\\Gamma} \\boldsymbol{\\sigma} \\cdot \\dGamma - \\int _ {\\Gamma} \\rho \\mathbf{v} (\\mathbf{v} \\cdot \\dGamma) \\label{eq:momcons1} \\end{equation} Gravity acts as a body force in the vertical direction, \\(\\hat{\\mathbf{z}}\\). We have introduced the stress tensor, \\(\\boldsymbol{\\sigma}\\); the force / unit area exerted on an arbitrarily oriented surface with normal \\(\\hat{\\mathbf {n}}\\) is \\begin{equation} \\nonumber f _ i = \\sigma _ {ij} n _ j \\end{equation} The application of Gauss' theorem, and using the arbitrary nature of the chosen volume to require the integrand to be zero, as before, gives \\begin{equation} \\frac{\\partial}{\\partial t}(\\rho \\mathbf{v}) +\\rho g \\hat{\\mathbf{z}} - \\nabla \\cdot \\sigma + \\nabla \\cdot (\\rho \\mathbf{v} \\mathbf{v}) = 0 \\label{eq:momcons2} \\end{equation} Vector notation allows us to keep a number of equations written as one single equation. However, at this point, keeping the equations in vector notation makes the situation more confusing. Particularly, the last term of equation \\( (\\ref{eq:momcons2}) \\) stretches the current notation rather too much. Instead, we consider the individual components of momentum, each of which must satisfy the conservation law independently In index notation, equation (\\ref{eq:momcons2}) is written as \\begin{equation} \\nonumber \\Green{\\frac{\\partial \\rho v _ i}{\\partial t}} + \\rho g \\delta _ {i3} - \\frac{\\partial \\sigma _ {ij}}{\\partial x _ j} + \\Blue{\\frac{\\partial(\\rho v _ i v _ j)}{\\partial x _ j}} = 0 \\label{eq:momindx} \\end{equation} where the troublesome final term is now unambiguous. Repeated indeces in each term are implicitly to be summed. \\( \\delta _ {ij}\\) is the kronecker delta which obeys: \\begin{equation} \\nonumber \\delta _ {ij} = \\begin{cases} 0 & \\text{if i \\neq j i \\neq j }, \\\\ 1 & \\text{if i = j i = j }. \\end{cases} \\end{equation} With some foresight, we expand the derivatives of products of two terms and gather up some of the resulting terms: \\begin{equation} \\nonumber v _ i \\left[\\Green{\\frac{\\partial \\rho}{\\partial t}} + \\Blue{ \\frac{\\partial \\rho v _ j}{\\partial x _ j}} \\right] + \\Green{\\rho \\frac{\\partial v _ i}{\\partial t}} + \\Blue{\\rho v _ j \\frac{\\partial v _ i}{\\partial x _ j}} = \\frac{\\partial \\sigma _ {ij}}{\\partial x _ j} -\\rho g \\delta _ {i3} \\end{equation} The term in square brackets is, in fact, a restatement of the conservation of mass derived above and must vanish. The remaining terms can now be written out in vector notation as \\begin{equation} \\nonumber \\Red{ \\rho \\left( \\frac{\\partial \\mathbf{v}}{\\partial t} + (\\mathbf{v} \\cdot \\nabla) \\mathbf{v} \\right) = \\nabla \\cdot \\boldsymbol{\\sigma} - g\\rho\\hat{\\mathbf{z}} } \\label{eq:momcons3} \\end{equation} Note: The \\(\\mathbf{v} \\cdot \\nabla \\) notation we introduced earlier is now an operator on a vector. In this context, the \\(\\mathbf{v} \\cdot \\nabla \\) operator acts on each component of the vector independently. Written out it looks like this: \\begin{equation} \\nonumber \\begin{split} (\\mathbf{v} \\cdot \\nabla) \\mathbf{u} = & \\hat{\\boldsymbol{\\imath}} \\left( v _ 1 \\frac{\\partial u _ 1}{\\partial x _ 1} + v_2 \\frac{\\partial u _ 1}{\\partial x _ 2} + v_3 \\frac{\\partial u _ 1}{\\partial x _ 3} \\right) \\\\ & \\hat{\\boldsymbol{\\jmath}} \\left( v_1 \\frac{\\partial u_2}{\\partial x_1} + v_2 \\frac{\\partial u _ 2}{\\partial x _ 2} + v _ 3 \\frac{\\partial u _ 2}{\\partial x _ 3} \\right) \\\\ & \\hat{\\boldsymbol{k}} \\left( v_1 \\frac{\\partial u_3}{\\partial x_1} + v_2 \\frac{\\partial u _ 3}{\\partial x _ 2} + v _ 3 \\frac{\\partial u _ 3}{\\partial x _ 3} \\right) \\\\ \\end{split} \\end{equation} Constitutive Laws The formulation above is quite general, and can be extended where necessary to include conservation laws for additional physical variables (for example, angular momentum, electric current). Specific to the type of material which is deforming is the constitutive law which describes the stress. In the case of an incompressible fluid, the stress is related to strain rate through a viscosity, \\(\\eta\\), and to the pressure, \\(p\\). Incompressibility is expressed as \\begin{equation} \\nonumber \\nabla \\cdot \\mathbf{u} = 0 \\end{equation} This is a tighter constraint than mass conservation and emerges from equation (\\ref{eq:masscons}) when \\( \\rho \\) is assumed to be constant. With this assumption, the constitutive law is written (A derivation for this is in Landau & Lifschitz). \\begin{equation} \\nonumber \\sigma _ {ij} = \\eta \\left( \\frac{\\partial v _ i}{\\partial x _ j} + \\frac{\\partial v _ j}{\\partial x _ i}\\right) - p\\delta _ {ij} \\end{equation} If the viscosity is constant, then we can substitute the constitutive law into the stress-divergence term of the momentum conservation equation. In index notation once again, \\begin{equation} \\nonumber \\begin{split} \\nabla \\cdot \\boldsymbol{\\sigma} & = \\frac{\\partial}{\\partial x _ j} \\eta \\left( \\frac{\\partial v _ i}{\\partial x _ j} + \\frac{\\partial v _ j}{\\partial x _ i} \\right) - \\frac{\\partial p}{\\partial x _ j} \\delta _ {ij} \\ & = \\eta \\frac{\\partial^2 v_i}{\\partial x _ j \\partial x _ j} + \\eta \\frac{\\partial^2 v _ j}{\\partial x _ i \\partial x _ j} - \\frac{\\partial p}{\\partial x _ i}\\ & = \\eta \\nabla^2 \\mathbf{v} + \\eta \\Green{\\nabla (\\nabla \\cdot \\mathbf{v})} - \\nabla p \\end{split} \\end{equation} the second term in this final form must vanish because of the incompressibility assumption, so the momentum conservation equation becomes \\begin{equation} \\Red{ \\rho \\left( \\frac{\\partial \\mathbf{v}}{\\partial t} + (\\mathbf{v} \\cdot \\nabla) \\mathbf{v} \\right) = \\eta \\nabla^2 \\mathbf{v} - \\nabla p - g\\rho\\hat{\\mathbf{z}} } \\label{eq:navstokes} \\end{equation} This is the Navier-Stokes equation. Once again some new notation has shown up uninvited. The Laplacian operator \\(\\nabla ^ 2\\) is defined (in a scalar context) as \\begin{equation} \\nonumber \\begin{split} \\nabla ^ 2 \\phi & = \\nabla \\cdot \\nabla \\phi \\\\ & = \\frac{\\partial ^ 2 \\phi}{\\partial x ^ 2} + \\frac{\\partial ^ 2 \\phi}{\\partial y ^ 2} + \\frac{\\partial ^ 2 \\phi}{\\partial z ^ 2} \\;\\;\\; \\textrm{(Cartesian)} \\end{split} \\end{equation} and in a vector context as \\begin{equation} \\nonumber \\begin{split} \\nabla^2 \\mathbf{u} & = \\nabla \\nabla \\cdot \\mathbf{u} - \\nabla \\times (\\nabla \\times \\mathbf{u}) \\ & = \\mathbf{i} \\nabla \\cdot \\nabla u_x + \\mathbf{j} \\nabla \\cdot \\nabla u_y + \\mathbf{k} \\nabla \\cdot \\nabla u_z \\;\\;\\; \\text{(Cartesian)} \\end{split} \\end{equation} In Cartesian coordinates, the Laplacian operator has a simple form, and the vector Laplacian is simply the scalar operator applied in each direction. In other coordinate systems this operator becomes substantially more elaborate. Boussinesq Approximation, Equation of State, Density Variations The equation which relates pressure, temperature and density is known as the equation of state. For the equations derived so far, we have specified an incompressible fluid, for which no density variations are possible. However, we have also included a source term for momentum which relies on gravity acting on density variations . This conflict is typical of fluid mechanics: simplifying assumptions if taken to their logical limit imply no motion or some other trivial solution to the equations. In this case, we make the assumption that density changes are typically small relative to the overall magnitude of the density itself. Terms which are scaled by density can therefore assume that it is a large constant value. Terms which contain gradients of density or density variations should consider the equation of state. This is the Boussinesq approximation and is only a suitable appropriation for nearly- incompressible fluids. In the Navier-Stokes equation, the hydrostatic pressure does not influence the velocity field at all. Only \\textit{differences} in density drive fluid flow, and so the sole term in which density needs to be considered variable is that of the gravitational body forces. In the case of density variations due to temperature, the equation of state is simply \\[ \\begin{equation} \\rho = \\rho_0 \\left(1 - \\alpha ( T-T_0 )\\right) \\label{eq:state} \\end{equation} \\] where \\(\\rho_0\\) is the density at a reference temperature \\(T_0\\). \\( \\alpha \\) is the coefficient of thermal expansion. It is generally much smaller than one, making the Boussinesq approximation a reasonable choice. The energy and momentum conservation equations thus become coupled through the term \\begin{equation} \\nonumber g\\rho\\hat{\\mathbf{z}} = g \\rho_0 \\left(1 - \\alpha(T-T_0)\\right) \\end{equation} Density variations due to pressure produce a perfectly vertical, isotropic forcing term on the momentum conservation equation. In the steady state case, this is balanced by the hydrostatic pressure gradient. (The isotropic term does not contribute at all the the deviatoric part of the stress equation and thus cannot induce steady flow). We therefore ignore the vertical density gradient due to the fluid overburden. Another density variation is that which results from variation in chemical composition from one fluid element to another. This is the case where two immiscible fluids live in the same region. Now density variations might be large -- the fluid domains must be considered separately. Advection and the Lagrangian Formulation We have seen the \\(\\mathbf{v} \\cdot \\nabla \\) operator a number of times now. The presence of this term causes major difficulties in continuum mechanics since it introduces a strong non-linearity into the momentum equation. It is this term which produces turbulence in high speed flows etc. This term is the 'advection' term which accounts for the passive transport of information (temperature, momentum, by the motion of the fluid. Advection also presents some serious headaches in numerical methods and has spawned entire literatures devoted to efficient and accurate solution methods. One obvious way to avoid the problem of advection is to consider an elemental volume of space which {\\em moves with the fluid}. The surface flux term from equation \\( (\\ref{eq:cons1}) \\) vanishes immediately. Mathematically, we introduce a new notation (of course), as follows: \\begin{equation} \\nonumber \\frac{D \\phi}{D t} = \\frac{d}{dt} \\phi[x _ 1(t),x _ 2(t),x _ 3(t),t] \\end{equation} where the change in the reference position \\( (x _ 1(t),x _ 2(t),x _ 3(t)) \\) is governed by the local flow velocity: \\begin{equation} \\nonumber \\frac{d x _ 1}{d t} = v _ 1 \\;\\;\\; \\frac{d x _ 2}{d t} = v _ 2 \\;\\;\\; \\frac{d x _ 3}{d t} = v _ 3 \\end{equation} which keeps the reference point moving with the fluid. Differentiating gives \\begin{align} \\frac{D \\phi}{D t} &= \\frac{\\partial \\phi}{\\partial t} \\frac{\\partial \\phi}{\\partial x _ 1}\\frac{d x _ 1}{d t} + \\frac{\\partial \\phi}{\\partial x _ 2}\\frac{d x _ 2}{d t} + \\frac{\\partial \\phi}{\\partial x _ 3}\\frac{d x _ 3}{d t} \\nonumber \\\\ \\textrm{and so leads to} \\;\\;\\; \\frac{D \\phi}{D t} &= \\frac{\\partial \\phi}{\\partial t} v_1 \\frac{\\partial \\phi}{\\partial x_1} + v_2 \\frac{\\partial \\phi}{\\partial x_2} + v_3 \\frac{\\partial \\phi}{\\partial x_3} \\nonumber \\\\ \\textrm{which is equivalent to} \\;\\;\\; \\frac{D \\phi}{D t} &= \\frac{\\partial \\phi}{\\partial t} + (\\mathbf{v} \\cdot \\nabla) \\phi \\end{align} If we think of \\( \\phi \\) as the concentration of a dye in the fluid, then the above is a conservation equation assuming the dye does not diffuse and has no sources or sinks. Viewed from a reference frame locked to a particular fluid element , the energy conservation equation becomes \\begin{equation} \\nonumber % \\rho ?? \\frac{D T}{Dt} = \\kappa \\nabla^2 T + \\frac{H}{C_p} \\end{equation} and the momentum conservation equation now becomes \\[ \\begin{equation} \\nonumber \\rho %% ? \\frac{D \\mathbf{v} }{D t} = \\eta \\nabla^2 \\mathbf{v} - \\nabla P - g\\rho\\hat{\\mathbf{z}} \\end{equation} \\] This is a considerably more compact way of writing the equations, but we have only really succeeded in hiding the nasty term under the rug, since it is now necessary to use a coordinate system which is locked into the fluid and rapidly deforms as the fluid flows. Before long, the coordinate system is unimaginably complex -- the advection problem returns in another guise. This formulation is known as the Lagrangian formulation and contrasts with the Eulerian viewpoint which is fixed in space. From the numerical point of view, however, this approach can have some distinct advantages. The computer can often track the distorted coordinate system far better than it can handle successive applications of the \\( \\mathbf{v} \\cdot \\nabla \\) operator at a fixed point in space. We will return to this point later. Non-dimensional equations & dimensionless numbers Before too long it would be a good idea to get a feeling for the flavour of these equations which continual rearrangements will not provide \u2014 it is necessary to examine some solutions. First of all, however, it is a good idea to make some simplifications based on the kinds of problems we will want to attack. The first thing to do, as is often the case when developing a model, is to test whether any of the terms in the equations are negligibly small, or utterly dominant. This is done by, essentially, dimensional analysis. Now we consider some 'typical values' for the independent dimensions of the system (mass, length, time, temperature, that sort of thing) which can be used to rescale the standard units. We rescale all lengths by the depth of the fluid, \\( d \\) (e.g. mantle thickness or depth of fluid in a lab tank), time according to the characteristic time for diffusion of heat, and temperature by the temperature difference across the depth of the layer. Obviously these choices are dependent on the problem in question but this exercise is a common one in fluid dynamics and provides a useful first step in the assault on the problem {: width=\"75%\"} Consider the fluid motions in a layer of arbitrary depth, \\(d\\). The fluid is assumed to have constant properties such as viscosity, thermal expansivity, thermal diffusivity. Small fluctuations in density due to temperature driven flow. Additional heat is carried (advected) by the flow from the hot boundary to the cool one whenever the fluid is moving. Various scalings result, with the new variables indicated using a prime (\\('\\)). \\begin{equation} \\nonumber \\begin{array}{llll} x = d.x' & \\partial / \\partial x = (1/d) \\partial / \\partial x' & \\nabla = (1/d) \\nabla ' \\\\ t = (d^2/\\kappa) t' & \\partial / \\partial t = (\\kappa/d^2) \\partial / \\partial t' & \\\\ T = \\Delta T T' & & \\\\ v = (\\kappa / d) v' && \\\\ p= p_0 + (\\eta \\kappa / d^2) p' \\end{array} \\end{equation} where \\begin{equation} \\nonumber \\nabla p _ 0 = - g \\rho_0 \\end{equation} Substituting for all the existing terms in the Navier-Stokes equation (\\ref{eq:navstokes}) using the equation of state for thermally induced variation in density (\\ref{eq:state}) gives: \\begin{equation} \\nonumber \\frac{\\rho_0 \\kappa}{d^2} \\frac{D}{Dt'} \\left( \\frac{\\kappa}{d} \\mathbf{v}' \\right) = \\frac{\\eta}{d^2} \\acute{\\nabla}^2 \\left( \\frac{\\kappa}{d} \\mathbf{v}' \\right) - \\frac{\\eta \\kappa}{d^3} \\acute{\\nabla} p' + g \\rho_0 \\alpha \\Delta T T' \\hat{\\mathbf{z}} \\end{equation} Collecting everything together gives \\begin{equation} \\nonumber \\frac{\\rho_0 \\kappa 2}{d 3} \\frac{D\\mathbf{v}'}{Dt'} = \\frac{\\eta \\kappa}{d^3} \\acute{\\nabla}^2 \\mathbf{v}' - \\frac{\\eta \\kappa}{d^3} \\acute{\\nabla} p' + g \\rho_0 \\alpha \\Delta T T' \\hat{\\mathbf{z}} \\end{equation} Divide throughout by \\(\\eta \\kappa / d^3\\) gives \\begin{equation} \\nonumber \\frac{\\rho \\kappa}{\\eta} \\frac{D\\mathbf{v}'}{Dt'} = \\acute{\\nabla} ^ 2 \\mathbf{v}' - \\acute{\\nabla} p' + \\frac{g \\rho _ 0 \\alpha \\Delta T d ^ 3}{\\kappa \\eta} T' \\hat{\\mathbf{z}} \\end{equation} where we can bundle up the coefficients into two dimensionless constants \\begin{equation} \\frac{1}{\\textrm{Pr}} \\frac{D \\mathbf{v}' }{Dt' } = \\acute{\\nabla}^2 \\mathbf{v}' - \\acute{\\nabla} p' + \\textrm{ Ra} T' \\hat{\\mathbf{z}} \\end{equation} \\(\\rm Pr\\) is known as the Prandtl number, and \\(\\rm Ra\\) is known as the Rayleigh number. By choosing to scale the equations (and this is still perfectly general as we haven't forced any particular choice of scaling yet), we have condensed the different physical variable quantities into just two numbers. The benefit of this procedure is that it tells us how different quantities trade off against one another. For example, we see that if the density doubles, and the viscosity doubles, then the solution should remain unchanged. In fact, the main purpose of this particular exercise is about to be revealed. The value of mantle viscosity is believed to lie somewhere between \\(10^{19}\\) and \\(10^{23}\\) \\({\\rm Pa . s}\\), the thermal diffusivity is around \\(10 ^ {-6}{\\rm m}^2{\\rm s}^{-1}\\), and density around \\(3300 {\\rm kg . m}^{-3}\\). This gives a Prandtl number greater than \\(10^{20}\\). Typical estimates for the Rayleigh number are between \\(10^6\\) and \\(10^8\\) depending on the supposed depth of convection, and the uncertain mantle viscosity. The fact that the constants may all vary with temperature and pressure increases the difficulty in specifying a meaningful single value of the Rayleigh number for any planet. Obviously, the time-dependent term (accelerations or the importance of inertia) can be neglected for the mantle, since it is at least twenty orders of magnitude smaller than other terms in the equations. The benefit of this is that the nasty advection term for momentum is eliminated -- flow in the mantle is at the opposite extreme to turbulent flow. The disadvantage is that the equations now become non-local: changes in the stress field are propogated instantly from point to point which can make the equations a lot harder to solve. This can be counter-intuitive but the consequences are important when considering the dynamic response of the Earth to changes in, for example, plate configurations. Incidentally, a third, independent dimensionless number can be derived for the thermally driven flow equations. This is the Nusselt number \\begin{equation} \\nonumber {\\rm Nu} = \\frac{Q}{k\\Delta T} \\end{equation} and is the ratio of actual heat transported by fluid motions in the layer compared to that transported conductively in the absence of fluid motion. All other dimensionless quantities for this system can be expressed as some combination of the Nusselt, Rayleigh and Prandtl numbers. The Prandtl number is a property of the fluid itself -- typical values are: air, \\( \\sim \\) 1; water, \\( \\sim\\) 6; non-conducting fluids \\(10^3\\) or more; liquid metal, \\( \\sim \\) 0.1. Rayleigh number and Nusselt number are both properties of the chosen geometry. Stream function / Vorticity Notation For incompressible flows in two dimensions it can be very convenient to work with the stream-function -- a scalar quantity which defines the flow everywhere. Another quantity much beloved of fluid dynamicists is the vorticity. Although the application of such quantities to deformation of the solid planets is actually quite limited, it is still useful for exploring the basic fluid dynamics of the large scale flow. Streamfunction The stream function is the scalar quantity, \\( \\psi \\), which satisfies \\[ \\begin{equation} v_1 = -\\frac{\\partial \\psi}{\\partial x_2} \\;\\;\\; v_2 = \\frac{\\partial \\psi}{\\partial x_1} \\label{eq:strmfn} \\end{equation} \\] so that, automatically, \\[ \\begin{equation} \\nonumber \\frac{\\partial v_1}{\\partial x_1} + \\frac{\\partial v_2}{\\partial x_2} = 0 \\end{equation} \\] Importantly, computing the following \\[ \\begin{equation} \\nonumber (\\mathbf{v} \\cdot \\nabla) \\psi = v_1 \\frac{\\partial \\psi}{\\partial x_1} + v_2 \\frac{\\partial \\psi}{\\partial x_2} = \\frac{\\partial \\psi}{\\partial x_2} \\frac{\\partial \\psi}{\\partial x_1} - \\frac{\\partial \\psi}{\\partial x_1} \\frac{\\partial \\psi}{\\partial x_2} = 0 \\end{equation} \\] tells us that \\(\\psi\\) does not change due to advection -- in other words, contours of constant \\(\\psi\\) re streamlines of the fluid. Provided we limit ourselves to the xy plane, it is possible to think of equation (\\ref{eq:strmfn}) as \\begin{equation} \\nonumber \\mathbf{v} = \\nabla \\times (\\psi \\hat{\\mathbf{k}}) \\end{equation} This form can be used to write down the2D axisymetric version of equation (\\ref{eq:strmfn}) at once \\begin{eqnarray} u_r = -\\frac{1}{r}\\frac{\\partial \\psi}{\\partial \\theta} & & u_\\theta = \\frac{\\partial \\psi}{\\partial r} \\end{eqnarray} which automatically satisfies the incompressibility condition in plane polar coordinates \\begin{equation} \\nonumber \\frac{1}{r}\\frac{\\partial}{\\partial r}(ru_r) + \\frac{1}{r}\\frac{\\partial u_\\theta}{\\partial \\theta} = 0 \\end{equation} Vorticity Vorticity is defined by \\begin{equation} \\nonumber \\boldsymbol{\\omega} = \\nabla \\times \\mathbf{v} \\end{equation} In 2D, the vorticity can be regarded as a scalar as it has only one component which lies out of the plane of the flow. \\begin{equation} \\nonumber \\omega = \\frac{\\partial v _ 2}{\\partial x _ 1} - \\frac{\\partial v _ 1}{\\partial x _ 2} \\end{equation} which is also exactly equal to twice the local measure of the spin in the fluid. Local here means that it applies to an infinitessimal region around the sample point but not to the fluid as a whole. This concept is most useful in the context of invicid flow where vorticity is conserved within the bulk of the fluid provided the fluid is subject to only conservative forces -- that is ones which can be described as the gradient of a single-valued potential. In the context of viscous flow, the viscous effects acts cause diffusion of vorticity, and in our context, the fact that buoyancy forces result from to (irreversible) heat transport means that vorticity has sources. Taking the curl of the Navier-Stokes equation, and substituting for the vorticity where possible gives \\begin{equation} \\nonumber \\frac{1}{\\rm Pr} \\left( \\frac{D \\boldsymbol{\\omega}}{D t} - (\\boldsymbol{\\omega} \\cdot \\nabla) \\mathbf{v} \\right) = \\eta \\nabla ^2 \\boldsymbol{\\omega} + {\\rm Ra} \\frac{\\partial T}{\\partial x _ 1} \\end{equation} The pressure drops out because \\( \\nabla \\times \\nabla P = 0 \\;\\;\\; \\forall P \\). Stream-function, Vorticity formulation In the context of highly viscous fluids in 2D, the vorticity equation is \\begin{equation} \\nabla ^2 \\omega = - Ra \\frac{\\partial T}{\\partial x _ 1} \\label{eq:vorteqn} \\end{equation} and, by considering the curl of \\( (-\\partial \\psi / \\partial x _ 2, \\partial \\psi / \\partial x _ 1, 0) \\) the stream function can be written \\begin{equation} \\nabla ^2 \\psi = \\omega \\label{eq:psivort} \\end{equation} This form is useful from a computational point of view because it is relatively easy to solve the Laplacian, and the code can be reused for each application of the operator. The Laplacian is also used for thermal diffusion -- one subroutine for three different bits of physics which is elegant in itself if nothing else. Biharmonic equation The biharmonic operator is defined as \\begin{equation} \\nonumber \\nabla^4 \\equiv \\nabla^2 ( \\nabla ^2) \\equiv \\left( \\frac{\\partial ^4}{\\partial x_1^4} + \\frac{\\partial ^2}{\\partial x_1^2} \\frac{\\partial ^2}{\\partial x_2^2} + \\frac{\\partial ^4}{\\partial x_2^4} \\right) \\end{equation} The latter form being the representation in Cartesian coordinates. Using this form, it is easy to show that equations (\\ref{eq:vorteqn}) and \\( (\\ref{eq:psivort}) \\) can be combined to give \\begin{equation} \\nabla^4 \\psi = -{\\rm Ra} \\frac{\\partial T}{\\partial x_1} \\label{eq:biharm} \\end{equation} Poloidal/Toroidal velocity decomposition The stream-function / vorticity form we have just used is a simplification of the more general case of the poloidal / toroidal velocity decomposition which turns out to be quite useful to understand the balance of different contributions to the governing equation. We can make a Helmholtz decomposition of the velocity vector field: \\begin{equation} \\nonumber \\mathbf{u} = \\nabla \\phi + \\nabla \\times \\mathbf{A} \\end{equation} Then for an incompressible flow, since \\(\\nabla \\cdot \\mathbf{u} = 0 \\), \\begin{equation} \\mathbf{u} = \\nabla \\times \\mathbf{A} \\label{eq:curlA} \\end{equation} Now suppose there is some direction (\\( \\hat{\\mathbf{z}} \\)) which we expect to be physically favoured in the solutions, we can rewrite \\ref{eq:curlA} as \\begin{equation} \\mathbf{u} = \\Red{\\nabla \\times(\\Psi \\hat{\\mathbf{z}})} + \\Blue{\\nabla \\times\\nabla \\times(\\Phi \\hat{\\mathbf{z}})} \\label{eq:poltor} \\end{equation} Where the first term on the right is the Toroidal part of the flow, and the second term is the Poloidal part. Why is this useful ? Let's substitute (\\ref{eq:poltor}) into the Stokes' equation for a constant viscosity fluid \\begin{equation} \\eta \\nabla^2 \\mathbf{u} - \\nabla p = g \\rho \\hat{\\mathbf{z}} \\label{eq:cvstokes} \\end{equation} where \\(\\hat{\\mathbf{z}}\\) is the vertical unit vector (defined by the direction of gravity) and is clearly the one identifiable special direction, then equate coefficients in the \\(\\hat{\\mathbf{z}}\\) direction, and using the following results: \\begin{equation} \\nonumber \\hat{\\mathbf{z}} \\cdot \\nabla \\times \\nabla^2 \\mathbf{u} = - \\nabla^2 \\nabla_h^2\\Psi \\end{equation} \\begin{equation} \\nonumber \\hat{\\mathbf{z}} \\cdot \\nabla \\times \\nabla \\times \\nabla^2 \\mathbf{u} = \\nabla^2 \\nabla^2 \\nabla_h^2\\Phi \\end{equation} where \\begin{equation} \\nonumber \\nabla_h = \\left( \\frac{\\partial}{\\partial x}, \\frac{\\partial}{\\partial y}, 0 \\right) \\end{equation} is a gradient operator limited to the plane perpendicular to the special direction, \\hat{\\mathbf{z}} \\hat{\\mathbf{z}} . If we first take the curl of (\\ref{eq:cvstokes}), and look at the \\hat{\\mathbf{z}} \\hat{\\mathbf{z}} direction, \\begin{equation} \\nonumber \\hat{\\mathbf{z}} \\cdot \\eta \\nabla \\times \\nabla^2 \\mathbf{u} = -\\eta \\nabla^2 \\nabla_h^2\\Psi = \\hat{\\mathbf{z}} \\cdot \\left( g \\nabla \\times \\left( \\rho \\hat{\\mathbf{z}}\\right)\\right) = 0 \\end{equation} we see that there is no contribution of the toroidal velocity field to the force balance. This balance occurs entirely through the poloidal part of the velocity field. If we take the curl twice and, once again, look at the \\hat{\\mathbf{z}} \\hat{\\mathbf{z}} direction: \\begin{equation} \\nonumber \\hat{\\mathbf{z}} \\cdot \\eta \\nabla \\times \\nabla \\times \\nabla^2 \\mathbf{u} = \\eta \\nabla^2 \\nabla^2 \\nabla_h^2 \\Phi = \\hat{\\mathbf{z}} \\cdot g \\nabla \\times \\nabla \\times \\left( \\rho \\hat{\\mathbf{z}}\\right) = \\nabla_h^2 (\\rho g) \\end{equation} Which is the 3D equivalent of the biharmonic equation that we derived above. Note: if the viscosity varies in the \\(\\hat{\\mathbf{z}}\\) direction, then this same decoupling still applies: bouyancy forces do not drive any toroidal flow. Lateral variations in viscosity (perpendicular to \\(\\hat{\\mathbf{z}}\\) ) couple the buoyancy to toroidal motion. This result is general in that it applies to the spherical geometry equally well assuming the radial direction (of gravity) to be special.","title":"Conservation Laws"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#conservation-laws","text":"\\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\] Let us start by deriving the equations of motion, energy balance and so on through a conservation principle. This will give a useful insight into the different forms of the equations which we will later encounter. A general conservation law does not distinguish the quantity which is being conserved -- it is a mathematical identity. Consider a quantity \\( \\phi \\) / unit mass which is carried around by a fluid. We can draw an arbitrary volume, \\( \\Omega \\) to contain some amount of this fluid at a given time. We label the surface of the volume \\( \\Omega \\) as \\( \\Gamma \\), and define an outward surface normal vector \\( \\dGamma \\) which is normal to the tangent plane of an infinitesimal element of the surface and has a magnitude equal to the area of this element. {: width=\"50%\"} Arbitrary fluid volume within which the fluid properties are to be conserved We also define a source/sink term, \\(H\\) / unit mass which generates/consumes the quantity \\( \\phi \\), and a flux term, \\(\\mathbf{F}\\) which occurs across the surface when the fluid is stationary (e.g. this might represent diffusion of \\( \\phi \\)). The rate of change of \\( \\phi \\) is given by combining the contribution due to the source term, the stationary flux term, and the effect of motion of the fluid. \\begin{equation} \\frac{d}{dt} \\int _ {\\Omega} \\rho \\phi d\\Omega = - \\int _ {\\Gamma} \\mathbf{F} \\cdot \\dGamma + \\int _ {\\Omega} \\rho H d\\Omega - \\int _ {\\Gamma} \\rho \\phi \\mathbf{v} \\cdot \\dGamma \\label{eq:1} \\end{equation} where the final term is the change due to the fluid carrying \\( \\phi \\) through the volume. Fluxes are positive outward (by our definition of \\( \\dGamma \\)), so a positive flux reduces \\( \\phi \\) within \\( d\\Omega \\) and negative signs are needed for these terms. We can generally use Gauss' theorem to write surface integrals as volume integrals: \\begin{equation} \\nonumber \\int _ {\\Gamma} \\phi \\mathbf{u} \\cdot \\dGamma = \\int _ {\\Omega} \\nabla \\cdot (\\phi \\mathbf{u}) d\\Omega \\end{equation} The test surface, \\( \\Gamma \\), and volume, \\( \\Omega \\) are assumed to be_fixed in the lab reference frame_ so that the order of integration and differentiation can be interchanged \\begin{equation} \\nonumber \\frac{d}{dt} \\int_\\Omega \\rho \\phi d\\Omega = \\int _ \\Omega \\frac{\\partial \\rho \\phi}{\\partial t} d \\Omega \\end{equation} Allowing us to write \\begin{equation} \\nonumber \\int_{\\Gamma} \\mathbf{F} \\cdot \\dGamma + \\int _ {\\Gamma} \\rho \\phi \\mathbf{v} \\cdot \\dGamma = \\int _ {\\Omega} \\nabla \\cdot (\\mathbf{F} + \\rho \\phi \\mathbf{v}) d\\Omega \\end{equation} so that we rewrite the general conservation equation as \\begin{equation} \\int_{\\Omega} \\left[ \\frac{d \\rho \\phi}{dt} + \\nabla \\cdot (\\mathbf{F} + \\rho \\phi \\mathbf{v}) -\\rho H \\right] d\\Omega=0 \\label{eq:cons1}) \\end{equation} We can now appeal to the fact that this conservation law holds regardless of our particular choice of volume \\( \\Omega \\) and, therefore, the integral above can only be zero for arbitrary \\( \\Omega \\) if the enclosed term is zero everywhere, i.e. \\begin{equation} \\frac{d \\rho \\phi}{dt} + \\nabla \\cdot (\\mathbf{F} + \\rho \\phi \\mathbf{v}) -\\rho H =0 \\label{eq:cons2} \\end{equation} This is a general conservation rule for any property \\( \\phi \\) of a moving fluid. We now consider various quantities that may be transported and develop specific conservation laws.","title":"Conservation Laws"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#conservation-of-mass","text":"In this case\\( \\phi = 1 \\) (since \\(\\int_\\Omega \\rho d\\Omega \\rightarrow \\mbox{mass}\\)), \\(\\mathbf{F} = H = 0\\). Thus, equation (\\ref{eq:cons2}) gives \\begin{equation} \\Red{\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot \\rho \\mathbf{v} = 0} \\label{eq:masscons} \\end{equation}","title":"Conservation of mass"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#conservation-of-heat-energy","text":"The thermal energy / unit mass is \\(C_p T\\) and the conductive heat flux \\(\\mathbf{F}\\) is given by \\( \\mathbf{F} = -k \\nabla T\\), where \\(k\\) is the thermal conductivity. Then the general conservation law of \\( (\\ref{eq:cons2}) \\) reduces to \\begin{equation} \\nonumber \\frac{\\partial (\\rho C_p T)}{\\partial t} + \\nabla \\cdot \\left(-k \\nabla T + \\rho C_p T \\mathbf{v}\\right) - \\rho H = 0 \\end{equation} Rearranging with some foresight gives the following: \\begin{equation} \\nonumber \\frac{C_p T}{\\rho} \\left[ \\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot \\rho \\mathbf{v} \\right] + \\frac{\\partial C_p T}{\\partial t} + \\mathbf{v} \\cdot \\nabla C_p T = \\frac{1}{\\rho} \\nabla \\cdot k \\nabla T + H \\end{equation} Where the term in square brackets is simply the statement of mass conservation which vanishes identically. The constant density assumption is just to simplify the discussion at this point and will have to be revisited later in the context of thermal convection where density changes drive the flow. If the heat capacity, \\(C_p\\) and thermal conductivity, \\(k \\) are constants, then the conservation equation becomes \\begin{equation} \\nonumber \\Red{ \\left( \\frac{\\partial T}{\\partial t} + \\mathbf{v} \\cdot \\nabla T \\right)= \\kappa \\nabla ^ 2 T + \\frac{H}{C _ p} } \\label{eq:energycons} \\end{equation} where \\(\\kappa\\) is the thermal diffusivity, \\(\\kappa = k/\\rho C_p\\). This equation is linear provided that the velocity field is specified and is independent of \\(T\\). Clearly this is generally not true because mantle circulation is driven by thermal buoyancy. A new bit of notation has been defined in the process of this derivation. The meaning of the \\(\\mathbf{v} \\cdot \\nabla\\) operator is \\begin{equation} \\nonumber \\mathbf{v} \\cdot \\nabla \\equiv v _ j \\frac{\\partial}{\\partial x _ j} \\end{equation} For later reference, this is how it looks: \\begin{equation} \\nonumber (\\mathbf{v} \\cdot \\nabla) T = v _ 1 \\frac{\\partial T}{\\partial x _ 1} + v _ 2 \\frac{\\partial T}{\\partial x _ 2} + v _ 3 \\frac{\\partial T}{\\partial x _ 3} \\end{equation} The Laplacian, \\(\\nabla ^ 2\\), is this expression (for scalar \\(T\\)) \\begin{equation} \\nonumber \\nabla ^ 2 T \\equiv \\frac{\\partial ^ 2 T}{\\partial x _ 1 ^ 2} + \\frac{\\partial ^ 2 T}{\\partial x _ 2 ^ 2} + \\frac{\\partial ^ 2 T}{\\partial x _ 3 ^ 2} \\end{equation}","title":"Conservation of (heat) energy"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#conservation-of-momentum","text":"Momentum is a vector quantity, so the form of (\\ref{eq:cons1}) is slightly different. The source term in a momentum equation represents a force, and the surface flux term represents a stress. \\begin{equation} \\frac{d}{dt}\\int _ {\\Omega} \\rho \\mathbf{v} d\\Omega = - \\int _ {\\Omega} \\rho g \\hat{\\mathbf{z}} d\\Omega + \\int _ {\\Gamma} \\boldsymbol{\\sigma} \\cdot \\dGamma - \\int _ {\\Gamma} \\rho \\mathbf{v} (\\mathbf{v} \\cdot \\dGamma) \\label{eq:momcons1} \\end{equation} Gravity acts as a body force in the vertical direction, \\(\\hat{\\mathbf{z}}\\). We have introduced the stress tensor, \\(\\boldsymbol{\\sigma}\\); the force / unit area exerted on an arbitrarily oriented surface with normal \\(\\hat{\\mathbf {n}}\\) is \\begin{equation} \\nonumber f _ i = \\sigma _ {ij} n _ j \\end{equation} The application of Gauss' theorem, and using the arbitrary nature of the chosen volume to require the integrand to be zero, as before, gives \\begin{equation} \\frac{\\partial}{\\partial t}(\\rho \\mathbf{v}) +\\rho g \\hat{\\mathbf{z}} - \\nabla \\cdot \\sigma + \\nabla \\cdot (\\rho \\mathbf{v} \\mathbf{v}) = 0 \\label{eq:momcons2} \\end{equation} Vector notation allows us to keep a number of equations written as one single equation. However, at this point, keeping the equations in vector notation makes the situation more confusing. Particularly, the last term of equation \\( (\\ref{eq:momcons2}) \\) stretches the current notation rather too much. Instead, we consider the individual components of momentum, each of which must satisfy the conservation law independently In index notation, equation (\\ref{eq:momcons2}) is written as \\begin{equation} \\nonumber \\Green{\\frac{\\partial \\rho v _ i}{\\partial t}} + \\rho g \\delta _ {i3} - \\frac{\\partial \\sigma _ {ij}}{\\partial x _ j} + \\Blue{\\frac{\\partial(\\rho v _ i v _ j)}{\\partial x _ j}} = 0 \\label{eq:momindx} \\end{equation} where the troublesome final term is now unambiguous. Repeated indeces in each term are implicitly to be summed. \\( \\delta _ {ij}\\) is the kronecker delta which obeys: \\begin{equation} \\nonumber \\delta _ {ij} = \\begin{cases} 0 & \\text{if i \\neq j i \\neq j }, \\\\ 1 & \\text{if i = j i = j }. \\end{cases} \\end{equation} With some foresight, we expand the derivatives of products of two terms and gather up some of the resulting terms: \\begin{equation} \\nonumber v _ i \\left[\\Green{\\frac{\\partial \\rho}{\\partial t}} + \\Blue{ \\frac{\\partial \\rho v _ j}{\\partial x _ j}} \\right] + \\Green{\\rho \\frac{\\partial v _ i}{\\partial t}} + \\Blue{\\rho v _ j \\frac{\\partial v _ i}{\\partial x _ j}} = \\frac{\\partial \\sigma _ {ij}}{\\partial x _ j} -\\rho g \\delta _ {i3} \\end{equation} The term in square brackets is, in fact, a restatement of the conservation of mass derived above and must vanish. The remaining terms can now be written out in vector notation as \\begin{equation} \\nonumber \\Red{ \\rho \\left( \\frac{\\partial \\mathbf{v}}{\\partial t} + (\\mathbf{v} \\cdot \\nabla) \\mathbf{v} \\right) = \\nabla \\cdot \\boldsymbol{\\sigma} - g\\rho\\hat{\\mathbf{z}} } \\label{eq:momcons3} \\end{equation} Note: The \\(\\mathbf{v} \\cdot \\nabla \\) notation we introduced earlier is now an operator on a vector. In this context, the \\(\\mathbf{v} \\cdot \\nabla \\) operator acts on each component of the vector independently. Written out it looks like this: \\begin{equation} \\nonumber \\begin{split} (\\mathbf{v} \\cdot \\nabla) \\mathbf{u} = & \\hat{\\boldsymbol{\\imath}} \\left( v _ 1 \\frac{\\partial u _ 1}{\\partial x _ 1} + v_2 \\frac{\\partial u _ 1}{\\partial x _ 2} + v_3 \\frac{\\partial u _ 1}{\\partial x _ 3} \\right) \\\\ & \\hat{\\boldsymbol{\\jmath}} \\left( v_1 \\frac{\\partial u_2}{\\partial x_1} + v_2 \\frac{\\partial u _ 2}{\\partial x _ 2} + v _ 3 \\frac{\\partial u _ 2}{\\partial x _ 3} \\right) \\\\ & \\hat{\\boldsymbol{k}} \\left( v_1 \\frac{\\partial u_3}{\\partial x_1} + v_2 \\frac{\\partial u _ 3}{\\partial x _ 2} + v _ 3 \\frac{\\partial u _ 3}{\\partial x _ 3} \\right) \\\\ \\end{split} \\end{equation}","title":"Conservation of momentum"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#constitutive-laws","text":"The formulation above is quite general, and can be extended where necessary to include conservation laws for additional physical variables (for example, angular momentum, electric current). Specific to the type of material which is deforming is the constitutive law which describes the stress. In the case of an incompressible fluid, the stress is related to strain rate through a viscosity, \\(\\eta\\), and to the pressure, \\(p\\). Incompressibility is expressed as \\begin{equation} \\nonumber \\nabla \\cdot \\mathbf{u} = 0 \\end{equation} This is a tighter constraint than mass conservation and emerges from equation (\\ref{eq:masscons}) when \\( \\rho \\) is assumed to be constant. With this assumption, the constitutive law is written (A derivation for this is in Landau & Lifschitz). \\begin{equation} \\nonumber \\sigma _ {ij} = \\eta \\left( \\frac{\\partial v _ i}{\\partial x _ j} + \\frac{\\partial v _ j}{\\partial x _ i}\\right) - p\\delta _ {ij} \\end{equation} If the viscosity is constant, then we can substitute the constitutive law into the stress-divergence term of the momentum conservation equation. In index notation once again, \\begin{equation} \\nonumber \\begin{split} \\nabla \\cdot \\boldsymbol{\\sigma} & = \\frac{\\partial}{\\partial x _ j} \\eta \\left( \\frac{\\partial v _ i}{\\partial x _ j} + \\frac{\\partial v _ j}{\\partial x _ i} \\right) - \\frac{\\partial p}{\\partial x _ j} \\delta _ {ij} \\ & = \\eta \\frac{\\partial^2 v_i}{\\partial x _ j \\partial x _ j} + \\eta \\frac{\\partial^2 v _ j}{\\partial x _ i \\partial x _ j} - \\frac{\\partial p}{\\partial x _ i}\\ & = \\eta \\nabla^2 \\mathbf{v} + \\eta \\Green{\\nabla (\\nabla \\cdot \\mathbf{v})} - \\nabla p \\end{split} \\end{equation} the second term in this final form must vanish because of the incompressibility assumption, so the momentum conservation equation becomes \\begin{equation} \\Red{ \\rho \\left( \\frac{\\partial \\mathbf{v}}{\\partial t} + (\\mathbf{v} \\cdot \\nabla) \\mathbf{v} \\right) = \\eta \\nabla^2 \\mathbf{v} - \\nabla p - g\\rho\\hat{\\mathbf{z}} } \\label{eq:navstokes} \\end{equation} This is the Navier-Stokes equation. Once again some new notation has shown up uninvited. The Laplacian operator \\(\\nabla ^ 2\\) is defined (in a scalar context) as \\begin{equation} \\nonumber \\begin{split} \\nabla ^ 2 \\phi & = \\nabla \\cdot \\nabla \\phi \\\\ & = \\frac{\\partial ^ 2 \\phi}{\\partial x ^ 2} + \\frac{\\partial ^ 2 \\phi}{\\partial y ^ 2} + \\frac{\\partial ^ 2 \\phi}{\\partial z ^ 2} \\;\\;\\; \\textrm{(Cartesian)} \\end{split} \\end{equation} and in a vector context as \\begin{equation} \\nonumber \\begin{split} \\nabla^2 \\mathbf{u} & = \\nabla \\nabla \\cdot \\mathbf{u} - \\nabla \\times (\\nabla \\times \\mathbf{u}) \\ & = \\mathbf{i} \\nabla \\cdot \\nabla u_x + \\mathbf{j} \\nabla \\cdot \\nabla u_y + \\mathbf{k} \\nabla \\cdot \\nabla u_z \\;\\;\\; \\text{(Cartesian)} \\end{split} \\end{equation} In Cartesian coordinates, the Laplacian operator has a simple form, and the vector Laplacian is simply the scalar operator applied in each direction. In other coordinate systems this operator becomes substantially more elaborate.","title":"Constitutive Laws"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#boussinesq-approximation-equation-of-state-density-variations","text":"The equation which relates pressure, temperature and density is known as the equation of state. For the equations derived so far, we have specified an incompressible fluid, for which no density variations are possible. However, we have also included a source term for momentum which relies on gravity acting on density variations . This conflict is typical of fluid mechanics: simplifying assumptions if taken to their logical limit imply no motion or some other trivial solution to the equations. In this case, we make the assumption that density changes are typically small relative to the overall magnitude of the density itself. Terms which are scaled by density can therefore assume that it is a large constant value. Terms which contain gradients of density or density variations should consider the equation of state. This is the Boussinesq approximation and is only a suitable appropriation for nearly- incompressible fluids. In the Navier-Stokes equation, the hydrostatic pressure does not influence the velocity field at all. Only \\textit{differences} in density drive fluid flow, and so the sole term in which density needs to be considered variable is that of the gravitational body forces. In the case of density variations due to temperature, the equation of state is simply \\[ \\begin{equation} \\rho = \\rho_0 \\left(1 - \\alpha ( T-T_0 )\\right) \\label{eq:state} \\end{equation} \\] where \\(\\rho_0\\) is the density at a reference temperature \\(T_0\\). \\( \\alpha \\) is the coefficient of thermal expansion. It is generally much smaller than one, making the Boussinesq approximation a reasonable choice. The energy and momentum conservation equations thus become coupled through the term \\begin{equation} \\nonumber g\\rho\\hat{\\mathbf{z}} = g \\rho_0 \\left(1 - \\alpha(T-T_0)\\right) \\end{equation} Density variations due to pressure produce a perfectly vertical, isotropic forcing term on the momentum conservation equation. In the steady state case, this is balanced by the hydrostatic pressure gradient. (The isotropic term does not contribute at all the the deviatoric part of the stress equation and thus cannot induce steady flow). We therefore ignore the vertical density gradient due to the fluid overburden. Another density variation is that which results from variation in chemical composition from one fluid element to another. This is the case where two immiscible fluids live in the same region. Now density variations might be large -- the fluid domains must be considered separately.","title":"Boussinesq Approximation, Equation of State, Density Variations"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#advection-and-the-lagrangian-formulation","text":"We have seen the \\(\\mathbf{v} \\cdot \\nabla \\) operator a number of times now. The presence of this term causes major difficulties in continuum mechanics since it introduces a strong non-linearity into the momentum equation. It is this term which produces turbulence in high speed flows etc. This term is the 'advection' term which accounts for the passive transport of information (temperature, momentum, by the motion of the fluid. Advection also presents some serious headaches in numerical methods and has spawned entire literatures devoted to efficient and accurate solution methods. One obvious way to avoid the problem of advection is to consider an elemental volume of space which {\\em moves with the fluid}. The surface flux term from equation \\( (\\ref{eq:cons1}) \\) vanishes immediately. Mathematically, we introduce a new notation (of course), as follows: \\begin{equation} \\nonumber \\frac{D \\phi}{D t} = \\frac{d}{dt} \\phi[x _ 1(t),x _ 2(t),x _ 3(t),t] \\end{equation} where the change in the reference position \\( (x _ 1(t),x _ 2(t),x _ 3(t)) \\) is governed by the local flow velocity: \\begin{equation} \\nonumber \\frac{d x _ 1}{d t} = v _ 1 \\;\\;\\; \\frac{d x _ 2}{d t} = v _ 2 \\;\\;\\; \\frac{d x _ 3}{d t} = v _ 3 \\end{equation} which keeps the reference point moving with the fluid. Differentiating gives \\begin{align} \\frac{D \\phi}{D t} &= \\frac{\\partial \\phi}{\\partial t} \\frac{\\partial \\phi}{\\partial x _ 1}\\frac{d x _ 1}{d t} + \\frac{\\partial \\phi}{\\partial x _ 2}\\frac{d x _ 2}{d t} + \\frac{\\partial \\phi}{\\partial x _ 3}\\frac{d x _ 3}{d t} \\nonumber \\\\ \\textrm{and so leads to} \\;\\;\\; \\frac{D \\phi}{D t} &= \\frac{\\partial \\phi}{\\partial t} v_1 \\frac{\\partial \\phi}{\\partial x_1} + v_2 \\frac{\\partial \\phi}{\\partial x_2} + v_3 \\frac{\\partial \\phi}{\\partial x_3} \\nonumber \\\\ \\textrm{which is equivalent to} \\;\\;\\; \\frac{D \\phi}{D t} &= \\frac{\\partial \\phi}{\\partial t} + (\\mathbf{v} \\cdot \\nabla) \\phi \\end{align} If we think of \\( \\phi \\) as the concentration of a dye in the fluid, then the above is a conservation equation assuming the dye does not diffuse and has no sources or sinks. Viewed from a reference frame locked to a particular fluid element , the energy conservation equation becomes \\begin{equation} \\nonumber % \\rho ?? \\frac{D T}{Dt} = \\kappa \\nabla^2 T + \\frac{H}{C_p} \\end{equation} and the momentum conservation equation now becomes \\[ \\begin{equation} \\nonumber \\rho %% ? \\frac{D \\mathbf{v} }{D t} = \\eta \\nabla^2 \\mathbf{v} - \\nabla P - g\\rho\\hat{\\mathbf{z}} \\end{equation} \\] This is a considerably more compact way of writing the equations, but we have only really succeeded in hiding the nasty term under the rug, since it is now necessary to use a coordinate system which is locked into the fluid and rapidly deforms as the fluid flows. Before long, the coordinate system is unimaginably complex -- the advection problem returns in another guise. This formulation is known as the Lagrangian formulation and contrasts with the Eulerian viewpoint which is fixed in space. From the numerical point of view, however, this approach can have some distinct advantages. The computer can often track the distorted coordinate system far better than it can handle successive applications of the \\( \\mathbf{v} \\cdot \\nabla \\) operator at a fixed point in space. We will return to this point later.","title":"Advection and the Lagrangian Formulation"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#non-dimensional-equations-dimensionless-numbers","text":"Before too long it would be a good idea to get a feeling for the flavour of these equations which continual rearrangements will not provide \u2014 it is necessary to examine some solutions. First of all, however, it is a good idea to make some simplifications based on the kinds of problems we will want to attack. The first thing to do, as is often the case when developing a model, is to test whether any of the terms in the equations are negligibly small, or utterly dominant. This is done by, essentially, dimensional analysis. Now we consider some 'typical values' for the independent dimensions of the system (mass, length, time, temperature, that sort of thing) which can be used to rescale the standard units. We rescale all lengths by the depth of the fluid, \\( d \\) (e.g. mantle thickness or depth of fluid in a lab tank), time according to the characteristic time for diffusion of heat, and temperature by the temperature difference across the depth of the layer. Obviously these choices are dependent on the problem in question but this exercise is a common one in fluid dynamics and provides a useful first step in the assault on the problem {: width=\"75%\"} Consider the fluid motions in a layer of arbitrary depth, \\(d\\). The fluid is assumed to have constant properties such as viscosity, thermal expansivity, thermal diffusivity. Small fluctuations in density due to temperature driven flow. Additional heat is carried (advected) by the flow from the hot boundary to the cool one whenever the fluid is moving. Various scalings result, with the new variables indicated using a prime (\\('\\)). \\begin{equation} \\nonumber \\begin{array}{llll} x = d.x' & \\partial / \\partial x = (1/d) \\partial / \\partial x' & \\nabla = (1/d) \\nabla ' \\\\ t = (d^2/\\kappa) t' & \\partial / \\partial t = (\\kappa/d^2) \\partial / \\partial t' & \\\\ T = \\Delta T T' & & \\\\ v = (\\kappa / d) v' && \\\\ p= p_0 + (\\eta \\kappa / d^2) p' \\end{array} \\end{equation} where \\begin{equation} \\nonumber \\nabla p _ 0 = - g \\rho_0 \\end{equation} Substituting for all the existing terms in the Navier-Stokes equation (\\ref{eq:navstokes}) using the equation of state for thermally induced variation in density (\\ref{eq:state}) gives: \\begin{equation} \\nonumber \\frac{\\rho_0 \\kappa}{d^2} \\frac{D}{Dt'} \\left( \\frac{\\kappa}{d} \\mathbf{v}' \\right) = \\frac{\\eta}{d^2} \\acute{\\nabla}^2 \\left( \\frac{\\kappa}{d} \\mathbf{v}' \\right) - \\frac{\\eta \\kappa}{d^3} \\acute{\\nabla} p' + g \\rho_0 \\alpha \\Delta T T' \\hat{\\mathbf{z}} \\end{equation} Collecting everything together gives \\begin{equation} \\nonumber \\frac{\\rho_0 \\kappa 2}{d 3} \\frac{D\\mathbf{v}'}{Dt'} = \\frac{\\eta \\kappa}{d^3} \\acute{\\nabla}^2 \\mathbf{v}' - \\frac{\\eta \\kappa}{d^3} \\acute{\\nabla} p' + g \\rho_0 \\alpha \\Delta T T' \\hat{\\mathbf{z}} \\end{equation} Divide throughout by \\(\\eta \\kappa / d^3\\) gives \\begin{equation} \\nonumber \\frac{\\rho \\kappa}{\\eta} \\frac{D\\mathbf{v}'}{Dt'} = \\acute{\\nabla} ^ 2 \\mathbf{v}' - \\acute{\\nabla} p' + \\frac{g \\rho _ 0 \\alpha \\Delta T d ^ 3}{\\kappa \\eta} T' \\hat{\\mathbf{z}} \\end{equation} where we can bundle up the coefficients into two dimensionless constants \\begin{equation} \\frac{1}{\\textrm{Pr}} \\frac{D \\mathbf{v}' }{Dt' } = \\acute{\\nabla}^2 \\mathbf{v}' - \\acute{\\nabla} p' + \\textrm{ Ra} T' \\hat{\\mathbf{z}} \\end{equation} \\(\\rm Pr\\) is known as the Prandtl number, and \\(\\rm Ra\\) is known as the Rayleigh number. By choosing to scale the equations (and this is still perfectly general as we haven't forced any particular choice of scaling yet), we have condensed the different physical variable quantities into just two numbers. The benefit of this procedure is that it tells us how different quantities trade off against one another. For example, we see that if the density doubles, and the viscosity doubles, then the solution should remain unchanged. In fact, the main purpose of this particular exercise is about to be revealed. The value of mantle viscosity is believed to lie somewhere between \\(10^{19}\\) and \\(10^{23}\\) \\({\\rm Pa . s}\\), the thermal diffusivity is around \\(10 ^ {-6}{\\rm m}^2{\\rm s}^{-1}\\), and density around \\(3300 {\\rm kg . m}^{-3}\\). This gives a Prandtl number greater than \\(10^{20}\\). Typical estimates for the Rayleigh number are between \\(10^6\\) and \\(10^8\\) depending on the supposed depth of convection, and the uncertain mantle viscosity. The fact that the constants may all vary with temperature and pressure increases the difficulty in specifying a meaningful single value of the Rayleigh number for any planet. Obviously, the time-dependent term (accelerations or the importance of inertia) can be neglected for the mantle, since it is at least twenty orders of magnitude smaller than other terms in the equations. The benefit of this is that the nasty advection term for momentum is eliminated -- flow in the mantle is at the opposite extreme to turbulent flow. The disadvantage is that the equations now become non-local: changes in the stress field are propogated instantly from point to point which can make the equations a lot harder to solve. This can be counter-intuitive but the consequences are important when considering the dynamic response of the Earth to changes in, for example, plate configurations. Incidentally, a third, independent dimensionless number can be derived for the thermally driven flow equations. This is the Nusselt number \\begin{equation} \\nonumber {\\rm Nu} = \\frac{Q}{k\\Delta T} \\end{equation} and is the ratio of actual heat transported by fluid motions in the layer compared to that transported conductively in the absence of fluid motion. All other dimensionless quantities for this system can be expressed as some combination of the Nusselt, Rayleigh and Prandtl numbers. The Prandtl number is a property of the fluid itself -- typical values are: air, \\( \\sim \\) 1; water, \\( \\sim\\) 6; non-conducting fluids \\(10^3\\) or more; liquid metal, \\( \\sim \\) 0.1. Rayleigh number and Nusselt number are both properties of the chosen geometry.","title":"Non-dimensional equations &amp; dimensionless numbers"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#stream-function-vorticity-notation","text":"For incompressible flows in two dimensions it can be very convenient to work with the stream-function -- a scalar quantity which defines the flow everywhere. Another quantity much beloved of fluid dynamicists is the vorticity. Although the application of such quantities to deformation of the solid planets is actually quite limited, it is still useful for exploring the basic fluid dynamics of the large scale flow.","title":"Stream function / Vorticity Notation"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#streamfunction","text":"The stream function is the scalar quantity, \\( \\psi \\), which satisfies \\[ \\begin{equation} v_1 = -\\frac{\\partial \\psi}{\\partial x_2} \\;\\;\\; v_2 = \\frac{\\partial \\psi}{\\partial x_1} \\label{eq:strmfn} \\end{equation} \\] so that, automatically, \\[ \\begin{equation} \\nonumber \\frac{\\partial v_1}{\\partial x_1} + \\frac{\\partial v_2}{\\partial x_2} = 0 \\end{equation} \\] Importantly, computing the following \\[ \\begin{equation} \\nonumber (\\mathbf{v} \\cdot \\nabla) \\psi = v_1 \\frac{\\partial \\psi}{\\partial x_1} + v_2 \\frac{\\partial \\psi}{\\partial x_2} = \\frac{\\partial \\psi}{\\partial x_2} \\frac{\\partial \\psi}{\\partial x_1} - \\frac{\\partial \\psi}{\\partial x_1} \\frac{\\partial \\psi}{\\partial x_2} = 0 \\end{equation} \\] tells us that \\(\\psi\\) does not change due to advection -- in other words, contours of constant \\(\\psi\\) re streamlines of the fluid. Provided we limit ourselves to the xy plane, it is possible to think of equation (\\ref{eq:strmfn}) as \\begin{equation} \\nonumber \\mathbf{v} = \\nabla \\times (\\psi \\hat{\\mathbf{k}}) \\end{equation} This form can be used to write down the2D axisymetric version of equation (\\ref{eq:strmfn}) at once \\begin{eqnarray} u_r = -\\frac{1}{r}\\frac{\\partial \\psi}{\\partial \\theta} & & u_\\theta = \\frac{\\partial \\psi}{\\partial r} \\end{eqnarray} which automatically satisfies the incompressibility condition in plane polar coordinates \\begin{equation} \\nonumber \\frac{1}{r}\\frac{\\partial}{\\partial r}(ru_r) + \\frac{1}{r}\\frac{\\partial u_\\theta}{\\partial \\theta} = 0 \\end{equation}","title":"Streamfunction"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#vorticity","text":"Vorticity is defined by \\begin{equation} \\nonumber \\boldsymbol{\\omega} = \\nabla \\times \\mathbf{v} \\end{equation} In 2D, the vorticity can be regarded as a scalar as it has only one component which lies out of the plane of the flow. \\begin{equation} \\nonumber \\omega = \\frac{\\partial v _ 2}{\\partial x _ 1} - \\frac{\\partial v _ 1}{\\partial x _ 2} \\end{equation} which is also exactly equal to twice the local measure of the spin in the fluid. Local here means that it applies to an infinitessimal region around the sample point but not to the fluid as a whole. This concept is most useful in the context of invicid flow where vorticity is conserved within the bulk of the fluid provided the fluid is subject to only conservative forces -- that is ones which can be described as the gradient of a single-valued potential. In the context of viscous flow, the viscous effects acts cause diffusion of vorticity, and in our context, the fact that buoyancy forces result from to (irreversible) heat transport means that vorticity has sources. Taking the curl of the Navier-Stokes equation, and substituting for the vorticity where possible gives \\begin{equation} \\nonumber \\frac{1}{\\rm Pr} \\left( \\frac{D \\boldsymbol{\\omega}}{D t} - (\\boldsymbol{\\omega} \\cdot \\nabla) \\mathbf{v} \\right) = \\eta \\nabla ^2 \\boldsymbol{\\omega} + {\\rm Ra} \\frac{\\partial T}{\\partial x _ 1} \\end{equation} The pressure drops out because \\( \\nabla \\times \\nabla P = 0 \\;\\;\\; \\forall P \\).","title":"Vorticity"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#stream-function-vorticity-formulation","text":"In the context of highly viscous fluids in 2D, the vorticity equation is \\begin{equation} \\nabla ^2 \\omega = - Ra \\frac{\\partial T}{\\partial x _ 1} \\label{eq:vorteqn} \\end{equation} and, by considering the curl of \\( (-\\partial \\psi / \\partial x _ 2, \\partial \\psi / \\partial x _ 1, 0) \\) the stream function can be written \\begin{equation} \\nabla ^2 \\psi = \\omega \\label{eq:psivort} \\end{equation} This form is useful from a computational point of view because it is relatively easy to solve the Laplacian, and the code can be reused for each application of the operator. The Laplacian is also used for thermal diffusion -- one subroutine for three different bits of physics which is elegant in itself if nothing else.","title":"Stream-function, Vorticity formulation"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#biharmonic-equation","text":"The biharmonic operator is defined as \\begin{equation} \\nonumber \\nabla^4 \\equiv \\nabla^2 ( \\nabla ^2) \\equiv \\left( \\frac{\\partial ^4}{\\partial x_1^4} + \\frac{\\partial ^2}{\\partial x_1^2} \\frac{\\partial ^2}{\\partial x_2^2} + \\frac{\\partial ^4}{\\partial x_2^4} \\right) \\end{equation} The latter form being the representation in Cartesian coordinates. Using this form, it is easy to show that equations (\\ref{eq:vorteqn}) and \\( (\\ref{eq:psivort}) \\) can be combined to give \\begin{equation} \\nabla^4 \\psi = -{\\rm Ra} \\frac{\\partial T}{\\partial x_1} \\label{eq:biharm} \\end{equation}","title":"Biharmonic equation"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-2.html#poloidaltoroidal-velocity-decomposition","text":"The stream-function / vorticity form we have just used is a simplification of the more general case of the poloidal / toroidal velocity decomposition which turns out to be quite useful to understand the balance of different contributions to the governing equation. We can make a Helmholtz decomposition of the velocity vector field: \\begin{equation} \\nonumber \\mathbf{u} = \\nabla \\phi + \\nabla \\times \\mathbf{A} \\end{equation} Then for an incompressible flow, since \\(\\nabla \\cdot \\mathbf{u} = 0 \\), \\begin{equation} \\mathbf{u} = \\nabla \\times \\mathbf{A} \\label{eq:curlA} \\end{equation} Now suppose there is some direction (\\( \\hat{\\mathbf{z}} \\)) which we expect to be physically favoured in the solutions, we can rewrite \\ref{eq:curlA} as \\begin{equation} \\mathbf{u} = \\Red{\\nabla \\times(\\Psi \\hat{\\mathbf{z}})} + \\Blue{\\nabla \\times\\nabla \\times(\\Phi \\hat{\\mathbf{z}})} \\label{eq:poltor} \\end{equation} Where the first term on the right is the Toroidal part of the flow, and the second term is the Poloidal part. Why is this useful ? Let's substitute (\\ref{eq:poltor}) into the Stokes' equation for a constant viscosity fluid \\begin{equation} \\eta \\nabla^2 \\mathbf{u} - \\nabla p = g \\rho \\hat{\\mathbf{z}} \\label{eq:cvstokes} \\end{equation} where \\(\\hat{\\mathbf{z}}\\) is the vertical unit vector (defined by the direction of gravity) and is clearly the one identifiable special direction, then equate coefficients in the \\(\\hat{\\mathbf{z}}\\) direction, and using the following results: \\begin{equation} \\nonumber \\hat{\\mathbf{z}} \\cdot \\nabla \\times \\nabla^2 \\mathbf{u} = - \\nabla^2 \\nabla_h^2\\Psi \\end{equation} \\begin{equation} \\nonumber \\hat{\\mathbf{z}} \\cdot \\nabla \\times \\nabla \\times \\nabla^2 \\mathbf{u} = \\nabla^2 \\nabla^2 \\nabla_h^2\\Phi \\end{equation} where \\begin{equation} \\nonumber \\nabla_h = \\left( \\frac{\\partial}{\\partial x}, \\frac{\\partial}{\\partial y}, 0 \\right) \\end{equation} is a gradient operator limited to the plane perpendicular to the special direction, \\hat{\\mathbf{z}} \\hat{\\mathbf{z}} . If we first take the curl of (\\ref{eq:cvstokes}), and look at the \\hat{\\mathbf{z}} \\hat{\\mathbf{z}} direction, \\begin{equation} \\nonumber \\hat{\\mathbf{z}} \\cdot \\eta \\nabla \\times \\nabla^2 \\mathbf{u} = -\\eta \\nabla^2 \\nabla_h^2\\Psi = \\hat{\\mathbf{z}} \\cdot \\left( g \\nabla \\times \\left( \\rho \\hat{\\mathbf{z}}\\right)\\right) = 0 \\end{equation} we see that there is no contribution of the toroidal velocity field to the force balance. This balance occurs entirely through the poloidal part of the velocity field. If we take the curl twice and, once again, look at the \\hat{\\mathbf{z}} \\hat{\\mathbf{z}} direction: \\begin{equation} \\nonumber \\hat{\\mathbf{z}} \\cdot \\eta \\nabla \\times \\nabla \\times \\nabla^2 \\mathbf{u} = \\eta \\nabla^2 \\nabla^2 \\nabla_h^2 \\Phi = \\hat{\\mathbf{z}} \\cdot g \\nabla \\times \\nabla \\times \\left( \\rho \\hat{\\mathbf{z}}\\right) = \\nabla_h^2 (\\rho g) \\end{equation} Which is the 3D equivalent of the biharmonic equation that we derived above. Note: if the viscosity varies in the \\(\\hat{\\mathbf{z}}\\) direction, then this same decoupling still applies: bouyancy forces do not drive any toroidal flow. Lateral variations in viscosity (perpendicular to \\(\\hat{\\mathbf{z}}\\) ) couple the buoyancy to toroidal motion. This result is general in that it applies to the spherical geometry equally well assuming the radial direction (of gravity) to be special.","title":"Poloidal/Toroidal velocity decomposition"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html","text":"\\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\] Thermal Convection Thermal convection describes the a process in which a fluid organizes itself into a structured flow pattern on a macroscopic scale to transport energy. Convection may be mechanically driven by stirring, but more commonly we refer to natural convection in which buoyancy due to a source of heat (and/or compositional variation) induces flow which transports and dissipates this anomalous buoyancy. The Earth's interior, on a geological timescale is a highly viscous fluid which is heated from below by heat escaping from the core, and internally by the decay of radioactive elements. In this respect Critical Rayleigh Number for a layer Does convection always occur in a layer heated from below ? In principle this would always provide a way to transport additional heat, but how much work would convection have to do in order to transport this extra heat ? One way to determine the answer is to consider small disturbances to a layer with otherwise uniform temperature and see under what conditions the perturbations grow (presumably into fully developed convection). This approach allows us to make {\\em linear} approximations to the otherwise non-linear equations by dropping the small, high order non-linear terms. We solve the incompressible flow equations (stream function form, \\ref{eq:biharm}) and energy conservation equation in stream function form: \\begin{equation} \\nonumber \\frac{\\partial T}{\\partial t} + \\left[ -\\frac{\\partial \\psi}{\\partial x_2}\\frac{\\partial T}{\\partial x_1} +\\frac {\\partial \\psi}{\\partial x_1}\\frac{\\partial T}{\\partial x_2} \\right] = \\nabla^2 T \\end{equation} By substituting throughout for a temperature which is a conductive profile with a small amplitude disturbance, \\(\\theta \\ll 1\\) \\begin{equation} \\nonumber T = 1- x_2 + \\theta \\end{equation} Remember that the equations are non-dimensional so that the layer depth is one, and the temperature drop is one. The advection term \\begin{equation} \\nonumber -\\frac{\\partial \\psi}{\\partial x_2}\\frac{\\partial T}{\\partial x_1} +\\frac {\\partial \\psi}{\\partial x_1}\\frac{\\partial T}{\\partial x_2} \\rightarrow -\\frac{\\partial \\psi}{\\partial x_2}\\frac{\\partial \\theta}{\\partial x_1} -\\frac{\\partial \\psi}{\\partial x_1} +\\frac {\\partial \\theta}{\\partial x_2}\\frac{\\partial \\psi}{\\partial x_1} \\end{equation} is dominated by the \\( \\partial \\psi / \\partial x_1 \\) since all others are the product of small terms. (Since we also know that \\(\\psi \\sim \\theta\\) from equation (\\ref{eq:biharm})). Therefore the energy conservation equation becomes \\begin{equation} \\nonumber \\frac{\\partial \\theta}{\\partial t} - \\frac{\\partial \\psi}{\\partial x_1} = \\nabla^2 \\theta \\end{equation} which is linear. Boundary conditions for this problem are zero normal velocity on \\(x_2 = 0,1\\) which implies \\( \\psi=0 \\) at these boundaries. The form of the perturbation is such that \\(\\theta =0 \\) on \\( x_2 = 0,1 \\), and we allow free slip along these boundaries such that \\begin{equation} \\nonumber \\sigma_{12} = \\frac{\\partial v_1}{\\partial x_2} + \\frac{\\partial v_2}{\\partial x_1} =0 \\end{equation} when \\(x_2 = 0,1\\) which implies \\(\\nabla^2 \\psi =0\\) there. Now introduce small harmonic perturbations to the driving terms and assuming a similar (i.e. harmonic) response in the flow. This takes the form \\begin{equation} \\nonumber \\begin{split} \\theta &= \\Theta(x_2) \\exp(\\sigma t) \\sin kx_1 \\\\ \\psi &= \\Psi(x_2) \\exp(\\sigma t) \\cos kx_1 \\end{split} \\end{equation} So that we can now separate variables. \\(\\sigma\\) is unknown, however, if \\( \\sigma < 0 \\) then the perturbations will decay, whereas if \\( \\sigma > 0 \\) they will grow. {. width=\"50%\"} Critical Rayleigh Number determination. A plot of growth rates for harmonic perturbations as a function of wavenumber for different {\\rm Ra} {\\rm Ra} . The critical value occurs when the maximum of the curve just touches the horizontal axis at zero. Substituting for the perturbations into the biharmonic equation and the linearized energy conservation equation gives \\begin{equation} \\left(\\frac{d 2}{d{x_2} 2} -k^2 \\right)^2 \\Psi = -{\\rm Ra} k \\Theta \\label{eq:psitheta1} \\end{equation} and \\begin{equation} \\sigma \\Theta + k \\Psi = \\left(\\frac{d 2}{d{x_2} 2} -k^2 \\right) \\Theta \\end{equation} Here we have shown and used the fact that \\begin{equation} \\nabla^2 \\equiv \\left(\\frac{\\partial^2}{\\partial {x _ 2}^2} -k^2 \\right) \\label{eq:psitheta2} \\end{equation} when a function is expanded in the form \\(\\phi(x,z) = \\Phi(z).\\sin kx \\) - more generally, this is the fourier transform of the Laplacian operator. Eliminating \\( \\Psi \\) between (\\ref{eq:psitheta1}) and (\\ref{eq:psitheta2}) gives \\begin{equation} \\nonumber \\sigma \\left(\\frac{d^2}{d {x_2}^2 } - k^2 \\right)^2 -{\\rm Ra} k^2 \\Theta = \\left(\\frac{d^2}{d {x_2}^2} -k^2 \\right)^3 \\Theta \\end{equation} This has a solution \\begin{equation} \\nonumber \\Theta = \\Theta_0 \\sin \\pi z \\end{equation} which satisfies all the stated boundary conditions and implies \\begin{equation} \\nonumber \\sigma = \\frac{k^2 {\\rm Ra}}{(\\pi^2 + k 2) 2} -(\\pi^2 + k^2) \\end{equation} a real function of \\(k \\) and \\(\\rm Ra\\). For a given wavenumber, what is the lowest value of \\rm Ra \\rm Ra for which perturbations at that wavenumber will grow ? \\begin{equation} \\nonumber = \\frac{(\\pi^2 + k 2) 3}{k^2} \\end{equation} The absolute minimum value of {\\rm Ra} {\\rm Ra} which produces growing perturbations is found by differentiating \\({\\rm Ra_0} \\) with respect to \\(k \\) and setting equal to zero to find the extremum. \\begin{equation} \\nonumber {\\rm Ra_c} = \\frac{27}{4} \\pi^4 = 657.51 \\end{equation} for a wavenumber of \\begin{equation} \\nonumber k = \\frac{\\pi}{2^{1/2}} = 2.22 \\end{equation} corresponding to a wavelength of 2.828 times the depth of the layer. Different boundary conditions produce different values of the critical Rayleigh number. If no-slip conditions are used, for example, then the \\( \\Theta \\) solution applied above does not satisfy the boundary conditions. In general, the critical Rayleigh number lies between about 100 and 3000. Boundary layer theory, Boundary Rayleigh Number Having determined the conditions under which convection will develop, we next consider what can be calculated about fully developed convection - i.e. when perturbations grow well beyond the linearization used to study the onset of instability. Let's consider fully developed convection with high Rayleigh number. From observations of real fluids in laboratory situations, it is well known how this looks. High Rayleigh number convection is dominated by the advection of heat. Diffusion is too slow to carry heat far into the fluid before the buoyancy anomaly becomes unstable. This leads to thin, horizontal boundary layers where diffusive heat transfer into and out of the fluid occurs. These are separated by approximately isothermal regions in the fluid interior. The horizontal boundary layers are connected by vertical boundary layers which take the form of sheets or cylindrical plumes depending on a number of things including the Rayleigh number. For the time being we consider only the sheet like downwellings since that allows us to continue working in 2D. {. width=\"75%\"} Boundary Layer Theory in its simplest form: assumes that the boundary layers are of constant thickness and the interior of the cell rotates as a passive lump Boundary layer analysis is a highly sophisticated field, and is used in a broad range of situations where differences in scales between different physical effects produce narrow accommodation zones where the weaker term dominates (e.g viscosity in an otherwise invicid flow around an obstacle). Here we first make a wild stab at an approximate theory describing the heat flow from a layer with a given Rayleigh number. The convective flow is shown in the Figure together with a rough sketch of what actually happens. Assuming the simplified flow pattern of the sketch, steady state, and replacing all derivatives by crude differences we obtain (using a vorticity form) \\begin{equation} \\nonumber \\kappa \\nabla^2 T = (\\mathbf{v} \\cdot \\nabla) T \\;\\;\\; \\longrightarrow \\;\\;\\; \\frac{v \\Delta T}{d} \\sim \\frac{\\Delta T \\kappa}{\\delta^2} \\end{equation} and \\begin{equation} \\nonumber \\nabla^2 \\omega = \\frac{g \\rho \\alpha}{\\eta} \\frac{\\partial T}{\\partial x} \\;\\;\\; \\longrightarrow \\;\\;\\; \\frac{\\omega}{\\delta ^2} \\sim \\frac{g \\rho \\alpha \\Delta T}{\\eta \\delta} \\end{equation} where \\(\\omega\\sim v / d \\) from the local rotation interpretation of vorticity and the approximate rigid-body rotation of the core of the convection cell, and \\(v/d \\sim \\kappa / \\delta^2\\). This gives \\begin{align} \\frac{\\delta}{d} & \\sim {\\rm Ra}^{-1/3} \\\\ v & \\sim \\frac{\\kappa}{d} {\\rm Ra}^{2/3} \\end{align} This theory balances diffusion of vorticity and temperature across and out of the boundary layer with advection of each quantity along the boundary layer to maintain a steady state.The Nusselt number is the ratio of advected heat transport to that purely conducted in the absence of fluid motion, or, using the above approximations, \\begin{equation} \\nonumber \\begin{split} {\\rm Nu} & \\sim \\frac{\\rho C_p v \\Delta T \\delta}{(k \\Delta T/d)d} \\\\ & \\sim {\\rm Ra}^{1/3} \\end{split} \\end{equation} This latter result being observed in experiments to be in reasonably good agreement with observation. If we define a boundary Rayleigh number \\begin{equation} \\nonumber {\\rm Ra_b} = \\frac{g \\rho \\alpha \\Delta T \\delta^3}{\\kappa \\eta} \\end{equation} then the expression for \\(\\delta\\) gives \\begin{equation} \\nonumber {\\rm Ra_b} \\sim 1 \\end{equation} so the boundary layer does not become more or less stable with increasing Rayleigh number (this is not universal -- for internal heating the boundary layer becomes less stable at higher Rayleigh number). {. width=\"75%\"} Boundary Layer Theory which accounts for the thickness variations along the boundary layer as it cools away from the upwelling Another wrinkle can be added to the boundary layer theory by trying to account for the variation in the boundary layer thickness as it moves along the horizontal boundary. This refinement in the theory can account for the form of this thickness, the potential energy change in rising or sinking plumes, and the aspect ratio of the convection (width to height of convection roll) by maximizing Nusselt number as a function of aspect ratio. Consider the boundary layer to be very thin above the upwelling plume (left side). As it moves to the right, it cools and the depth to any particular isotherm increases (this is clearly seen in the simulation). This can be treated exactly like a one dimensional problem if we work in the Lagrangian frame of reference attached to the boundary layer. That is, take the 1D half-space cooling model and replace the time with \\(x_1/v\\) (cf. the advection equation in which time and velocity / lengths are mixed). The standard solution is as follows. Assume a half-space at an intial temperature everywhere of \\(T_0 \\) to which a boundary condition, \\(T=T_s\\) is applied at \\(t=0,x_2=0\\). We solve for \\(T(x_2,t)\\) by first making a substitution, \\begin{equation} \\nonumber \\theta = \\frac{T-T_0}{T_s-T_0} \\end{equation} which is a dimensionless temperature, into the standard diffusion equation to obtain \\begin{equation} \\frac{\\partial \\theta(x_2,t)}{\\partial t} = \\kappa \\frac{\\partial ^2 \\theta(x_2,t)}{\\partial {x_2}^2} \\label{eq:difftheta} \\end{equation} The boundary conditions on \\(\\theta\\) are simple: \\begin{align} & \\theta(x_2,0) = 0 \\\\ & \\theta(0,t) = 1 \\\\ & \\theta(\\infty,0) = 0 \\end{align} {. width=\"75%\"} Cooling half-space calculation for a range of times (here everything is scaled to 1 In place of \\(t,x_2\\), we use the similarity transformation, \\begin{equation} \\nonumber \\eta = \\frac{x_2}{2\\sqrt{\\kappa t}} \\end{equation} which is found (more or less) intuitively. Now we need to substitute \\begin{align} \\frac{\\partial \\theta}{\\partial t} & = -\\frac{d \\theta}{d\\eta}(\\eta/2t) \\ \\frac{\\partial^2 \\theta}{\\partial {x_2}^2} & = \\frac{1}{4\\kappa t}\\frac{d^2 \\theta}{d \\eta^2} \\end{align} to transform \\( (\\ref{eq:difftheta}) \\) into \\begin{equation} -\\eta \\frac{d \\theta}{d\\eta} = \\frac{1}{2} \\frac{d^2 \\theta}{d \\eta^2} \\label{eq:diffode} \\end{equation} Boundary conditions transform to give \\begin{align} & \\theta(\\eta=\\infty) = 0 \\ & \\theta(\\eta=0) = 1 \\end{align} Write \\(\\phi = d\\theta / d\\eta\\) (for convenience only) to rewrite \\( (\\ref{eq:diffode}) \\) as \\begin{align} -\\eta \\phi &= \\frac{1}{2} \\frac{d \\phi}{d \\eta} \\\\ \\text{or} -\\eta d\\eta &= \\frac{1}{2} \\frac{d\\phi}{\\phi} \\\\end{align} \\begin{align} -\\eta \\phi &= \\frac{1}{2} \\frac{d \\phi}{d \\eta} \\\\ \\text{or} -\\eta d\\eta &= \\frac{1}{2} \\frac{d\\phi}{\\phi} \\\\end{align} This is a standard integral with solution \\begin{align} & -\\eta^2 = \\log_e \\phi -\\log_e c_1 \\ \\text{such that} & \\phi = c_1 \\exp(-\\eta^2) = \\frac{d\\theta}{d\\eta} \\end{align} This latter form is then integrated to give the solution: \\begin{equation} \\nonumber \\theta = c_1 \\int_0^\\eta \\exp(-{\\eta'}^2) d\\eta' +1 \\end{equation} Boundary conditions give \\begin{equation} \\nonumber \\theta = 1- \\frac{2}{\\sqrt{\\pi}} \\int_0 \\eta\\exp(-{\\eta'} 2) d\\eta' \\end{equation} Which is the definition of the complementary error function ( \\(\\erfc(\\eta)\\)). Undoing the remaining substitutions gives \\begin{equation} \\nonumber \\frac{T-T_0}{T_s-T_0} = \\erfc \\left( \\frac{x_2}{2\\sqrt{\\kappa t}} \\right) \\end{equation} In our original context of the cooling boundary layer, then, \\(T _ s\\) is the surface temperature, \\(T_0$\\) is the interior temperature of the convection cell ( \\(\\Delta T /2 \\) ) and \\(t \\leftarrow x_1/v \\). The thickness of the boundary layer is found by assuming it is defined by a characteristic isotherm (doesn't much matter which). The progression of this isotherm is \\begin{equation} \\nonumber \\delta \\propto \\sqrt{\\kappa t} \\end{equation} or, in the Eulerian frame, \\begin{equation} \\nonumber \\delta \\propto \\sqrt{\\kappa x_1 / v} \\end{equation} Internal Heating The definition of the Rayleigh number when the layer is significantly internally heated is \\begin{equation} \\nonumber {\\rm Ra} = \\frac{g \\rho^2 \\alpha H d^5}{\\eta \\kappa k} \\end{equation} where \\(H \\) is the rate of internal heat generation per unit mass. The definition of the Nusselt number is the heat flow through the upper surface divided by the average basal temperature. This allows a Nusselt number to be calculated for internally heated convection where the basal temperature is not known a priori . Internally heated convection is a problem to simulate in the lab, directly, but the same effect is achieved by reducing the temperature of the upper surface as a function of time. When Viscosity is not Constant When viscosity is not constant (and for rocks, the strong dependence of viscosity on temperature makes this generally the case), the equations are quite a lot more complicated. It is no longer possible to form the biharmonic equation since \\(\\eta(x,z)\\) cannot be taken outside the differential operators. Nor can stream-function / vorticity formulations be used directly for the same reasons. Spectral methods \u2014 the decomposition of the problem into a sum of independent problems in the wavenumber domain \u2014 is no longer simple since the individual problems are coupled, not independent. The Rayleigh number is no longer uniquely defined for the system since the viscosity to which it refers must take some suitable average over the layer \u2014 the nature of this average depends on the circumstances. The form of convection changes since boundary layers at the top and bottom of the system (cold v hot) are no longer symmetric with each other. The convecting system gains another control parameter which is a measure of the viscosity contrast as a function of temperature. Applications to the Earth The application of realistic convection models to the Earth and other planets \u2014 particularly Venus.. The simplest computational and boundary layer solutions to the Stokes' convection equations made the simplifying assumption that the viscosity was constant. Despite the experimental evidence which suggests viscosity variations should dominate in the Earth, agreement with some important observations was remarkably good. Such simulations were not able to produce plate-like motions at the surface (instead producing smoothly distributed deformation) but the average velocity, the heat flow and the observed pattern of subsidence of the ocean floor were well matched. Mantle Rheology Experimental determination of the rheology of mantle materials gives \\begin{equation} \\nonumber \\dot{\\epsilon} \\propto \\sigma^n d^{-m} \\exp\\left( -\\frac{E+PV}{RT} \\right) \\end{equation} where \\(\\sigma\\) is a stress, \\(d \\) is grain size, \\(E \\)is an activation energy, \\(V \\) is an activation volume, and \\(T\\) is absolute temperature. ( \\(R \\) is the universal gas constant). This translates to a viscosity \\begin{equation} \\nonumber \\eta \\propto \\sigma^{1-n} d^m exp\\left( \\frac{E+PV}{RT} \\right) \\end{equation} In the mantle two forms of creep are dominant: dislocation creep with \\(n ~ 3.0\\), \\(m~0\\), \\(E ~ 430-540 KJ/mol\\), \\(V ~ 10 - 20 cm^3/mol \\); and diffusion creep with \\(n ~ 1.0 \\), \\(m~2.5\\), \\(E ~ 240-300 KJ/mol\\), \\(V ~ 5-6 cm^3/mol\\). This is for olivine --- other minerals will produce different results, of course. Convection with Temperature Dependent Viscosity More sophisticated models included the effect of temperature dependent viscosity as a step towards more realistic simulations. In fact, the opposite was observed: convection with temperature dependent viscosity is a much worse description of the oceanic lithosphere than constant viscosity convection. It may, however, describe Venus rather well. Theoretical studies of the asymptotic limit of convection in which the viscosity variation becomes very large (comparable to values determined for mantle rocks in laboratory experiments) find that the upper surface becomes entirely stagnant with little or no observable motion. Vigorous convection continues underneath the stagnant layer with very little surface manifestation. This theoretical work demonstrates that the numerical simulations are producing correct results, and suggests that we should look for physics beyond pure viscous flow in explaining plate motions. Non-linear Viscosity and Brittle Effects Realistic rheological laws show the viscosity may depend upon stress. This makes the problem non-linear since the stress clearly depends upon viscosity. In order to obtain a solution it is necessary to iterate velocity and viscosity until they no longer change. The obvious association of plate boundaries with earthquake activity suggests that relevant effects are to be found in the brittle nature of the cold plates. Brittle materials have a finite strength and if they are stressed beyond that point they break. This is a familiar enough property of everyday materials, but rocks in the lithosphere are non-uniform, subject to great confining pressures and high temperatures, and they deform over extremely long periods of time. This makes it difficult to know how to apply laboratory results for rock breakage experiments to simulations of the plates. An ideal, very general rheological model for the brittle lithosphere would incorporate the effects due to small-scale cracks, faults, ductile shear localization due to dynamic recrystalization, anisotropy (... kitchen sink). Needless to say, most attempts to date to account for the brittle nature of the plates have greatly simplified the picture. Some models have imposed weak zones which represent plate boundaries, others have included sharp discontinuities which represent the plate-bounding faults, still others have used continuum methods in which the yield properties of the lithosphere are known but not the geometry of any breaks. Of these approaches, the continuum approach is best able to demonstrate the spectrum of behaviours as convection in the mantle interacts with brittle lithospheric plates. For studying the evolution of individual plate boundaries methods which explicitly include discontinuities work best. The simplest possible continuum formulation includes a yield stress expressed as an non-linear effective viscosity. \\begin{equation} \\nonumber \\eta _ {\\rm eff} = \\frac{\\tau _ {\\rm yield}}{\\dot{\\varepsilon}} \\end{equation} This formulation can be incorporated very easily into the mantle dynamics modeling approach that we have outlined above as it involves making modifications only to the viscosity law. There may be some numerical difficulties, however, as the strongly non-linear rheology can lead to dramatic variations in the viscosity across relatively narrow zones. Thermochemical Convection The Rayleigh number is defined in terms of thermal buoyancy but other sources of buoyancy are possible in fluids. For example, in the oceans, dissolved salt makes water heavy. When hot salty water (e.g. the outflows of shallow seas such as the Mediterranean) mixes with cold less salty water, there is a complicated interaction. This is double diffusive convection and produces remarkable layerings etc since the diffusion coefficients of salt and heat are different by a factor of 100. In the mantle, bulk chemical differences due to subduction of crustal material can be treated in a similar manner. From the point of view of the diffusion equations, the diffusivity of bulk chemistry in the mantle is tiny (pure advection). Fluid flows with chemical v. thermal bouyancy are termed thermochemical convection problems.","title":"MathPhysicsBackground 3"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html#thermal-convection","text":"Thermal convection describes the a process in which a fluid organizes itself into a structured flow pattern on a macroscopic scale to transport energy. Convection may be mechanically driven by stirring, but more commonly we refer to natural convection in which buoyancy due to a source of heat (and/or compositional variation) induces flow which transports and dissipates this anomalous buoyancy. The Earth's interior, on a geological timescale is a highly viscous fluid which is heated from below by heat escaping from the core, and internally by the decay of radioactive elements. In this respect","title":"Thermal Convection"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html#critical-rayleigh-number-for-a-layer","text":"Does convection always occur in a layer heated from below ? In principle this would always provide a way to transport additional heat, but how much work would convection have to do in order to transport this extra heat ? One way to determine the answer is to consider small disturbances to a layer with otherwise uniform temperature and see under what conditions the perturbations grow (presumably into fully developed convection). This approach allows us to make {\\em linear} approximations to the otherwise non-linear equations by dropping the small, high order non-linear terms. We solve the incompressible flow equations (stream function form, \\ref{eq:biharm}) and energy conservation equation in stream function form: \\begin{equation} \\nonumber \\frac{\\partial T}{\\partial t} + \\left[ -\\frac{\\partial \\psi}{\\partial x_2}\\frac{\\partial T}{\\partial x_1} +\\frac {\\partial \\psi}{\\partial x_1}\\frac{\\partial T}{\\partial x_2} \\right] = \\nabla^2 T \\end{equation} By substituting throughout for a temperature which is a conductive profile with a small amplitude disturbance, \\(\\theta \\ll 1\\) \\begin{equation} \\nonumber T = 1- x_2 + \\theta \\end{equation} Remember that the equations are non-dimensional so that the layer depth is one, and the temperature drop is one. The advection term \\begin{equation} \\nonumber -\\frac{\\partial \\psi}{\\partial x_2}\\frac{\\partial T}{\\partial x_1} +\\frac {\\partial \\psi}{\\partial x_1}\\frac{\\partial T}{\\partial x_2} \\rightarrow -\\frac{\\partial \\psi}{\\partial x_2}\\frac{\\partial \\theta}{\\partial x_1} -\\frac{\\partial \\psi}{\\partial x_1} +\\frac {\\partial \\theta}{\\partial x_2}\\frac{\\partial \\psi}{\\partial x_1} \\end{equation} is dominated by the \\( \\partial \\psi / \\partial x_1 \\) since all others are the product of small terms. (Since we also know that \\(\\psi \\sim \\theta\\) from equation (\\ref{eq:biharm})). Therefore the energy conservation equation becomes \\begin{equation} \\nonumber \\frac{\\partial \\theta}{\\partial t} - \\frac{\\partial \\psi}{\\partial x_1} = \\nabla^2 \\theta \\end{equation} which is linear. Boundary conditions for this problem are zero normal velocity on \\(x_2 = 0,1\\) which implies \\( \\psi=0 \\) at these boundaries. The form of the perturbation is such that \\(\\theta =0 \\) on \\( x_2 = 0,1 \\), and we allow free slip along these boundaries such that \\begin{equation} \\nonumber \\sigma_{12} = \\frac{\\partial v_1}{\\partial x_2} + \\frac{\\partial v_2}{\\partial x_1} =0 \\end{equation} when \\(x_2 = 0,1\\) which implies \\(\\nabla^2 \\psi =0\\) there. Now introduce small harmonic perturbations to the driving terms and assuming a similar (i.e. harmonic) response in the flow. This takes the form \\begin{equation} \\nonumber \\begin{split} \\theta &= \\Theta(x_2) \\exp(\\sigma t) \\sin kx_1 \\\\ \\psi &= \\Psi(x_2) \\exp(\\sigma t) \\cos kx_1 \\end{split} \\end{equation} So that we can now separate variables. \\(\\sigma\\) is unknown, however, if \\( \\sigma < 0 \\) then the perturbations will decay, whereas if \\( \\sigma > 0 \\) they will grow. {. width=\"50%\"} Critical Rayleigh Number determination. A plot of growth rates for harmonic perturbations as a function of wavenumber for different {\\rm Ra} {\\rm Ra} . The critical value occurs when the maximum of the curve just touches the horizontal axis at zero. Substituting for the perturbations into the biharmonic equation and the linearized energy conservation equation gives \\begin{equation} \\left(\\frac{d 2}{d{x_2} 2} -k^2 \\right)^2 \\Psi = -{\\rm Ra} k \\Theta \\label{eq:psitheta1} \\end{equation} and \\begin{equation} \\sigma \\Theta + k \\Psi = \\left(\\frac{d 2}{d{x_2} 2} -k^2 \\right) \\Theta \\end{equation} Here we have shown and used the fact that \\begin{equation} \\nabla^2 \\equiv \\left(\\frac{\\partial^2}{\\partial {x _ 2}^2} -k^2 \\right) \\label{eq:psitheta2} \\end{equation} when a function is expanded in the form \\(\\phi(x,z) = \\Phi(z).\\sin kx \\) - more generally, this is the fourier transform of the Laplacian operator. Eliminating \\( \\Psi \\) between (\\ref{eq:psitheta1}) and (\\ref{eq:psitheta2}) gives \\begin{equation} \\nonumber \\sigma \\left(\\frac{d^2}{d {x_2}^2 } - k^2 \\right)^2 -{\\rm Ra} k^2 \\Theta = \\left(\\frac{d^2}{d {x_2}^2} -k^2 \\right)^3 \\Theta \\end{equation} This has a solution \\begin{equation} \\nonumber \\Theta = \\Theta_0 \\sin \\pi z \\end{equation} which satisfies all the stated boundary conditions and implies \\begin{equation} \\nonumber \\sigma = \\frac{k^2 {\\rm Ra}}{(\\pi^2 + k 2) 2} -(\\pi^2 + k^2) \\end{equation} a real function of \\(k \\) and \\(\\rm Ra\\). For a given wavenumber, what is the lowest value of \\rm Ra \\rm Ra for which perturbations at that wavenumber will grow ? \\begin{equation} \\nonumber = \\frac{(\\pi^2 + k 2) 3}{k^2} \\end{equation} The absolute minimum value of {\\rm Ra} {\\rm Ra} which produces growing perturbations is found by differentiating \\({\\rm Ra_0} \\) with respect to \\(k \\) and setting equal to zero to find the extremum. \\begin{equation} \\nonumber {\\rm Ra_c} = \\frac{27}{4} \\pi^4 = 657.51 \\end{equation} for a wavenumber of \\begin{equation} \\nonumber k = \\frac{\\pi}{2^{1/2}} = 2.22 \\end{equation} corresponding to a wavelength of 2.828 times the depth of the layer. Different boundary conditions produce different values of the critical Rayleigh number. If no-slip conditions are used, for example, then the \\( \\Theta \\) solution applied above does not satisfy the boundary conditions. In general, the critical Rayleigh number lies between about 100 and 3000.","title":"Critical Rayleigh Number for a layer"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html#boundary-layer-theory-boundary-rayleigh-number","text":"Having determined the conditions under which convection will develop, we next consider what can be calculated about fully developed convection - i.e. when perturbations grow well beyond the linearization used to study the onset of instability. Let's consider fully developed convection with high Rayleigh number. From observations of real fluids in laboratory situations, it is well known how this looks. High Rayleigh number convection is dominated by the advection of heat. Diffusion is too slow to carry heat far into the fluid before the buoyancy anomaly becomes unstable. This leads to thin, horizontal boundary layers where diffusive heat transfer into and out of the fluid occurs. These are separated by approximately isothermal regions in the fluid interior. The horizontal boundary layers are connected by vertical boundary layers which take the form of sheets or cylindrical plumes depending on a number of things including the Rayleigh number. For the time being we consider only the sheet like downwellings since that allows us to continue working in 2D. {. width=\"75%\"} Boundary Layer Theory in its simplest form: assumes that the boundary layers are of constant thickness and the interior of the cell rotates as a passive lump Boundary layer analysis is a highly sophisticated field, and is used in a broad range of situations where differences in scales between different physical effects produce narrow accommodation zones where the weaker term dominates (e.g viscosity in an otherwise invicid flow around an obstacle). Here we first make a wild stab at an approximate theory describing the heat flow from a layer with a given Rayleigh number. The convective flow is shown in the Figure together with a rough sketch of what actually happens. Assuming the simplified flow pattern of the sketch, steady state, and replacing all derivatives by crude differences we obtain (using a vorticity form) \\begin{equation} \\nonumber \\kappa \\nabla^2 T = (\\mathbf{v} \\cdot \\nabla) T \\;\\;\\; \\longrightarrow \\;\\;\\; \\frac{v \\Delta T}{d} \\sim \\frac{\\Delta T \\kappa}{\\delta^2} \\end{equation} and \\begin{equation} \\nonumber \\nabla^2 \\omega = \\frac{g \\rho \\alpha}{\\eta} \\frac{\\partial T}{\\partial x} \\;\\;\\; \\longrightarrow \\;\\;\\; \\frac{\\omega}{\\delta ^2} \\sim \\frac{g \\rho \\alpha \\Delta T}{\\eta \\delta} \\end{equation} where \\(\\omega\\sim v / d \\) from the local rotation interpretation of vorticity and the approximate rigid-body rotation of the core of the convection cell, and \\(v/d \\sim \\kappa / \\delta^2\\). This gives \\begin{align} \\frac{\\delta}{d} & \\sim {\\rm Ra}^{-1/3} \\\\ v & \\sim \\frac{\\kappa}{d} {\\rm Ra}^{2/3} \\end{align} This theory balances diffusion of vorticity and temperature across and out of the boundary layer with advection of each quantity along the boundary layer to maintain a steady state.The Nusselt number is the ratio of advected heat transport to that purely conducted in the absence of fluid motion, or, using the above approximations, \\begin{equation} \\nonumber \\begin{split} {\\rm Nu} & \\sim \\frac{\\rho C_p v \\Delta T \\delta}{(k \\Delta T/d)d} \\\\ & \\sim {\\rm Ra}^{1/3} \\end{split} \\end{equation} This latter result being observed in experiments to be in reasonably good agreement with observation. If we define a boundary Rayleigh number \\begin{equation} \\nonumber {\\rm Ra_b} = \\frac{g \\rho \\alpha \\Delta T \\delta^3}{\\kappa \\eta} \\end{equation} then the expression for \\(\\delta\\) gives \\begin{equation} \\nonumber {\\rm Ra_b} \\sim 1 \\end{equation} so the boundary layer does not become more or less stable with increasing Rayleigh number (this is not universal -- for internal heating the boundary layer becomes less stable at higher Rayleigh number). {. width=\"75%\"} Boundary Layer Theory which accounts for the thickness variations along the boundary layer as it cools away from the upwelling Another wrinkle can be added to the boundary layer theory by trying to account for the variation in the boundary layer thickness as it moves along the horizontal boundary. This refinement in the theory can account for the form of this thickness, the potential energy change in rising or sinking plumes, and the aspect ratio of the convection (width to height of convection roll) by maximizing Nusselt number as a function of aspect ratio. Consider the boundary layer to be very thin above the upwelling plume (left side). As it moves to the right, it cools and the depth to any particular isotherm increases (this is clearly seen in the simulation). This can be treated exactly like a one dimensional problem if we work in the Lagrangian frame of reference attached to the boundary layer. That is, take the 1D half-space cooling model and replace the time with \\(x_1/v\\) (cf. the advection equation in which time and velocity / lengths are mixed). The standard solution is as follows. Assume a half-space at an intial temperature everywhere of \\(T_0 \\) to which a boundary condition, \\(T=T_s\\) is applied at \\(t=0,x_2=0\\). We solve for \\(T(x_2,t)\\) by first making a substitution, \\begin{equation} \\nonumber \\theta = \\frac{T-T_0}{T_s-T_0} \\end{equation} which is a dimensionless temperature, into the standard diffusion equation to obtain \\begin{equation} \\frac{\\partial \\theta(x_2,t)}{\\partial t} = \\kappa \\frac{\\partial ^2 \\theta(x_2,t)}{\\partial {x_2}^2} \\label{eq:difftheta} \\end{equation} The boundary conditions on \\(\\theta\\) are simple: \\begin{align} & \\theta(x_2,0) = 0 \\\\ & \\theta(0,t) = 1 \\\\ & \\theta(\\infty,0) = 0 \\end{align} {. width=\"75%\"} Cooling half-space calculation for a range of times (here everything is scaled to 1 In place of \\(t,x_2\\), we use the similarity transformation, \\begin{equation} \\nonumber \\eta = \\frac{x_2}{2\\sqrt{\\kappa t}} \\end{equation} which is found (more or less) intuitively. Now we need to substitute \\begin{align} \\frac{\\partial \\theta}{\\partial t} & = -\\frac{d \\theta}{d\\eta}(\\eta/2t) \\ \\frac{\\partial^2 \\theta}{\\partial {x_2}^2} & = \\frac{1}{4\\kappa t}\\frac{d^2 \\theta}{d \\eta^2} \\end{align} to transform \\( (\\ref{eq:difftheta}) \\) into \\begin{equation} -\\eta \\frac{d \\theta}{d\\eta} = \\frac{1}{2} \\frac{d^2 \\theta}{d \\eta^2} \\label{eq:diffode} \\end{equation} Boundary conditions transform to give \\begin{align} & \\theta(\\eta=\\infty) = 0 \\ & \\theta(\\eta=0) = 1 \\end{align} Write \\(\\phi = d\\theta / d\\eta\\) (for convenience only) to rewrite \\( (\\ref{eq:diffode}) \\) as \\begin{align} -\\eta \\phi &= \\frac{1}{2} \\frac{d \\phi}{d \\eta} \\\\ \\text{or} -\\eta d\\eta &= \\frac{1}{2} \\frac{d\\phi}{\\phi} \\\\end{align} \\begin{align} -\\eta \\phi &= \\frac{1}{2} \\frac{d \\phi}{d \\eta} \\\\ \\text{or} -\\eta d\\eta &= \\frac{1}{2} \\frac{d\\phi}{\\phi} \\\\end{align} This is a standard integral with solution \\begin{align} & -\\eta^2 = \\log_e \\phi -\\log_e c_1 \\ \\text{such that} & \\phi = c_1 \\exp(-\\eta^2) = \\frac{d\\theta}{d\\eta} \\end{align} This latter form is then integrated to give the solution: \\begin{equation} \\nonumber \\theta = c_1 \\int_0^\\eta \\exp(-{\\eta'}^2) d\\eta' +1 \\end{equation} Boundary conditions give \\begin{equation} \\nonumber \\theta = 1- \\frac{2}{\\sqrt{\\pi}} \\int_0 \\eta\\exp(-{\\eta'} 2) d\\eta' \\end{equation} Which is the definition of the complementary error function ( \\(\\erfc(\\eta)\\)). Undoing the remaining substitutions gives \\begin{equation} \\nonumber \\frac{T-T_0}{T_s-T_0} = \\erfc \\left( \\frac{x_2}{2\\sqrt{\\kappa t}} \\right) \\end{equation} In our original context of the cooling boundary layer, then, \\(T _ s\\) is the surface temperature, \\(T_0$\\) is the interior temperature of the convection cell ( \\(\\Delta T /2 \\) ) and \\(t \\leftarrow x_1/v \\). The thickness of the boundary layer is found by assuming it is defined by a characteristic isotherm (doesn't much matter which). The progression of this isotherm is \\begin{equation} \\nonumber \\delta \\propto \\sqrt{\\kappa t} \\end{equation} or, in the Eulerian frame, \\begin{equation} \\nonumber \\delta \\propto \\sqrt{\\kappa x_1 / v} \\end{equation}","title":"Boundary layer theory, Boundary Rayleigh Number"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html#internal-heating","text":"The definition of the Rayleigh number when the layer is significantly internally heated is \\begin{equation} \\nonumber {\\rm Ra} = \\frac{g \\rho^2 \\alpha H d^5}{\\eta \\kappa k} \\end{equation} where \\(H \\) is the rate of internal heat generation per unit mass. The definition of the Nusselt number is the heat flow through the upper surface divided by the average basal temperature. This allows a Nusselt number to be calculated for internally heated convection where the basal temperature is not known a priori . Internally heated convection is a problem to simulate in the lab, directly, but the same effect is achieved by reducing the temperature of the upper surface as a function of time.","title":"Internal Heating"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html#when-viscosity-is-not-constant","text":"When viscosity is not constant (and for rocks, the strong dependence of viscosity on temperature makes this generally the case), the equations are quite a lot more complicated. It is no longer possible to form the biharmonic equation since \\(\\eta(x,z)\\) cannot be taken outside the differential operators. Nor can stream-function / vorticity formulations be used directly for the same reasons. Spectral methods \u2014 the decomposition of the problem into a sum of independent problems in the wavenumber domain \u2014 is no longer simple since the individual problems are coupled, not independent. The Rayleigh number is no longer uniquely defined for the system since the viscosity to which it refers must take some suitable average over the layer \u2014 the nature of this average depends on the circumstances. The form of convection changes since boundary layers at the top and bottom of the system (cold v hot) are no longer symmetric with each other. The convecting system gains another control parameter which is a measure of the viscosity contrast as a function of temperature.","title":"When Viscosity is not Constant"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html#applications-to-the-earth","text":"The application of realistic convection models to the Earth and other planets \u2014 particularly Venus.. The simplest computational and boundary layer solutions to the Stokes' convection equations made the simplifying assumption that the viscosity was constant. Despite the experimental evidence which suggests viscosity variations should dominate in the Earth, agreement with some important observations was remarkably good. Such simulations were not able to produce plate-like motions at the surface (instead producing smoothly distributed deformation) but the average velocity, the heat flow and the observed pattern of subsidence of the ocean floor were well matched.","title":"Applications to the Earth"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html#mantle-rheology","text":"Experimental determination of the rheology of mantle materials gives \\begin{equation} \\nonumber \\dot{\\epsilon} \\propto \\sigma^n d^{-m} \\exp\\left( -\\frac{E+PV}{RT} \\right) \\end{equation} where \\(\\sigma\\) is a stress, \\(d \\) is grain size, \\(E \\)is an activation energy, \\(V \\) is an activation volume, and \\(T\\) is absolute temperature. ( \\(R \\) is the universal gas constant). This translates to a viscosity \\begin{equation} \\nonumber \\eta \\propto \\sigma^{1-n} d^m exp\\left( \\frac{E+PV}{RT} \\right) \\end{equation} In the mantle two forms of creep are dominant: dislocation creep with \\(n ~ 3.0\\), \\(m~0\\), \\(E ~ 430-540 KJ/mol\\), \\(V ~ 10 - 20 cm^3/mol \\); and diffusion creep with \\(n ~ 1.0 \\), \\(m~2.5\\), \\(E ~ 240-300 KJ/mol\\), \\(V ~ 5-6 cm^3/mol\\). This is for olivine --- other minerals will produce different results, of course.","title":"Mantle Rheology"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html#convection-with-temperature-dependent-viscosity","text":"More sophisticated models included the effect of temperature dependent viscosity as a step towards more realistic simulations. In fact, the opposite was observed: convection with temperature dependent viscosity is a much worse description of the oceanic lithosphere than constant viscosity convection. It may, however, describe Venus rather well. Theoretical studies of the asymptotic limit of convection in which the viscosity variation becomes very large (comparable to values determined for mantle rocks in laboratory experiments) find that the upper surface becomes entirely stagnant with little or no observable motion. Vigorous convection continues underneath the stagnant layer with very little surface manifestation. This theoretical work demonstrates that the numerical simulations are producing correct results, and suggests that we should look for physics beyond pure viscous flow in explaining plate motions.","title":"Convection with Temperature Dependent Viscosity"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html#non-linear-viscosity-and-brittle-effects","text":"Realistic rheological laws show the viscosity may depend upon stress. This makes the problem non-linear since the stress clearly depends upon viscosity. In order to obtain a solution it is necessary to iterate velocity and viscosity until they no longer change. The obvious association of plate boundaries with earthquake activity suggests that relevant effects are to be found in the brittle nature of the cold plates. Brittle materials have a finite strength and if they are stressed beyond that point they break. This is a familiar enough property of everyday materials, but rocks in the lithosphere are non-uniform, subject to great confining pressures and high temperatures, and they deform over extremely long periods of time. This makes it difficult to know how to apply laboratory results for rock breakage experiments to simulations of the plates. An ideal, very general rheological model for the brittle lithosphere would incorporate the effects due to small-scale cracks, faults, ductile shear localization due to dynamic recrystalization, anisotropy (... kitchen sink). Needless to say, most attempts to date to account for the brittle nature of the plates have greatly simplified the picture. Some models have imposed weak zones which represent plate boundaries, others have included sharp discontinuities which represent the plate-bounding faults, still others have used continuum methods in which the yield properties of the lithosphere are known but not the geometry of any breaks. Of these approaches, the continuum approach is best able to demonstrate the spectrum of behaviours as convection in the mantle interacts with brittle lithospheric plates. For studying the evolution of individual plate boundaries methods which explicitly include discontinuities work best. The simplest possible continuum formulation includes a yield stress expressed as an non-linear effective viscosity. \\begin{equation} \\nonumber \\eta _ {\\rm eff} = \\frac{\\tau _ {\\rm yield}}{\\dot{\\varepsilon}} \\end{equation} This formulation can be incorporated very easily into the mantle dynamics modeling approach that we have outlined above as it involves making modifications only to the viscosity law. There may be some numerical difficulties, however, as the strongly non-linear rheology can lead to dramatic variations in the viscosity across relatively narrow zones.","title":"Non-linear Viscosity and Brittle Effects"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-3.html#thermochemical-convection","text":"The Rayleigh number is defined in terms of thermal buoyancy but other sources of buoyancy are possible in fluids. For example, in the oceans, dissolved salt makes water heavy. When hot salty water (e.g. the outflows of shallow seas such as the Mediterranean) mixes with cold less salty water, there is a complicated interaction. This is double diffusive convection and produces remarkable layerings etc since the diffusion coefficients of salt and heat are different by a factor of 100. In the mantle, bulk chemical differences due to subduction of crustal material can be treated in a similar manner. From the point of view of the diffusion equations, the diffusivity of bulk chemistry in the mantle is tiny (pure advection). Fluid flows with chemical v. thermal bouyancy are termed thermochemical convection problems.","title":"Thermochemical Convection"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-4.html","text":"\\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\] Rayleigh-Taylor Instability & Diapirism {. width=\"75%\" id=\"rayleigh-taylor\"} Salt diapirs result when a buried layer of salt(a,b) becomes convectively unstable and rises through the overlying sediment layers (c,d). The idealized geometry for the Rayleigh-Taylor instability problem is outlined in the lower diagram Diapirism is the buoyant upwelling of rock which is lighter than its surroundings. This can include mantle plumes and other purely thermal phenomena but it often applied to compositionally distinct rock masses such as melts infiltrating the crust (in the Archean) or salt rising through denser sediments. Salt layers may result from the evaporation of seawater. If subsequent sedimentation covers the salt, a gravitionally unstable configuration results with heavier material (sediments) on top of light material (salt). The rheology of salt is distinctly non-linear and also sensitive to temperature. Once buried, the increased temperature of the salt layer causes its viscosity to decrease to the point where instabilities can grow. Note, since there is always a strong density contrast between the two rock types, the critical Rayleigh number argument does not apply -- this situation is always unstable, but instabilities can only grow at a reasonable rate once the salt has become weak. The geometry is outlined above in the Figure above . We suppose initially that the surface is slightly perturbed with a form of \\begin{equation} \\nonumber w_m = w_{m0} \\cos kx \\end{equation} where \\( k \\) is the wavenumber, \\( k=2\\pi / \\lambda \\), \\( \\lambda \\) being the wavelength of the disturbance. We assume that the magnitude of the disturbance is always much smaller than the wavelength. The problem is easiest to solve if we deal with the biharmonic equation for the stream function. Experience leads us to try to separate variables and look for solutions of the form \\begin{equation} \\nonumber \\psi = \\left( A \\sin kx + B \\cos kx \\right ) Y(y) \\end{equation} where the function Y Y is to be determined. The form we have chosen for \\(w_m\\) in fact means A=1,B=0 A=1,B=0 which we can assume from now on to simplify the algebra. Substituting the trial solution for \\psi \\psi into the biharmonic equation gives \\begin{equation} \\nonumber \\frac{d^4 Y}{d y^4} -2k^2 \\frac{d^2 Y}{dy^2} +k^4 Y = 0 \\end{equation} which has solutions of the form \\begin{equation} \\nonumber Y = A \\exp(m y) \\end{equation} where A A is an arbitrary constant. Subtituting gives us an equation for m m \\begin{equation} \\nonumber m^4 - 2 k^2 m^2 + k^4 = (m^2 - k 2) 2 = 0 \\label{eq:diapaux} \\end{equation} or \\begin{equation} \\nonumber m = \\pm k \\end{equation} Because we have degenerate eigenvalues (i.e. of the four possible solutions to the auxilliary equation (\\ref{eq:diapaux}), two pairs are equal) we need to extend the form of the solution to \\begin{equation} \\nonumber Y = (By+A) \\exp(m y) \\end{equation} to give the general form of the solution in this situation to be \\begin{equation} \\psi = \\sin kx \\left ( A e ^ {- ky} + B y e ^ {- ky} + C e ^ {ky} + D y e ^ {ky} \\right ) \\end{equation} or, equivalently, \\begin{equation} \\psi = \\sin kx \\left ( A _ 1 \\cosh ky + B _ 1 \\sinh ky + C _ 1 y \\cosh ky + D _ 1 y \\sinh ky \\right ) \\label{eq:biharmsoln1} \\end{equation} \\begin{equation} \\psi = \\sin kx \\left ( A _ 1 \\cosh ky + B _ 1 \\sinh ky + C _ 1 y \\cosh ky + D _ 1 y \\sinh ky \\right ) \\label{eq:biharmsoln1} \\end{equation} or, equivalently \\begin{equation} \\psi = \\sin kx \\left ( A _ 1 \\cosh ky + B _ 1 \\sinh ky + C _ 1 y \\cosh ky + D _ 1 y \\sinh ky \\right ) \\label{eq:biharmsoln2} \\end{equation} \\begin{equation} \\psi = \\sin kx \\left ( A _ 1 \\cosh ky + B _ 1 \\sinh ky + C _ 1 y \\cosh ky + D _ 1 y \\sinh ky \\right ) \\label{eq:biharmsoln2} \\end{equation} This equation applies in each of the layers separately. We therefore need to find two sets of constants \\{A_1,B_1,C_1,D_1\\} \\{A_1,B_1,C_1,D_1\\} and \\{A_2,B_2,C_3,D_4\\} \\{A_2,B_2,C_3,D_4\\} by the application of suitable boundary conditions. These are known in terms of the velocities in each layer, \\mathbf{v}_1 = \\mathbf{i} u_1 +\\mathbf{j} v_1 \\mathbf{v}_1 = \\mathbf{i} u_1 +\\mathbf{j} v_1 and \\mathbf{v}_2 = \\mathbf{i} u_2 +\\mathbf{j} v_2 \\mathbf{v}_2 = \\mathbf{i} u_2 +\\mathbf{j} v_2 : \\begin{align} u_1 = v_1 &= 0 \\;\\;\\; \\text{ on } \\;\\;\\; y = -b \\\\ u_2 = v_2 &= 0 \\;\\;\\; \\text{ on } \\;\\;\\; y = b \\end{align} together with a continuity condition across the interface (which we assume is imperceptibly deformed} \\[ \\begin{equation} \\nonumber u_1 = u_2 \\;\\;\\; \\text{ and } \\;\\;\\; v_1 = v_2 \\;\\;\\; \\text{ on } \\;\\;\\; y = 0 \\end{equation} \\] The shear stress (\\( \\sigma_{xy}\\) ) should also be continuous across the interface, which, if we assume equal viscosities, gives \\begin{equation} \\nonumber \\frac{\\partial u_1}{\\partial y} + \\frac{\\partial v_1}{\\partial x} = \\frac{\\partial u_2}{\\partial y} + \\frac{\\partial v_2}{\\partial x} \\;\\;\\; \\text{on} \\;\\;\\; y = 0 \\end{equation} and, to simplify matters, if the velocity is continuous across y=0 y=0 then any velocity derivatives in the x x direction evaluated at y=0 y=0 will also be continuous (i.e. \\partial v_2 / \\partial x = \\partial v_1 / \\partial x \\partial v_2 / \\partial x = \\partial v_1 / \\partial x ). The expressions for velocity in terms of the solution (\\ref{eq:biharmsoln2}) are \\begin{align} u = -\\frac{\\partial \\psi}{\\partial y} & = -\\sin kx \\left( (A_1 k + D_1 + C_1 k y) \\sinh ky + (B_1 k + C_1 + D_1 ky) \\cosh ky \\right) \\\\ v = \\frac{\\partial \\psi}{\\partial x} & = k \\cos kx \\left( (A_1 +C_1 y)\\cosh ky + (B_1 +D_1 y) \\sinh ky \\right) \\end{align} From here to the solution requires much tedious rearrangement, and the usual argument based on the arbitrary choice of wavenumber k k but we finally arrive at \\begin{multline} \\psi_1 = A_1 \\sin kx \\cosh ky + \\\\ A_1 \\sin kx \\left[ \\frac{y}{k b^2} \\tanh kb \\sinh ky + \\left( \\frac{y}{b} \\cosh ky \\frac{1}{kb} \\sinh ky \\right) \\cdot \\left( \\frac{1}{kb} + \\frac{1}{\\sinh bk \\cosh bk} \\right) \\right] \\times \\\\ \\left[ \\frac{1}{\\sinh bk \\cosh bk} - \\frac{1}{(b 2k 2} \\tanh bk \\right] ^{-1} \\label{eq:raytays1} \\end{multline} The stream function for the lower layer is found by replacing y y with -y -y in this equation. This is already a relatively nasty expression, but we haven't finished since the constant A_1 A_1 remains. This occurs because we have so far considered the form of flows which satisfy all the boundary conditions but have not yet considered what drives the flow in each layer. To eliminate A_1 A_1 , we have to consider the physical scales inherent in the problem itself. We are interested (primarily) in the behaviour of the interface which moves with a velocity \\partial w / \\partial t \\partial w / \\partial t . As we are working with small deflections of the interface, \\begin{equation} \\nonumber \\frac{\\partial w}{\\partial t} = \\left. v \\right| _ {y=0} \\end{equation} {. width=\"75%\" id=\"rayleigh-taylor2\"} The restoring force for a stable layering is proportional to the excess density when a fluid element is displaced across the boundary Consider what happens when the fluid above the interface is lighter than the fluid below -- this situation is stable so we expect the layering to be preserved, and if the interface is disturbed the disturbance to decay. This implies that there must be a restoring force acting on an element of fluid which is somehow displaced across the boundary at y=0 y=0 (Figure above) . This restoring force is due to the density difference between the displaced material and the layer in which it finds itself. The expression for the force is exactly that from Archimedes principle which explains how a boat can float (only in the opposite direction) \\begin{equation} \\nonumber \\left. F_2 \\right| _ {y=0} = \\delta x g w (\\rho _ 2 - \\rho _ 1) \\end{equation} \\begin{equation} \\nonumber \\left. F_2 \\right| _ {y=0} = \\delta x g w (\\rho _ 2 - \\rho _ 1) \\end{equation} which can be expressed as a normal stress difference (assumed to apply, once again, at the boundary). The viscous component of the normal stress turns out to be zero -- proven by evaluating \\partial v / \\partial y \\partial v / \\partial y at y=0 y=0 using the expression for \\( \\phi \\) in equation (\\ref{eq:raytays1}). Thus the restoring stress is purely pressure \\[ \\begin{equation} \\nonumber \\left. P_2 \\right| _ {y=0} = g w (\\rho _ 2 - \\rho _ 1) \\end{equation} \\] The pressure in terms of the solution (so far) for \\psi \\psi is found from the equation of motion in the horizontal direction (substituting the stream function formulation) and is then equated to the restoring pressure above. \\[ \\begin{equation} \\nonumber (\\rho_1-\\rho_2) g w = -\\frac{4 \\eta k A_1}{b} \\cos kx \\left(\\frac{1}{kb} + \\frac{1}{\\sinh bk \\cosh bk} \\right) \\cdot \\left( \\frac{1}{\\sinh bk \\cosh bk} - \\frac{1}{(b 2k 2} \\tanh bk \\right)^{-1} \\end{equation} \\] which allows us to substitute for A_1 A_1 in our expression for \\partial w / \\partial t \\partial w / \\partial t above. Since A_1 A_1 is independent of t t , we can see that the solution for w w will be of a growing or decaying exponential form with growth/decay constant coming from the argument above. \\[ \\begin{equation} \\nonumber w(t) = w_0 \\exp((t-t_0)/\\tau) \\end{equation} \\] where \\[ \\begin{equation} \\nonumber \\tau = \\frac{4 \\eta}{(\\rho_1-\\rho_2) g b} \\left( \\frac{1}{kb} + \\frac{1}{\\sinh bk \\cosh bk} \\right) \\cdot \\left( \\frac{1}{k 2b 2} \\tanh kb - \\frac{1}{\\sinh kb \\cosh kb} \\right)^{-1} \\end{equation} \\] So, finally, an answer -- the rate at which instabilities on the interface between two layers will grow (or shrink) which depends on viscosity, layer depth and density differences, together with the geometrical consideration of the layer thicknesses. A stable layering results from light fluid resting on heavy fluid; a heavy fluid resting on a light fluid is always unstable (no critical Rayleigh number applies) although the growth rate can be so small that no deformation occurs in practice. The growth rate is also dependent on wavenumber. There is a minimum in the growth time as a function of dimensional wavenumber which occurs at k b = 2.4 k b = 2.4 , so instabilities close to this wavenumber are expected to grow first and dominate. Remember that this derivation is simplified for fluids of equal viscosity, and layers of identical depth. Remember also that the solution is for {\\rm infinitessimal} deformations of the interface. If the deformation grows then the approximations of small deformation no longer hold. This gives a clue as to how difficulties dealing with the advection term of the transport equations arise. At some point it becomes impossible to obtain meaningful results without computer simulation. However, plenty of further work has already been done on this area for non-linear fluids, temperature dependent viscosity \\&c and the solutions are predictably long and tedious to read, much less solve. When the viscosity is not constant, the use of a stream function notation is not particularly helpful as the biharmonic form no longer appears. \\Emerald{(e.g. read work by Ribe, Houseman et al.)} The methodology used here is instructive, as it can be used in a number of different applications to related areas. The equations are similar, the boundary conditions different. Post-Glacial Rebound {. width=\"75%\" id=\"postglacial-relaxation\"} The relaxation of the Earth's surface after removal of an ice load In the postglacial rebound problem, consider a viscous half space with an imposed topography at t=0 t=0 . The ice load is removed at t=0 t=0 and the interface relaxes back to its original flat state. This can be studied one wavenumber at a time --- computing a decay rate for each component of the topography. The intial loading is computed from the fourier transform of the ice bottom topography. The system is similar to that of the diapirs except that the loading is now applied to one surface rather than the interface between two fluids. Phase Changes in the mantle A different interface problem is that of mantle phase changes. Here a bouyancy anomaly results if the phase change boundary is distorted. This can result from advection normal to the boundary bringing cooler or warmer material across the boundary. The buoyancy balance argument used above can be recycled here to determine a scaling for the ability of plumes/downwellings to cross the phase boundary. Sensitivity Kernels for Surface Observables The solution method used for the Rayleigh Taylor problem can also be used in determining spectral Green's functions for mantle flow in response to thermal perturbations. This is a particularly abstract application of the identical theory. Folding of Layered (Viscous) Medium {. width=\"75%\" id=\"folding-layer\"} Instability in a thin, viscous layer compressed from both ends If a thin viscous layer is compressed from one end then it may develop buckling instabilities in which velocities grow perpendicular to the plane of the layer. If the layer is embedded between two semi-infinite layers of viscous fluid with viscosity much smaller than the viscosity of the layer, then Biot theory tells us the wavelength of the initial buckling instability, and the rate at which it grows. The fold geometry evolves as \\[ \\begin{equation} \\nonumber w=w_m \\cos(kx) e^{\\frac{t}{\\tau_a}} \\end{equation} \\] where \\[ \\begin{equation} \\nonumber \\tau_a = \\frac{1}{\\bar{P}}\\left[ \\frac{4 \\eta_0}{k} + \\frac{\\eta_1 h 3}{3k 2} \\right] \\end{equation} \\] and the fastest growing wavenumber is \\[ \\begin{equation} \\nonumber k = \\frac{6}{h}\\left( \\frac{\\eta_1}{\\eta_0} \\right)^{\\frac{1}{3}} \\end{equation} \\] For large deformations we eventually must resort to numerical simulation. Gravity Currents Gravity currents can occur when a viscous fluid flows under its own weight as shown in the Figure above . We assume that the fluid has constant viscosity, \\eta \\eta and that the length of the current is considerably greater than its height. The fluid is embedded in a low viscosity medium of density \\rho-\\Delta \\rho \\rho-\\Delta \\rho where \\rho \\rho is the density of the fluid itself. The force balance is between buoyancy and viscosity. The assumptions of geometry allow us to simplify the Stokes equation by assuming horizontal pressure gradients due to the surface slope drive the flow. \\[ \\begin{equation} \\nonumber \\nabla p = \\eta\\nabla^2 u \\approx g \\Delta \\rho \\frac{\\partial h}{\\partial x} \\end{equation} \\] We assume near-zero shear stress at the top of the current to give \\[ \\begin{equation} \\nonumber \\frac{\\partial u}{\\partial z} (x,h,t) = 0 \\end{equation} \\] and zero velocity at the base of the current. Hence \\[ \\begin{equation} \\nonumber u(x,z,t) = -\\frac{1}{2} \\frac{g \\Delta \\rho}{\\eta} \\frac{\\partial h}{\\partial x} z(2h-z) \\end{equation} \\] Continuity integrated over depth implies \\[ \\begin{equation} \\nonumber \\frac{\\partial h}{\\partial t} + \\frac{\\partial }{\\partial x} \\int_0^h u dz = 0 \\end{equation} \\] Combining these equations gives \\[ \\begin{equation} \\nonumber \\frac{\\partial h}{\\partial t} -\\frac{1}{3} \\frac{g \\Delta \\rho}{\\eta} \\frac{\\partial }{\\partial x} \\left( h^3 \\frac{\\partial h}{\\partial x} \\right) = 0 \\end{equation} \\] Finally, a global constraint fixes the total amount of fluid at any given time \\[ \\begin{equation} \\nonumber \\int_0^{x_N(t)} h(x,t)dx = qt^\\alpha \\end{equation} \\] The latter term being a fluid source at the origin, and x_ {N(t)} x_ {N(t)} the location of the front of the current. A similarity variable can be used to transform this problem: \\[ \\begin{equation} \\nonumber \\nu = \\left( \\frac{1}{3} g\\Delta \\rho q^3 / \\eta \\right)^{-\\frac{1}{5}} x t^{-(3\\alpha +1) / 5} \\end{equation} \\] giving a solution of the form \\[ \\begin{equation} \\nonumber h(x,t) = \\nu_N^{2/3} (3q^2 \\eta / (g\\Delta\\rho))^{1/5} t^{(2\\alpha -1) / 5} \\phi(\\nu/\\nu_N) \\end{equation} \\] where \\nu_N \\nu_N is the value of \\nu \\nu at x=x_N(t) x=x_N(t) . Substituting into the equation for \\partial h / \\partial t \\partial h / \\partial t we find that \\phi(\\nu/\\nu_N) \\phi(\\nu/\\nu_N) satisfies \\[ \\begin{equation} \\nonumber \\phi({\\nu}/{\\nu_N}) = \\left[ \\frac{3}{5}(3\\alpha+1)\\right]^{\\frac{1}{3}} \\left(1-\\frac{\\nu}{\\nu_N} \\right)^{\\frac{1}{3}} \\left[ 1 - \\frac{3\\alpha-4}{24(3\\alpha+1)}\\left(1-\\frac{\\nu}{\\nu_N} \\right) + O \\left(1-\\frac{\\nu}{\\nu_N} \\right)^2 \\right] \\end{equation} \\] Which has an analytic solution if \\alpha=0 \\alpha=0 (only constant sources or sinks) \\[ \\begin{equation} \\nonumber \\begin{split} \\phi({\\nu}/{\\nu_N}) &= \\left( \\frac{3}{10}\\right)^{\\frac{1}{3}} \\left( 1-\\left(\\frac{\\nu}{\\nu_N}\\right)^2 \\right)^{\\frac{1}{3}} \\ \\nu_N &= \\left[ \\frac{1}{5} \\left( \\frac{3}{10}\\right)^{\\frac{1}{3}} \\pi^{\\frac{1}{2}} \\Gamma (1/3) / \\Gamma (5/6) \\right]^{-\\frac{3}{5}} = 1.411 \\end{split} \\end{equation} \\] For all other values of \\alpha \\alpha numerical integration schemes must be used for \\( \\phi \\). It is also possible to obtain solutions if axisymmetric geometry is used.","title":"MathPhysicsBackground 4"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-4.html#rayleigh-taylor-instability-diapirism","text":"{. width=\"75%\" id=\"rayleigh-taylor\"} Salt diapirs result when a buried layer of salt(a,b) becomes convectively unstable and rises through the overlying sediment layers (c,d). The idealized geometry for the Rayleigh-Taylor instability problem is outlined in the lower diagram Diapirism is the buoyant upwelling of rock which is lighter than its surroundings. This can include mantle plumes and other purely thermal phenomena but it often applied to compositionally distinct rock masses such as melts infiltrating the crust (in the Archean) or salt rising through denser sediments. Salt layers may result from the evaporation of seawater. If subsequent sedimentation covers the salt, a gravitionally unstable configuration results with heavier material (sediments) on top of light material (salt). The rheology of salt is distinctly non-linear and also sensitive to temperature. Once buried, the increased temperature of the salt layer causes its viscosity to decrease to the point where instabilities can grow. Note, since there is always a strong density contrast between the two rock types, the critical Rayleigh number argument does not apply -- this situation is always unstable, but instabilities can only grow at a reasonable rate once the salt has become weak. The geometry is outlined above in the Figure above . We suppose initially that the surface is slightly perturbed with a form of \\begin{equation} \\nonumber w_m = w_{m0} \\cos kx \\end{equation} where \\( k \\) is the wavenumber, \\( k=2\\pi / \\lambda \\), \\( \\lambda \\) being the wavelength of the disturbance. We assume that the magnitude of the disturbance is always much smaller than the wavelength. The problem is easiest to solve if we deal with the biharmonic equation for the stream function. Experience leads us to try to separate variables and look for solutions of the form \\begin{equation} \\nonumber \\psi = \\left( A \\sin kx + B \\cos kx \\right ) Y(y) \\end{equation} where the function Y Y is to be determined. The form we have chosen for \\(w_m\\) in fact means A=1,B=0 A=1,B=0 which we can assume from now on to simplify the algebra. Substituting the trial solution for \\psi \\psi into the biharmonic equation gives \\begin{equation} \\nonumber \\frac{d^4 Y}{d y^4} -2k^2 \\frac{d^2 Y}{dy^2} +k^4 Y = 0 \\end{equation} which has solutions of the form \\begin{equation} \\nonumber Y = A \\exp(m y) \\end{equation} where A A is an arbitrary constant. Subtituting gives us an equation for m m \\begin{equation} \\nonumber m^4 - 2 k^2 m^2 + k^4 = (m^2 - k 2) 2 = 0 \\label{eq:diapaux} \\end{equation} or \\begin{equation} \\nonumber m = \\pm k \\end{equation} Because we have degenerate eigenvalues (i.e. of the four possible solutions to the auxilliary equation (\\ref{eq:diapaux}), two pairs are equal) we need to extend the form of the solution to \\begin{equation} \\nonumber Y = (By+A) \\exp(m y) \\end{equation} to give the general form of the solution in this situation to be \\begin{equation} \\psi = \\sin kx \\left ( A e ^ {- ky} + B y e ^ {- ky} + C e ^ {ky} + D y e ^ {ky} \\right ) \\end{equation} or, equivalently, \\begin{equation} \\psi = \\sin kx \\left ( A _ 1 \\cosh ky + B _ 1 \\sinh ky + C _ 1 y \\cosh ky + D _ 1 y \\sinh ky \\right ) \\label{eq:biharmsoln1} \\end{equation} \\begin{equation} \\psi = \\sin kx \\left ( A _ 1 \\cosh ky + B _ 1 \\sinh ky + C _ 1 y \\cosh ky + D _ 1 y \\sinh ky \\right ) \\label{eq:biharmsoln1} \\end{equation} or, equivalently \\begin{equation} \\psi = \\sin kx \\left ( A _ 1 \\cosh ky + B _ 1 \\sinh ky + C _ 1 y \\cosh ky + D _ 1 y \\sinh ky \\right ) \\label{eq:biharmsoln2} \\end{equation} \\begin{equation} \\psi = \\sin kx \\left ( A _ 1 \\cosh ky + B _ 1 \\sinh ky + C _ 1 y \\cosh ky + D _ 1 y \\sinh ky \\right ) \\label{eq:biharmsoln2} \\end{equation} This equation applies in each of the layers separately. We therefore need to find two sets of constants \\{A_1,B_1,C_1,D_1\\} \\{A_1,B_1,C_1,D_1\\} and \\{A_2,B_2,C_3,D_4\\} \\{A_2,B_2,C_3,D_4\\} by the application of suitable boundary conditions. These are known in terms of the velocities in each layer, \\mathbf{v}_1 = \\mathbf{i} u_1 +\\mathbf{j} v_1 \\mathbf{v}_1 = \\mathbf{i} u_1 +\\mathbf{j} v_1 and \\mathbf{v}_2 = \\mathbf{i} u_2 +\\mathbf{j} v_2 \\mathbf{v}_2 = \\mathbf{i} u_2 +\\mathbf{j} v_2 : \\begin{align} u_1 = v_1 &= 0 \\;\\;\\; \\text{ on } \\;\\;\\; y = -b \\\\ u_2 = v_2 &= 0 \\;\\;\\; \\text{ on } \\;\\;\\; y = b \\end{align} together with a continuity condition across the interface (which we assume is imperceptibly deformed} \\[ \\begin{equation} \\nonumber u_1 = u_2 \\;\\;\\; \\text{ and } \\;\\;\\; v_1 = v_2 \\;\\;\\; \\text{ on } \\;\\;\\; y = 0 \\end{equation} \\] The shear stress (\\( \\sigma_{xy}\\) ) should also be continuous across the interface, which, if we assume equal viscosities, gives \\begin{equation} \\nonumber \\frac{\\partial u_1}{\\partial y} + \\frac{\\partial v_1}{\\partial x} = \\frac{\\partial u_2}{\\partial y} + \\frac{\\partial v_2}{\\partial x} \\;\\;\\; \\text{on} \\;\\;\\; y = 0 \\end{equation} and, to simplify matters, if the velocity is continuous across y=0 y=0 then any velocity derivatives in the x x direction evaluated at y=0 y=0 will also be continuous (i.e. \\partial v_2 / \\partial x = \\partial v_1 / \\partial x \\partial v_2 / \\partial x = \\partial v_1 / \\partial x ). The expressions for velocity in terms of the solution (\\ref{eq:biharmsoln2}) are \\begin{align} u = -\\frac{\\partial \\psi}{\\partial y} & = -\\sin kx \\left( (A_1 k + D_1 + C_1 k y) \\sinh ky + (B_1 k + C_1 + D_1 ky) \\cosh ky \\right) \\\\ v = \\frac{\\partial \\psi}{\\partial x} & = k \\cos kx \\left( (A_1 +C_1 y)\\cosh ky + (B_1 +D_1 y) \\sinh ky \\right) \\end{align} From here to the solution requires much tedious rearrangement, and the usual argument based on the arbitrary choice of wavenumber k k but we finally arrive at \\begin{multline} \\psi_1 = A_1 \\sin kx \\cosh ky + \\\\ A_1 \\sin kx \\left[ \\frac{y}{k b^2} \\tanh kb \\sinh ky + \\left( \\frac{y}{b} \\cosh ky \\frac{1}{kb} \\sinh ky \\right) \\cdot \\left( \\frac{1}{kb} + \\frac{1}{\\sinh bk \\cosh bk} \\right) \\right] \\times \\\\ \\left[ \\frac{1}{\\sinh bk \\cosh bk} - \\frac{1}{(b 2k 2} \\tanh bk \\right] ^{-1} \\label{eq:raytays1} \\end{multline} The stream function for the lower layer is found by replacing y y with -y -y in this equation. This is already a relatively nasty expression, but we haven't finished since the constant A_1 A_1 remains. This occurs because we have so far considered the form of flows which satisfy all the boundary conditions but have not yet considered what drives the flow in each layer. To eliminate A_1 A_1 , we have to consider the physical scales inherent in the problem itself. We are interested (primarily) in the behaviour of the interface which moves with a velocity \\partial w / \\partial t \\partial w / \\partial t . As we are working with small deflections of the interface, \\begin{equation} \\nonumber \\frac{\\partial w}{\\partial t} = \\left. v \\right| _ {y=0} \\end{equation} {. width=\"75%\" id=\"rayleigh-taylor2\"} The restoring force for a stable layering is proportional to the excess density when a fluid element is displaced across the boundary Consider what happens when the fluid above the interface is lighter than the fluid below -- this situation is stable so we expect the layering to be preserved, and if the interface is disturbed the disturbance to decay. This implies that there must be a restoring force acting on an element of fluid which is somehow displaced across the boundary at y=0 y=0 (Figure above) . This restoring force is due to the density difference between the displaced material and the layer in which it finds itself. The expression for the force is exactly that from Archimedes principle which explains how a boat can float (only in the opposite direction) \\begin{equation} \\nonumber \\left. F_2 \\right| _ {y=0} = \\delta x g w (\\rho _ 2 - \\rho _ 1) \\end{equation} \\begin{equation} \\nonumber \\left. F_2 \\right| _ {y=0} = \\delta x g w (\\rho _ 2 - \\rho _ 1) \\end{equation} which can be expressed as a normal stress difference (assumed to apply, once again, at the boundary). The viscous component of the normal stress turns out to be zero -- proven by evaluating \\partial v / \\partial y \\partial v / \\partial y at y=0 y=0 using the expression for \\( \\phi \\) in equation (\\ref{eq:raytays1}). Thus the restoring stress is purely pressure \\[ \\begin{equation} \\nonumber \\left. P_2 \\right| _ {y=0} = g w (\\rho _ 2 - \\rho _ 1) \\end{equation} \\] The pressure in terms of the solution (so far) for \\psi \\psi is found from the equation of motion in the horizontal direction (substituting the stream function formulation) and is then equated to the restoring pressure above. \\[ \\begin{equation} \\nonumber (\\rho_1-\\rho_2) g w = -\\frac{4 \\eta k A_1}{b} \\cos kx \\left(\\frac{1}{kb} + \\frac{1}{\\sinh bk \\cosh bk} \\right) \\cdot \\left( \\frac{1}{\\sinh bk \\cosh bk} - \\frac{1}{(b 2k 2} \\tanh bk \\right)^{-1} \\end{equation} \\] which allows us to substitute for A_1 A_1 in our expression for \\partial w / \\partial t \\partial w / \\partial t above. Since A_1 A_1 is independent of t t , we can see that the solution for w w will be of a growing or decaying exponential form with growth/decay constant coming from the argument above. \\[ \\begin{equation} \\nonumber w(t) = w_0 \\exp((t-t_0)/\\tau) \\end{equation} \\] where \\[ \\begin{equation} \\nonumber \\tau = \\frac{4 \\eta}{(\\rho_1-\\rho_2) g b} \\left( \\frac{1}{kb} + \\frac{1}{\\sinh bk \\cosh bk} \\right) \\cdot \\left( \\frac{1}{k 2b 2} \\tanh kb - \\frac{1}{\\sinh kb \\cosh kb} \\right)^{-1} \\end{equation} \\] So, finally, an answer -- the rate at which instabilities on the interface between two layers will grow (or shrink) which depends on viscosity, layer depth and density differences, together with the geometrical consideration of the layer thicknesses. A stable layering results from light fluid resting on heavy fluid; a heavy fluid resting on a light fluid is always unstable (no critical Rayleigh number applies) although the growth rate can be so small that no deformation occurs in practice. The growth rate is also dependent on wavenumber. There is a minimum in the growth time as a function of dimensional wavenumber which occurs at k b = 2.4 k b = 2.4 , so instabilities close to this wavenumber are expected to grow first and dominate. Remember that this derivation is simplified for fluids of equal viscosity, and layers of identical depth. Remember also that the solution is for {\\rm infinitessimal} deformations of the interface. If the deformation grows then the approximations of small deformation no longer hold. This gives a clue as to how difficulties dealing with the advection term of the transport equations arise. At some point it becomes impossible to obtain meaningful results without computer simulation. However, plenty of further work has already been done on this area for non-linear fluids, temperature dependent viscosity \\&c and the solutions are predictably long and tedious to read, much less solve. When the viscosity is not constant, the use of a stream function notation is not particularly helpful as the biharmonic form no longer appears. \\Emerald{(e.g. read work by Ribe, Houseman et al.)} The methodology used here is instructive, as it can be used in a number of different applications to related areas. The equations are similar, the boundary conditions different.","title":"Rayleigh-Taylor Instability &amp; Diapirism"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-4.html#post-glacial-rebound","text":"{. width=\"75%\" id=\"postglacial-relaxation\"} The relaxation of the Earth's surface after removal of an ice load In the postglacial rebound problem, consider a viscous half space with an imposed topography at t=0 t=0 . The ice load is removed at t=0 t=0 and the interface relaxes back to its original flat state. This can be studied one wavenumber at a time --- computing a decay rate for each component of the topography. The intial loading is computed from the fourier transform of the ice bottom topography. The system is similar to that of the diapirs except that the loading is now applied to one surface rather than the interface between two fluids.","title":"Post-Glacial Rebound"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-4.html#phase-changes-in-the-mantle","text":"A different interface problem is that of mantle phase changes. Here a bouyancy anomaly results if the phase change boundary is distorted. This can result from advection normal to the boundary bringing cooler or warmer material across the boundary. The buoyancy balance argument used above can be recycled here to determine a scaling for the ability of plumes/downwellings to cross the phase boundary.","title":"Phase Changes in the mantle"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-4.html#sensitivity-kernels-for-surface-observables","text":"The solution method used for the Rayleigh Taylor problem can also be used in determining spectral Green's functions for mantle flow in response to thermal perturbations. This is a particularly abstract application of the identical theory.","title":"Sensitivity Kernels for Surface Observables"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-4.html#folding-of-layered-viscous-medium","text":"{. width=\"75%\" id=\"folding-layer\"} Instability in a thin, viscous layer compressed from both ends If a thin viscous layer is compressed from one end then it may develop buckling instabilities in which velocities grow perpendicular to the plane of the layer. If the layer is embedded between two semi-infinite layers of viscous fluid with viscosity much smaller than the viscosity of the layer, then Biot theory tells us the wavelength of the initial buckling instability, and the rate at which it grows. The fold geometry evolves as \\[ \\begin{equation} \\nonumber w=w_m \\cos(kx) e^{\\frac{t}{\\tau_a}} \\end{equation} \\] where \\[ \\begin{equation} \\nonumber \\tau_a = \\frac{1}{\\bar{P}}\\left[ \\frac{4 \\eta_0}{k} + \\frac{\\eta_1 h 3}{3k 2} \\right] \\end{equation} \\] and the fastest growing wavenumber is \\[ \\begin{equation} \\nonumber k = \\frac{6}{h}\\left( \\frac{\\eta_1}{\\eta_0} \\right)^{\\frac{1}{3}} \\end{equation} \\] For large deformations we eventually must resort to numerical simulation.","title":"Folding of Layered (Viscous) Medium"},{"location":"Geodynamics/TheoreticalBackground/MathPhysicsBackground-4.html#gravity-currents","text":"Gravity currents can occur when a viscous fluid flows under its own weight as shown in the Figure above . We assume that the fluid has constant viscosity, \\eta \\eta and that the length of the current is considerably greater than its height. The fluid is embedded in a low viscosity medium of density \\rho-\\Delta \\rho \\rho-\\Delta \\rho where \\rho \\rho is the density of the fluid itself. The force balance is between buoyancy and viscosity. The assumptions of geometry allow us to simplify the Stokes equation by assuming horizontal pressure gradients due to the surface slope drive the flow. \\[ \\begin{equation} \\nonumber \\nabla p = \\eta\\nabla^2 u \\approx g \\Delta \\rho \\frac{\\partial h}{\\partial x} \\end{equation} \\] We assume near-zero shear stress at the top of the current to give \\[ \\begin{equation} \\nonumber \\frac{\\partial u}{\\partial z} (x,h,t) = 0 \\end{equation} \\] and zero velocity at the base of the current. Hence \\[ \\begin{equation} \\nonumber u(x,z,t) = -\\frac{1}{2} \\frac{g \\Delta \\rho}{\\eta} \\frac{\\partial h}{\\partial x} z(2h-z) \\end{equation} \\] Continuity integrated over depth implies \\[ \\begin{equation} \\nonumber \\frac{\\partial h}{\\partial t} + \\frac{\\partial }{\\partial x} \\int_0^h u dz = 0 \\end{equation} \\] Combining these equations gives \\[ \\begin{equation} \\nonumber \\frac{\\partial h}{\\partial t} -\\frac{1}{3} \\frac{g \\Delta \\rho}{\\eta} \\frac{\\partial }{\\partial x} \\left( h^3 \\frac{\\partial h}{\\partial x} \\right) = 0 \\end{equation} \\] Finally, a global constraint fixes the total amount of fluid at any given time \\[ \\begin{equation} \\nonumber \\int_0^{x_N(t)} h(x,t)dx = qt^\\alpha \\end{equation} \\] The latter term being a fluid source at the origin, and x_ {N(t)} x_ {N(t)} the location of the front of the current. A similarity variable can be used to transform this problem: \\[ \\begin{equation} \\nonumber \\nu = \\left( \\frac{1}{3} g\\Delta \\rho q^3 / \\eta \\right)^{-\\frac{1}{5}} x t^{-(3\\alpha +1) / 5} \\end{equation} \\] giving a solution of the form \\[ \\begin{equation} \\nonumber h(x,t) = \\nu_N^{2/3} (3q^2 \\eta / (g\\Delta\\rho))^{1/5} t^{(2\\alpha -1) / 5} \\phi(\\nu/\\nu_N) \\end{equation} \\] where \\nu_N \\nu_N is the value of \\nu \\nu at x=x_N(t) x=x_N(t) . Substituting into the equation for \\partial h / \\partial t \\partial h / \\partial t we find that \\phi(\\nu/\\nu_N) \\phi(\\nu/\\nu_N) satisfies \\[ \\begin{equation} \\nonumber \\phi({\\nu}/{\\nu_N}) = \\left[ \\frac{3}{5}(3\\alpha+1)\\right]^{\\frac{1}{3}} \\left(1-\\frac{\\nu}{\\nu_N} \\right)^{\\frac{1}{3}} \\left[ 1 - \\frac{3\\alpha-4}{24(3\\alpha+1)}\\left(1-\\frac{\\nu}{\\nu_N} \\right) + O \\left(1-\\frac{\\nu}{\\nu_N} \\right)^2 \\right] \\end{equation} \\] Which has an analytic solution if \\alpha=0 \\alpha=0 (only constant sources or sinks) \\[ \\begin{equation} \\nonumber \\begin{split} \\phi({\\nu}/{\\nu_N}) &= \\left( \\frac{3}{10}\\right)^{\\frac{1}{3}} \\left( 1-\\left(\\frac{\\nu}{\\nu_N}\\right)^2 \\right)^{\\frac{1}{3}} \\ \\nu_N &= \\left[ \\frac{1}{5} \\left( \\frac{3}{10}\\right)^{\\frac{1}{3}} \\pi^{\\frac{1}{2}} \\Gamma (1/3) / \\Gamma (5/6) \\right]^{-\\frac{3}{5}} = 1.411 \\end{split} \\end{equation} \\] For all other values of \\alpha \\alpha numerical integration schemes must be used for \\( \\phi \\). It is also possible to obtain solutions if axisymmetric geometry is used.","title":"Gravity Currents"},{"location":"Numerical/Introduction.html","text":"Computational Geodynamics Notes Chapter 1 - Introduction Chapter 2 - Simple Example Chapter 3 - Advection Example Chapter 4 - Finite Elements 1 Chapter 5 - Finite Elements 2 Chapter 6 - Finite Elements 3","title":"Introduction"},{"location":"Numerical/Introduction.html#computational-geodynamics-notes","text":"Chapter 1 - Introduction Chapter 2 - Simple Example Chapter 3 - Advection Example Chapter 4 - Finite Elements 1 Chapter 5 - Finite Elements 2 Chapter 6 - Finite Elements 3","title":"Computational Geodynamics Notes"},{"location":"Numerical/NumericalMethodsPrimer/index.html","text":"Table of Contents Chapter 1 - Introduction Chapter 2 - Simple Example Chapter 3 - Advection Example Chapter 4 - Finite Elements 1 Chapter 5 - Finite Elements 2 Chapter 6 - Finite Elements 3","title":"Table of Contents"},{"location":"Numerical/NumericalMethodsPrimer/index.html#table-of-contents","text":"Chapter 1 - Introduction Chapter 2 - Simple Example Chapter 3 - Advection Example Chapter 4 - Finite Elements 1 Chapter 5 - Finite Elements 2 Chapter 6 - Finite Elements 3","title":"Table of Contents"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html","text":"Computational Geodynamics: Introduction to Numerical Methods \\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\] Introductory Remarks We have considered the application of mathematical reasoning to construct models of the behaviour of physical systems. Unfortunately, most of these models do not have analytic solutions or the solutions are so complicated that they don't help us understand the problem. The alternative is to solve such problems numerically (approximately). Such solutions are also possible for far more elaborate systems than we would even consider trying to obtain exact solutions for. On the other hand, numerical modeling only provides solutions to mathematical equations and only approximate solutions at that. Unless the modeler has an understanding of the methods and the models themselves, then the output of some computer code may be terribly misleading. It is always necessary to design a numerical experiment as carefully as one would a physical experiment --- although it won't blow up and kill you. It is possible to change the laws of physics in the numerical model often inadvertantly, so a careful analysis of the problem first is not a bad idea. This part of the course contains some really hard material. This is deliberate because the workings of most computer codes are actually based on some astonishingly intricate mathematics. This is really intended to provide a decent reference for people coming back to use finite elements or other numerical methods later. A Variety of Numerical Methods We will discuss a number of different numerical solution techniques which are suitable for solid Earth dynamics problems. Obviously only a brief discussion of any one method is possible, and more methods are inevitably available. Some methods work well for particular problem and others roll over and die with hardly any sign of impending doom. We need to know what the tools in our toolbox are for and what will cause them to break before we try to solve serious problems with them. After all, in uncharted territory, a numerical instability might look enticingly like an exiting new result. Obviously there is not room to do justice to any particular method and I will concentrate on how the finite element method works (though for some things other methods have many advantages). Finite Differences If we have encountered calculus at all, then we are already familiar with finite differences. Where we would, in determining a derivative, consider the limit of a quantity over a small interval as that interval shrinks to an infinitessimal size, in finite differences, we compute the value of such quantities for small, finite intervals. Typically we use regular meshes of points on which to compute the differences (but we don't have to !). Finite Difference methods are simple and can be very fast. They can be intuitively easy to understand and program and are also universally applied. They may encounter difficulties when extreme variations in properties occur from one grid point to another, particularly if there are discontinuities. Finite Elements Finite elements work from a variational principle (more, much more, later) which is an integral version of the governing equations. They work with a spatial discretization into a mesh spanned by elements with unknowns allocated to their vertices. The application of finite elements to complex problems and those with very complex domains is a programmers joy since the entire methodology match object - oriented programming techniques. The flip-side of this is that the construction of the numerical equations which need to be solved may take considerably longer than the actual solution process. The retention of the integral form can be beneficial for problems with difficult boundary conditions or discontinuities which can be integrated. Variational methods are, however, difficult to constrain particularly when iterative, implicit solution methods are used. Finite Volumes In finite volumes one combines some of the best features of finite elements and finite differences. The method is grid based and works with both the original mesh and its dual. The formulation starts with a weak form of the equations based on volume fluxes into the local volume surrounding a node (based on the dual mesh). These integrals are then converted via Gauss' theorem, into surface integrals on the edges/faces of the dual mesh. Depending on the subsequent discretization, the resulting algorithm can be akin to finite differences or to finite elements (whether surface integrals remain in the formulation or are replaced by some differencing form) Advantages include the fact that the surface integral formulation can be tailored to satisfy local constraints without additional messing about thus making for very rapid solution times. The formulation can also deal with arbitrary mesh configurations. Disadvantages include a difficulty in applying some boundary conditions since the dual surface is not defined outside the mesh (formally). Natural Elements An extension of finite elements using the theory of Natural Neighbour interpolation schemes to provide shape functions for all grid point arrangements. A best (Delaunay) triangulation is defined for such a set of points and interpolation functions exist which give an optimal representation of the interpolant. These functions can provide a basis for a finite element scheme. Advantages include the fact that elements can be highly distorted but this does not affect the convergence of the method in the same catastrophic way as for normal schemes. Disadvantages include the fact that shape functions overlap other elements and therefore precise integration is difficult. Also this is a relatively novel method and there are some odds and ends to be ironed out. Spectral (time/space) We saw in the theoretical treatment of instabilities in a layer how beneficial it can be to deal with harmonic functions in one or more dimensions. This turns the problem from a partial differential equation into a set of ordinary differential equations for the different wavenumbers. The method works well for systems with homogeneous material properties otherwise spatial variations in these properties are couple the different wavenumbers together and add layer upon layer of complexity to the problem. Due to the existence of the fast fourier transform, these methods, when they can be applied, are potentially very quick. They are limited to relatively regular geometries, however. Discrete Elements Meshless methods which deal with either actual discrete systems such as large systems of granular materials or systems in which notional particles represent elements of the continuum. The method works by treating simple interactions of very many particles dynamically. For each particle an explicit solution of F=ma F=ma for translations and L=I\\ddot{\\theta} L=I\\ddot{\\theta} for the rotations is found in reponse to the interaction forces with every other particle. In discrete elements such interactions are usually local (e.g. only when particles are in contact) and this makes the system manageable in size. Very good for treatment of fracture and dynamic responses of granular systems but can be hampered by the time taken for the fully explicit nature of the algorithm, i.e. elastic waves must be resolved in order to model deformation. Also, these methods are based on particle interaction functions and so properties of the continuum such as viscosity are not direct inputs but have to be computed as an average response of the system (just like the real thing). Compare this to molecular dynamics simulations. Smooth Particle Hydrodynamics Another meshless method in which \"particles\" are the centres of smooth functions such as gaussians. These can be used to interpolate any field across the solution domain. They are also differentiable and can therefore represent differential equations relatively efficiently. Very useful for high velocity flows and astrophysics type problems. Historically there have been problems representing boundary conditions and viscosity so not ideal for the highly viscous and/or elastic type problems of the solid earth. Particle in Cell methods In these methods both particles and a mesh exist. The mesh supplies the velocities which move particles around, but the particles carry information with them in a Lagrangian sense. Derivatives are computed on the mesh using the values of nodal variables but material property variations are measured by the particles. Advantages include the fact that the method is geometrically simple for relatively complex deformations and uses a horses-for-courses approach with the mesh doing what it is best at - derivatives, fast solutions and the particles doing their part on the Lagrangian components of the problem. Major disadvantages include the fact that the particle properties and the mesh properties must be synchronized and this may involve some averaging to the mesh. Worse, there need to be more particles than grid points for the algorithm to work, but this means there are more particle degrees of freedom than can be constrained by the mesh --- this means that multiple particle configurations can produce the same solution on the mesh some of which may be unstable and incapable of being damped during the solution. Lagrangian/Eulerian meshes As we hinted earlier, Lagrangian formulations eliminate convective terms from the equations but at the expense of geometrical complexity. Finite elements are not particularly troubled by complex meshes provided the elements do not become too distorted. Thus, for moderate deformations, advection of the grid points provides a simple way to eliminate the much more complicated advection terms from the differential equations. For fluids, however, the deformation ultimately ruins the ability of the mesh to converge on the correct solution and the results have to be interpolated to a new mesh, losing some accuracy as a result. ALE An alternative to this approach is to use ALE which is, sad to say, an acronym for Arbitrary-Lagrangian-Eulerian. The node points are advected but not at the same rate as the flow. This can be used to prevent mesh tangling while still mitigating the worst difficulties associated with the advection term. But, on the other hand, it hasn't entirely eliminated that term. DLR A different method is to start with an optimal, Delaunay, triangulation and allow the grid points to move locked into the fluid so as to eliminate the convection term from the differential equation. The mesh connections are checked at each timestep to see if the triangulation is still optimal. From near-optimal to optimal in this way involves exchanging a few node connections. This continual updating avoids ever being in a situation where full remeshing is required and thus avoids the loss of information during that process. Additional nodes can be introduced to improve resolution where required. The disadvantage of this procedure is in tracking history variables such as stress rate. These are only defined on the element interiors not the nodes, so the result is the tracking of a somewhat smoothed quantity. (brought to you by the Natural Element people)","title":"NumericalMethodsPrimer 1"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#computational-geodynamics-introduction-to-numerical-methods","text":"\\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\]","title":"Computational Geodynamics: Introduction to Numerical Methods"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#introductory-remarks","text":"We have considered the application of mathematical reasoning to construct models of the behaviour of physical systems. Unfortunately, most of these models do not have analytic solutions or the solutions are so complicated that they don't help us understand the problem. The alternative is to solve such problems numerically (approximately). Such solutions are also possible for far more elaborate systems than we would even consider trying to obtain exact solutions for. On the other hand, numerical modeling only provides solutions to mathematical equations and only approximate solutions at that. Unless the modeler has an understanding of the methods and the models themselves, then the output of some computer code may be terribly misleading. It is always necessary to design a numerical experiment as carefully as one would a physical experiment --- although it won't blow up and kill you. It is possible to change the laws of physics in the numerical model often inadvertantly, so a careful analysis of the problem first is not a bad idea. This part of the course contains some really hard material. This is deliberate because the workings of most computer codes are actually based on some astonishingly intricate mathematics. This is really intended to provide a decent reference for people coming back to use finite elements or other numerical methods later.","title":"Introductory Remarks"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#a-variety-of-numerical-methods","text":"We will discuss a number of different numerical solution techniques which are suitable for solid Earth dynamics problems. Obviously only a brief discussion of any one method is possible, and more methods are inevitably available. Some methods work well for particular problem and others roll over and die with hardly any sign of impending doom. We need to know what the tools in our toolbox are for and what will cause them to break before we try to solve serious problems with them. After all, in uncharted territory, a numerical instability might look enticingly like an exiting new result. Obviously there is not room to do justice to any particular method and I will concentrate on how the finite element method works (though for some things other methods have many advantages).","title":"A Variety of Numerical Methods"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#finite-differences","text":"If we have encountered calculus at all, then we are already familiar with finite differences. Where we would, in determining a derivative, consider the limit of a quantity over a small interval as that interval shrinks to an infinitessimal size, in finite differences, we compute the value of such quantities for small, finite intervals. Typically we use regular meshes of points on which to compute the differences (but we don't have to !). Finite Difference methods are simple and can be very fast. They can be intuitively easy to understand and program and are also universally applied. They may encounter difficulties when extreme variations in properties occur from one grid point to another, particularly if there are discontinuities.","title":"Finite Differences"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#finite-elements","text":"Finite elements work from a variational principle (more, much more, later) which is an integral version of the governing equations. They work with a spatial discretization into a mesh spanned by elements with unknowns allocated to their vertices. The application of finite elements to complex problems and those with very complex domains is a programmers joy since the entire methodology match object - oriented programming techniques. The flip-side of this is that the construction of the numerical equations which need to be solved may take considerably longer than the actual solution process. The retention of the integral form can be beneficial for problems with difficult boundary conditions or discontinuities which can be integrated. Variational methods are, however, difficult to constrain particularly when iterative, implicit solution methods are used.","title":"Finite Elements"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#finite-volumes","text":"In finite volumes one combines some of the best features of finite elements and finite differences. The method is grid based and works with both the original mesh and its dual. The formulation starts with a weak form of the equations based on volume fluxes into the local volume surrounding a node (based on the dual mesh). These integrals are then converted via Gauss' theorem, into surface integrals on the edges/faces of the dual mesh. Depending on the subsequent discretization, the resulting algorithm can be akin to finite differences or to finite elements (whether surface integrals remain in the formulation or are replaced by some differencing form) Advantages include the fact that the surface integral formulation can be tailored to satisfy local constraints without additional messing about thus making for very rapid solution times. The formulation can also deal with arbitrary mesh configurations. Disadvantages include a difficulty in applying some boundary conditions since the dual surface is not defined outside the mesh (formally).","title":"Finite Volumes"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#natural-elements","text":"An extension of finite elements using the theory of Natural Neighbour interpolation schemes to provide shape functions for all grid point arrangements. A best (Delaunay) triangulation is defined for such a set of points and interpolation functions exist which give an optimal representation of the interpolant. These functions can provide a basis for a finite element scheme. Advantages include the fact that elements can be highly distorted but this does not affect the convergence of the method in the same catastrophic way as for normal schemes. Disadvantages include the fact that shape functions overlap other elements and therefore precise integration is difficult. Also this is a relatively novel method and there are some odds and ends to be ironed out.","title":"Natural Elements"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#spectral-timespace","text":"We saw in the theoretical treatment of instabilities in a layer how beneficial it can be to deal with harmonic functions in one or more dimensions. This turns the problem from a partial differential equation into a set of ordinary differential equations for the different wavenumbers. The method works well for systems with homogeneous material properties otherwise spatial variations in these properties are couple the different wavenumbers together and add layer upon layer of complexity to the problem. Due to the existence of the fast fourier transform, these methods, when they can be applied, are potentially very quick. They are limited to relatively regular geometries, however.","title":"Spectral (time/space)"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#discrete-elements","text":"Meshless methods which deal with either actual discrete systems such as large systems of granular materials or systems in which notional particles represent elements of the continuum. The method works by treating simple interactions of very many particles dynamically. For each particle an explicit solution of F=ma F=ma for translations and L=I\\ddot{\\theta} L=I\\ddot{\\theta} for the rotations is found in reponse to the interaction forces with every other particle. In discrete elements such interactions are usually local (e.g. only when particles are in contact) and this makes the system manageable in size. Very good for treatment of fracture and dynamic responses of granular systems but can be hampered by the time taken for the fully explicit nature of the algorithm, i.e. elastic waves must be resolved in order to model deformation. Also, these methods are based on particle interaction functions and so properties of the continuum such as viscosity are not direct inputs but have to be computed as an average response of the system (just like the real thing). Compare this to molecular dynamics simulations.","title":"Discrete Elements"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#smooth-particle-hydrodynamics","text":"Another meshless method in which \"particles\" are the centres of smooth functions such as gaussians. These can be used to interpolate any field across the solution domain. They are also differentiable and can therefore represent differential equations relatively efficiently. Very useful for high velocity flows and astrophysics type problems. Historically there have been problems representing boundary conditions and viscosity so not ideal for the highly viscous and/or elastic type problems of the solid earth.","title":"Smooth Particle Hydrodynamics"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#particle-in-cell-methods","text":"In these methods both particles and a mesh exist. The mesh supplies the velocities which move particles around, but the particles carry information with them in a Lagrangian sense. Derivatives are computed on the mesh using the values of nodal variables but material property variations are measured by the particles. Advantages include the fact that the method is geometrically simple for relatively complex deformations and uses a horses-for-courses approach with the mesh doing what it is best at - derivatives, fast solutions and the particles doing their part on the Lagrangian components of the problem. Major disadvantages include the fact that the particle properties and the mesh properties must be synchronized and this may involve some averaging to the mesh. Worse, there need to be more particles than grid points for the algorithm to work, but this means there are more particle degrees of freedom than can be constrained by the mesh --- this means that multiple particle configurations can produce the same solution on the mesh some of which may be unstable and incapable of being damped during the solution.","title":"Particle in Cell methods"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#lagrangianeulerian-meshes","text":"As we hinted earlier, Lagrangian formulations eliminate convective terms from the equations but at the expense of geometrical complexity. Finite elements are not particularly troubled by complex meshes provided the elements do not become too distorted. Thus, for moderate deformations, advection of the grid points provides a simple way to eliminate the much more complicated advection terms from the differential equations. For fluids, however, the deformation ultimately ruins the ability of the mesh to converge on the correct solution and the results have to be interpolated to a new mesh, losing some accuracy as a result.","title":"Lagrangian/Eulerian meshes"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#ale","text":"An alternative to this approach is to use ALE which is, sad to say, an acronym for Arbitrary-Lagrangian-Eulerian. The node points are advected but not at the same rate as the flow. This can be used to prevent mesh tangling while still mitigating the worst difficulties associated with the advection term. But, on the other hand, it hasn't entirely eliminated that term.","title":"ALE"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-1.html#dlr","text":"A different method is to start with an optimal, Delaunay, triangulation and allow the grid points to move locked into the fluid so as to eliminate the convection term from the differential equation. The mesh connections are checked at each timestep to see if the triangulation is still optimal. From near-optimal to optimal in this way involves exchanging a few node connections. This continual updating avoids ever being in a situation where full remeshing is required and thus avoids the loss of information during that process. Additional nodes can be introduced to improve resolution where required. The disadvantage of this procedure is in tracking history variables such as stress rate. These are only defined on the element interiors not the nodes, so the result is the tracking of a somewhat smoothed quantity. (brought to you by the Natural Element people)","title":"DLR"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-2.html","text":"\\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\] We start with the numerical solution of a very simple differential equation. In fact we choose something simple enough that we already know the answer. \\begin{equation} \\frac{d\\theta}{dt} = - k \\theta \\label{eq:ode_decay} \\end{equation} \\begin{equation} \\frac{d\\theta}{dt} = - k \\theta \\label{eq:ode_decay} \\end{equation} This is the equation which governs radioactive decay, in which case \\(\\theta \\) is the amount of the radioactive isotope remaining and \\(d\\theta / dt\\) is the activity that we can measure. \\(k\\) is closely related to the half life. The solution to this equation is \\[ \\theta(t) = \\theta_0 e^{-kt} \\] where \\(\\theta_0\\) is the amount of the radioactive material remaining. The same equation also describes the cooling of, say, a cup of coffee. In this case we interpret \\(\\theta \\) as the excess temperature (above room temperature). {. width=\"75%\"} Marching through time in small increments We want to march forward in time from our starting point where \\(\\theta = \\theta_0\\) to obtain the value of \\( \\theta \\) at later times. To do this, we need to approximate the original differential equation, and, in particular, the value of the time derivative at each time. There are a number of ways to do this. A first order numerical approximation Assume that the variation in \\(\\theta(t) \\) is linear, i.e. \\[ \\theta(t') = \\theta _ n + \\beta t' \\] where we use a local time coordinate \\(t' = t - n\\Delta t\\), so that when we differentiate \\[ \\frac{d \\theta}{dt} = \\beta \\] To determine the approximation for the derivative therefore becomes the solution to the following equation: \\[ \\begin{split} & \\theta _ {n+1} = \\theta _ n + \\beta \\Delta t \\ & \\Rightarrow \\beta = \\frac{d \\theta}{dt} = \\frac{\\theta _ {n+1} - \\theta _ n}{\\Delta t} \\end{split} \\] This is a first order difference expression for the derivative which we substitute into the original differential equation (\\( \\ref{eq:ode_decay}\\) ) at the current timestep \\[ \\frac{\\theta _ {n+1} - \\theta _ n}{\\Delta t} = - k \\theta _ n \\] This rearranges to give us a time-marching algorithm: \\[ \\theta _ {n+1} = \\theta _ n (1-k \\Delta t) \\] It is an indication of the fact that this problem is really not all that difficult that this difference equation can be written recursively to give: \\[ \\theta_{n+1} = \\theta_0 (1-k \\Delta t)^n \\] In a moment we will compute some values for this expression to see how accurate it is. First we consider whether we can improve the accuracy of the approximation by doing a bit more work. Higher order expansion First we try fitting the local expansion for \\(\\theta \\) through an additional point. This time we assume that the variation in \\(\\theta(t)\\) is quadratic, i.e. \\[ \\theta(t') = \\theta _ {n-1} + \\beta t' + \\gamma {t'}^2 \\] The local time coordinate is \\(t' = t - (n-1)\\Delta t \\), and when we differentiate \\begin{equation} \\frac{d \\theta}{dt} = \\beta + 2 \\gamma t' \\label{eq:dthdt2} \\end{equation} To solve for \\(\\beta \\) and \\(\\gamma \\) we fit the curve through the sample points: \\[ \\begin{split} \\theta _ n &= \\theta _ {n-1} + \\beta \\Delta t + \\gamma (\\Delta t)^2 \\ \\theta _ {n+1} &= \\theta _ {n-1} + 2 \\beta \\Delta t + 4 \\gamma (\\Delta t)^2 \\end{split} \\] Which we can solve to give this: \\[ \\begin{split} \\beta &= \\left( 4 \\theta _ n - \\theta_{n+1} - 3\\theta _ {n-1} \\right) \\frac{1}{2\\Delta t} \\ \\gamma &= \\left( \\theta _ {n+1} + \\theta _ {n-1} -2 \\theta _ n \\right) \\frac{1}{2\\Delta t^2} \\end{split} \\] By substituting into \\( (\\ref{eq:dthdt2}) \\), and then into the original differential equation \\( (\\ref{eq:ode_decay}) \\) we obtain the following \\[ \\left. \\frac{d\\theta}{dt} \\right| _ {t=n\\Delta t} = \\beta + 2\\gamma \\Delta t = \\frac{1}{2\\Delta t} \\left( \\theta _ {n+1} - \\theta _ {n-1} \\right) = -k \\theta _ n \\] The difference approximation to the derivative turns out to be the average of the expressions for the previous derivative and the new derivative. We have now included information about the current timestep and the previous timestep in our expression for the value of \\(\\theta \\) at the forthcoming timestep: \\[ \\theta _ {n+1} = \\theta _ {n-1} -2k \\theta _ n \\Delta t \\] A comparison of the two schemes How well do these two methods perform, and what difference does the choice of \\(\\Delta t \\) make on the accuracy ? t \\(^1\\theta _ H\\) \\(^1\\theta _ h\\) \\(^2\\theta _ H\\) \\(^2\\theta _ h\\) Exact 0.0 1.0000 1.0000 1.0000 1.0000 1.0000 0.1 - 0.9000 - 0.9000 0.9048 0.2 0.8000 0.8100 0.8000 0.8200 0.8187 0.3 - 0.7290 - 0.7360 0.7408 0.4 0.6400 0.6561 0.6800 0.6728 0.6703 0.5 - 0.5905 - 0.6014 0.6063 0.6 0.5120 0.5314 0.5280 0.5525 0.5488 0.7 - 0.4783 - 0.4909 0.4966 0.8 0.4096 0.4305 0.4688 0.4543 0.4483 0.9 - 0.3874 - 0.4001 0.4066 1.0 0.3277 0.3487 0.3405 0.3743 0.3679 Results of the numerical simulation of the decay equation. \\(^1\\theta_H \\) is from the first order expansion of \\(\\theta(t) \\), at a timestep of \\( \\Delta t=0.2 \\), and \\(^2\\theta_H \\) is the second order expansion at \\( \\Delta t=0.2 \\); other calculations are at \\(\\Delta t = 0.1 \\). \\(^1\\theta_h\\) is the linear expansion, \\(^2\\theta_h\\) results from the quadratic expansion of \\(\\theta(t)\\). The actual solution is also shown. The results are more accurate when a smaller timestep is used although it requires more computation to achieve the greater accuracy. Higher order expansion also increases the accuracy and may be more efficient in terms of the number of computations required for a given level of accuracy. Note, however, that the supposedly better quadratic expansion produces an error which oscillates as time increases. Does this error grow ? Does this make second order expansions useless ? Second Order Runge-Kutta {. width=\"75%\"} A different way to derive a second order method The Runge-Kutta approach to higher order integration methods is illustrated above. The idea is to estimate the gradient \\(d \\theta / d t\\) at the half way point between two timestep values. This is done in two stages. Initially a first order estimate, \\(\\hat{\\theta}\\) is made for the value of the function \\( \\theta \\) at \\(t=t+\\Delta t /2\\) in the future. This value is then subsituted into the differential equation to obtain the estimate for the gradient at this time. The revised gradient is then used to update the original \\(\\theta(t)\\) by an entire timestep. The first order step is \\[ \\begin{split} \\hat{\\theta}(t+\\Delta t /2) & = \\theta(t) + \\left. \\frac{d \\theta}{d t} \\right| _ t \\frac{\\Delta t}{2} \\ &= \\theta(t) \\left[ 1-\\frac{k\\Delta t}{2} \\right] \\end{split} \\] Substitute to estimate the gradient at the mid-point \\[ \\left. \\frac{d \\theta}{d t} \\right| _ {t+\\Delta t /2} \\approx -k \\theta(t) \\left[ 1-\\frac{k\\Delta t}{2} \\right] \\] Use this value as the average gradient over the interval \\( t\\rightarrow t+\\Delta t \\) to update \\(\\theta \\) \\[ \\begin{split} \\theta(t+\\Delta t) & \\approx \\theta(t) + \\delta t \\left( -k \\theta(t) \\left[ 1-\\frac{k\\Delta t}{2} \\right] \\right) \\ & \\approx \\theta(t) \\left( 1 - k \\Delta t + k^2 \\frac{\\Delta t^2}{2} \\right) \\end{split} \\] It's worth noting that the Taylor expansion of the solution should look like \\[ e^{-kt} = 1 - kt + \\frac{k^2 t^2}{2!} - \\frac{k^3 t^3}{3!} + \\ldots \\] The Runge Kutta method can be extended by repeating the estimates on smaller regions of the interval. The usual choice is fourth order RK. This is largely because, obviously, it's accurate to fourth order, but also because the number of operations to go higher than fourth order is disproportionately large. See Numerical Recipes for a discussion on this and better methods for ODE's. Our First Code The simplest way to answer our earlier question is to code the methods and see. The following is a script which computes the first and second order expressions and the Runge-Kutta method, and compares with the \"exact\" solution (which is numerical as well, of course, but computed through series expansions and the like). # Compare various ways to integrate the # ODE for radioactive decay / Newton's law of cooling. # Assume decay constant of 1.0 import math timestep = 0.1 ; # Initial value ... theta1_0 = 1.0 theta2_0 = 1.0 thetaRK2_0 = 1.0 # First timestep (by hand, then automate for subsequent cases) theta1_1 = 0.9 theta2_1 = 0.9 thetaRK_1 = 0.905 # Now we have enough information to automate all the methods for i in range ( 2 , 1001 ): time = timestep * i exact = 1.0 * math . exp ( - 1.0 * time ) theta1_2 = theta1_1 * ( 1.0 - timestep ) theta2_2 = theta2_0 - 2.0 * theta2_1 * timestep thetaRK_2 = thetaRK_1 * ( 1.0 - timestep + 0.5 * timestep * timestep ) # output data every 25th step if not ( i % 25 ): print \"Timestep {:04d} {:.4f} \\t {:.4e} \\t {:.4e} \\t {:.4e} \\t {:.4e}\" . format ( i , time , theta1_2 , theta2_2 , thetaRK_2 , exact ) # keep old values (not all are needed, of course) theta1_0 = theta1_1 theta2_0 = theta2_1 thetaRK_0 = thetaRK_1 theta1_1 = theta1_2 theta2_1 = theta2_2 thetaRK_1 = thetaRK_2 \"\"\" Output: Timestep 0025 2.5000 +7.1790e-02 +5.2117e-02 +8.2455e-02 +8.2085e-02 Timestep 0050 5.0000 +5.1538e-03 +3.7201e-01 +6.7987e-03 +6.7379e-03 Timestep 0075 7.5000 +3.6999e-04 -4.4304e+00 +5.6059e-04 +5.5308e-04 Timestep 0100 10.0000 +2.6561e-05 +5.3757e+01 +4.6223e-05 +4.5400e-05 Timestep 0125 12.5000 +1.9068e-06 -6.5219e+02 +3.8113e-06 +3.7267e-06 Timestep 0150 15.0000 +1.3689e-07 +7.9124e+03 +3.1426e-07 +3.0590e-07 Timestep 0175 17.5000 +9.8274e-09 -9.5993e+04 +2.5912e-08 +2.5110e-08 Timestep 0200 20.0000 +7.0551e-10 +1.1646e+06 +2.1366e-09 +2.0612e-09 Timestep 0225 22.5000 +5.0648e-11 -1.4129e+07 +1.7617e-10 +1.6919e-10 Timestep 0250 25.0000 +3.6360e-12 +1.7141e+08 +1.4526e-11 +1.3888e-11 Timestep 0275 27.5000 +2.6103e-13 -2.0796e+09 +1.1977e-12 +1.1400e-12 Timestep 0300 30.0000 +1.8739e-14 +2.5230e+10 +9.8758e-14 +9.3576e-14 Timestep 0325 32.5000 +1.3453e-15 -3.0609e+11 +8.1431e-15 +7.6812e-15 Timestep 0350 35.0000 +9.6578e-17 +3.7135e+12 +6.7143e-16 +6.3051e-16 Timestep 0375 37.5000 +6.9333e-18 -4.5052e+13 +5.5363e-17 +5.1756e-17 Timestep 0400 40.0000 +4.9774e-19 +5.4658e+14 +4.5649e-18 +4.2484e-18 Timestep 0425 42.5000 +3.5733e-20 -6.6311e+15 +3.7640e-19 +3.4873e-19 Timestep 0450 45.0000 +2.5652e-21 +8.0449e+16 +3.1036e-20 +2.8625e-20 Timestep 0475 47.5000 +1.8416e-22 -9.7602e+17 +2.5590e-21 +2.3497e-21 Timestep 0500 50.0000 +1.3221e-23 +1.1841e+19 +2.1100e-22 +1.9287e-22 Timestep 0525 52.5000 +9.4911e-25 -1.4366e+20 +1.7398e-23 +1.5832e-23 Timestep 0550 55.0000 +6.8137e-26 +1.7429e+21 +1.4346e-24 +1.2996e-24 Timestep 0575 57.5000 +4.8915e-27 -2.1144e+22 +1.1829e-25 +1.0668e-25 Timestep 0600 60.0000 +3.5116e-28 +2.5653e+23 +9.7532e-27 +8.7565e-27 Timestep 0625 62.5000 +2.5210e-29 -3.1122e+24 +8.0420e-28 +7.1878e-28 Timestep 0650 65.0000 +1.8098e-30 +3.7757e+25 +6.6310e-29 +5.9001e-29 Timestep 0675 67.5000 +1.2993e-31 -4.5807e+26 +5.4675e-30 +4.8431e-30 Timestep 0700 70.0000 +9.3273e-33 +5.5574e+27 +4.5082e-31 +3.9754e-31 Timestep 0725 72.5000 +6.6961e-34 -6.7423e+28 +3.7172e-32 +3.2632e-32 Timestep 0750 75.0000 +4.8071e-35 +8.1797e+29 +3.0650e-33 +2.6786e-33 Timestep 0775 77.5000 +3.4510e-36 -9.9237e+30 +2.5273e-34 +2.1988e-34 Timestep 0800 80.0000 +2.4775e-37 +1.2040e+32 +2.0838e-35 +1.8049e-35 Timestep 0825 82.5000 +1.7786e-38 -1.4606e+33 +1.7182e-36 +1.4815e-36 Timestep 0850 85.0000 +1.2768e-39 +1.7721e+34 +1.4167e-37 +1.2161e-37 Timestep 0875 87.5000 +9.1663e-41 -2.1499e+35 +1.1682e-38 +9.9824e-39 Timestep 0900 90.0000 +6.5805e-42 +2.6082e+36 +9.6321e-40 +8.1940e-40 Timestep 0925 92.5000 +4.7241e-43 -3.1643e+37 +7.9421e-41 +6.7261e-41 Timestep 0950 95.0000 +3.3914e-44 +3.8390e+38 +6.5486e-42 +5.5211e-42 Timestep 0975 97.5000 +2.4347e-45 -4.6575e+39 +5.3996e-43 +4.5320e-43 Timestep 1000 100.0000 +1.7479e-46 +5.6505e+40 +4.4522e-44 +3.7201e-44 \"\"\" Clearly, the quadratic expansion does not do a good job in this particular case, since the error is a growing instability which makes the solution useless almost immediately. However, the second order Runge-Kutta method is very accurate. This gives a clear warning that numerical solution of equations is partly an art (though the stability or otherwise of a given method can actually be proven formally in many cases). Explicit versus Implicit Methods} The use of implicit solution methods is crucial when we come to the mantle flow problem (and to elasticity). It is a deceptively simple concept which we use to eliminate oscillatory errors and other problems associated with stiff systems, and in other cases where we want to take large timesteps. Again, Numerical Recipes provides a good example of the way in which implicit methods avoid the shuffling timesteps which result when an otherwise unimportant component of the system blows up. To obtain an implicit solution, we express our updated timestep in terms of values of derivatives etc which are evaluated at that timestep. This may result in dependencies which mean large systems of simultaneous equations need to be solved. But our simple example can also be solved implicitly as follows \\[ \\theta(t+\\Delta t) = \\left. \\frac{d \\theta}{d t} \\right| _ {t+\\Delta t} + \\theta(t) \\] Easy to write down, but \\(\\left. d \\theta/ dt \\right| _ {t+\\Delta t}\\) is not known until \\(\\theta(t+\\Delta t) \\) is known. In this case, however, we simply substitute \\(\\left. d \\theta/ dt \\right| _ {t+\\Delta t}= -k \\theta(t+\\Delta t) \\) from the differential equation and write \\[ \\begin{split} \\theta(t+\\Delta t) & = -k \\theta(t+\\Delta t) \\Delta t + \\theta(t) \\ & = \\frac{\\theta(t) }{ 1 + k\\Delta t} \\end{split} \\] A higher order implicit method can be found by considering how the second order Runge-Kutta method constructs its estimates. In place of the first order approximation above, we try \\[ \\theta(t+\\Delta t) = \\left. \\frac{d \\theta}{d t} \\right| _ {t+\\Delta t/2} + \\theta(t) \\] where we assume \\[ \\left. \\frac{d \\theta}{d t} \\right| _ {t+\\Delta t/2} = \\frac{1}{2} \\left. \\frac{d \\theta}{d t} \\right| _ {t+\\Delta t} + \\frac{1}{2} \\left. \\frac{d \\theta}{d t} \\right| _ {t} \\] Substituting from our original differential equation now gives \\[ \\begin{split} \\theta(t+\\Delta t) &= -k\\frac{\\theta(t+\\Delta t) + \\theta(t)}{2} \\Delta t + \\theta(t) \\ &= \\theta(t) \\frac{1-k\\Delta t / 2}{1+k\\Delta t / 2} \\end{split} \\] \\(t\\) \\(^1\\theta_i\\) \\(^2\\theta_i\\) Exact 0.0 1.0000 1.0000 1.0000 0.1 0.9091 0.9048 0.9048 0.2 0.8264 0.8186 0.8187 0.3 0.7513 0.7406 0.7408 0.4 0.6830 0.6701 0.6703 0.5 0.6209 0.6063 0.6063 Results of the two implicit schemes derived above for the numerical integration of the decay equation The first order scheme is of approximately the same accuracy as the explicit scheme, not surprisingly, but the mid-point implicit method has very impressive results considering that there is very little extra work involved in deriving the formulation. Can this strategy stabilize the quadratic expansion which proved to be so unstable in explicit form ? Evaluating the derivatives at time \\(t=(n+1)\\Delta t\\) gives \\[ \\left. \\frac{d \\theta}{d t} \\right| _ {(n+1)\\Delta t} = 3 \\theta _ {n+1} - 4\\theta _ n + \\theta _ {n-1} \\] which we substitute into the differential equation at \\(t=(n+1)\\Delta t\\) to give (eventually) \\[ \\theta _ {n+1} = \\frac{4\\theta _ n - \\theta _ {n-1}}{3+2k\\Delta t} \\] This method is stable although not as accurate as the second order Runge-Kutta Scheme. It is trivial to modify the Python script to demonstrate this. References ...","title":"NumericalMethodsPrimer 2"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-2.html#a-first-order-numerical-approximation","text":"Assume that the variation in \\(\\theta(t) \\) is linear, i.e. \\[ \\theta(t') = \\theta _ n + \\beta t' \\] where we use a local time coordinate \\(t' = t - n\\Delta t\\), so that when we differentiate \\[ \\frac{d \\theta}{dt} = \\beta \\] To determine the approximation for the derivative therefore becomes the solution to the following equation: \\[ \\begin{split} & \\theta _ {n+1} = \\theta _ n + \\beta \\Delta t \\ & \\Rightarrow \\beta = \\frac{d \\theta}{dt} = \\frac{\\theta _ {n+1} - \\theta _ n}{\\Delta t} \\end{split} \\] This is a first order difference expression for the derivative which we substitute into the original differential equation (\\( \\ref{eq:ode_decay}\\) ) at the current timestep \\[ \\frac{\\theta _ {n+1} - \\theta _ n}{\\Delta t} = - k \\theta _ n \\] This rearranges to give us a time-marching algorithm: \\[ \\theta _ {n+1} = \\theta _ n (1-k \\Delta t) \\] It is an indication of the fact that this problem is really not all that difficult that this difference equation can be written recursively to give: \\[ \\theta_{n+1} = \\theta_0 (1-k \\Delta t)^n \\] In a moment we will compute some values for this expression to see how accurate it is. First we consider whether we can improve the accuracy of the approximation by doing a bit more work.","title":"A first order numerical approximation"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-2.html#higher-order-expansion","text":"First we try fitting the local expansion for \\(\\theta \\) through an additional point. This time we assume that the variation in \\(\\theta(t)\\) is quadratic, i.e. \\[ \\theta(t') = \\theta _ {n-1} + \\beta t' + \\gamma {t'}^2 \\] The local time coordinate is \\(t' = t - (n-1)\\Delta t \\), and when we differentiate \\begin{equation} \\frac{d \\theta}{dt} = \\beta + 2 \\gamma t' \\label{eq:dthdt2} \\end{equation} To solve for \\(\\beta \\) and \\(\\gamma \\) we fit the curve through the sample points: \\[ \\begin{split} \\theta _ n &= \\theta _ {n-1} + \\beta \\Delta t + \\gamma (\\Delta t)^2 \\ \\theta _ {n+1} &= \\theta _ {n-1} + 2 \\beta \\Delta t + 4 \\gamma (\\Delta t)^2 \\end{split} \\] Which we can solve to give this: \\[ \\begin{split} \\beta &= \\left( 4 \\theta _ n - \\theta_{n+1} - 3\\theta _ {n-1} \\right) \\frac{1}{2\\Delta t} \\ \\gamma &= \\left( \\theta _ {n+1} + \\theta _ {n-1} -2 \\theta _ n \\right) \\frac{1}{2\\Delta t^2} \\end{split} \\] By substituting into \\( (\\ref{eq:dthdt2}) \\), and then into the original differential equation \\( (\\ref{eq:ode_decay}) \\) we obtain the following \\[ \\left. \\frac{d\\theta}{dt} \\right| _ {t=n\\Delta t} = \\beta + 2\\gamma \\Delta t = \\frac{1}{2\\Delta t} \\left( \\theta _ {n+1} - \\theta _ {n-1} \\right) = -k \\theta _ n \\] The difference approximation to the derivative turns out to be the average of the expressions for the previous derivative and the new derivative. We have now included information about the current timestep and the previous timestep in our expression for the value of \\(\\theta \\) at the forthcoming timestep: \\[ \\theta _ {n+1} = \\theta _ {n-1} -2k \\theta _ n \\Delta t \\]","title":"Higher order expansion"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-2.html#a-comparison-of-the-two-schemes","text":"How well do these two methods perform, and what difference does the choice of \\(\\Delta t \\) make on the accuracy ? t \\(^1\\theta _ H\\) \\(^1\\theta _ h\\) \\(^2\\theta _ H\\) \\(^2\\theta _ h\\) Exact 0.0 1.0000 1.0000 1.0000 1.0000 1.0000 0.1 - 0.9000 - 0.9000 0.9048 0.2 0.8000 0.8100 0.8000 0.8200 0.8187 0.3 - 0.7290 - 0.7360 0.7408 0.4 0.6400 0.6561 0.6800 0.6728 0.6703 0.5 - 0.5905 - 0.6014 0.6063 0.6 0.5120 0.5314 0.5280 0.5525 0.5488 0.7 - 0.4783 - 0.4909 0.4966 0.8 0.4096 0.4305 0.4688 0.4543 0.4483 0.9 - 0.3874 - 0.4001 0.4066 1.0 0.3277 0.3487 0.3405 0.3743 0.3679 Results of the numerical simulation of the decay equation. \\(^1\\theta_H \\) is from the first order expansion of \\(\\theta(t) \\), at a timestep of \\( \\Delta t=0.2 \\), and \\(^2\\theta_H \\) is the second order expansion at \\( \\Delta t=0.2 \\); other calculations are at \\(\\Delta t = 0.1 \\). \\(^1\\theta_h\\) is the linear expansion, \\(^2\\theta_h\\) results from the quadratic expansion of \\(\\theta(t)\\). The actual solution is also shown. The results are more accurate when a smaller timestep is used although it requires more computation to achieve the greater accuracy. Higher order expansion also increases the accuracy and may be more efficient in terms of the number of computations required for a given level of accuracy. Note, however, that the supposedly better quadratic expansion produces an error which oscillates as time increases. Does this error grow ? Does this make second order expansions useless ?","title":"A comparison of the two schemes"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-2.html#second-order-runge-kutta","text":"{. width=\"75%\"} A different way to derive a second order method The Runge-Kutta approach to higher order integration methods is illustrated above. The idea is to estimate the gradient \\(d \\theta / d t\\) at the half way point between two timestep values. This is done in two stages. Initially a first order estimate, \\(\\hat{\\theta}\\) is made for the value of the function \\( \\theta \\) at \\(t=t+\\Delta t /2\\) in the future. This value is then subsituted into the differential equation to obtain the estimate for the gradient at this time. The revised gradient is then used to update the original \\(\\theta(t)\\) by an entire timestep. The first order step is \\[ \\begin{split} \\hat{\\theta}(t+\\Delta t /2) & = \\theta(t) + \\left. \\frac{d \\theta}{d t} \\right| _ t \\frac{\\Delta t}{2} \\ &= \\theta(t) \\left[ 1-\\frac{k\\Delta t}{2} \\right] \\end{split} \\] Substitute to estimate the gradient at the mid-point \\[ \\left. \\frac{d \\theta}{d t} \\right| _ {t+\\Delta t /2} \\approx -k \\theta(t) \\left[ 1-\\frac{k\\Delta t}{2} \\right] \\] Use this value as the average gradient over the interval \\( t\\rightarrow t+\\Delta t \\) to update \\(\\theta \\) \\[ \\begin{split} \\theta(t+\\Delta t) & \\approx \\theta(t) + \\delta t \\left( -k \\theta(t) \\left[ 1-\\frac{k\\Delta t}{2} \\right] \\right) \\ & \\approx \\theta(t) \\left( 1 - k \\Delta t + k^2 \\frac{\\Delta t^2}{2} \\right) \\end{split} \\] It's worth noting that the Taylor expansion of the solution should look like \\[ e^{-kt} = 1 - kt + \\frac{k^2 t^2}{2!} - \\frac{k^3 t^3}{3!} + \\ldots \\] The Runge Kutta method can be extended by repeating the estimates on smaller regions of the interval. The usual choice is fourth order RK. This is largely because, obviously, it's accurate to fourth order, but also because the number of operations to go higher than fourth order is disproportionately large. See Numerical Recipes for a discussion on this and better methods for ODE's.","title":"Second Order Runge-Kutta"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-2.html#our-first-code","text":"The simplest way to answer our earlier question is to code the methods and see. The following is a script which computes the first and second order expressions and the Runge-Kutta method, and compares with the \"exact\" solution (which is numerical as well, of course, but computed through series expansions and the like). # Compare various ways to integrate the # ODE for radioactive decay / Newton's law of cooling. # Assume decay constant of 1.0 import math timestep = 0.1 ; # Initial value ... theta1_0 = 1.0 theta2_0 = 1.0 thetaRK2_0 = 1.0 # First timestep (by hand, then automate for subsequent cases) theta1_1 = 0.9 theta2_1 = 0.9 thetaRK_1 = 0.905 # Now we have enough information to automate all the methods for i in range ( 2 , 1001 ): time = timestep * i exact = 1.0 * math . exp ( - 1.0 * time ) theta1_2 = theta1_1 * ( 1.0 - timestep ) theta2_2 = theta2_0 - 2.0 * theta2_1 * timestep thetaRK_2 = thetaRK_1 * ( 1.0 - timestep + 0.5 * timestep * timestep ) # output data every 25th step if not ( i % 25 ): print \"Timestep {:04d} {:.4f} \\t {:.4e} \\t {:.4e} \\t {:.4e} \\t {:.4e}\" . format ( i , time , theta1_2 , theta2_2 , thetaRK_2 , exact ) # keep old values (not all are needed, of course) theta1_0 = theta1_1 theta2_0 = theta2_1 thetaRK_0 = thetaRK_1 theta1_1 = theta1_2 theta2_1 = theta2_2 thetaRK_1 = thetaRK_2 \"\"\" Output: Timestep 0025 2.5000 +7.1790e-02 +5.2117e-02 +8.2455e-02 +8.2085e-02 Timestep 0050 5.0000 +5.1538e-03 +3.7201e-01 +6.7987e-03 +6.7379e-03 Timestep 0075 7.5000 +3.6999e-04 -4.4304e+00 +5.6059e-04 +5.5308e-04 Timestep 0100 10.0000 +2.6561e-05 +5.3757e+01 +4.6223e-05 +4.5400e-05 Timestep 0125 12.5000 +1.9068e-06 -6.5219e+02 +3.8113e-06 +3.7267e-06 Timestep 0150 15.0000 +1.3689e-07 +7.9124e+03 +3.1426e-07 +3.0590e-07 Timestep 0175 17.5000 +9.8274e-09 -9.5993e+04 +2.5912e-08 +2.5110e-08 Timestep 0200 20.0000 +7.0551e-10 +1.1646e+06 +2.1366e-09 +2.0612e-09 Timestep 0225 22.5000 +5.0648e-11 -1.4129e+07 +1.7617e-10 +1.6919e-10 Timestep 0250 25.0000 +3.6360e-12 +1.7141e+08 +1.4526e-11 +1.3888e-11 Timestep 0275 27.5000 +2.6103e-13 -2.0796e+09 +1.1977e-12 +1.1400e-12 Timestep 0300 30.0000 +1.8739e-14 +2.5230e+10 +9.8758e-14 +9.3576e-14 Timestep 0325 32.5000 +1.3453e-15 -3.0609e+11 +8.1431e-15 +7.6812e-15 Timestep 0350 35.0000 +9.6578e-17 +3.7135e+12 +6.7143e-16 +6.3051e-16 Timestep 0375 37.5000 +6.9333e-18 -4.5052e+13 +5.5363e-17 +5.1756e-17 Timestep 0400 40.0000 +4.9774e-19 +5.4658e+14 +4.5649e-18 +4.2484e-18 Timestep 0425 42.5000 +3.5733e-20 -6.6311e+15 +3.7640e-19 +3.4873e-19 Timestep 0450 45.0000 +2.5652e-21 +8.0449e+16 +3.1036e-20 +2.8625e-20 Timestep 0475 47.5000 +1.8416e-22 -9.7602e+17 +2.5590e-21 +2.3497e-21 Timestep 0500 50.0000 +1.3221e-23 +1.1841e+19 +2.1100e-22 +1.9287e-22 Timestep 0525 52.5000 +9.4911e-25 -1.4366e+20 +1.7398e-23 +1.5832e-23 Timestep 0550 55.0000 +6.8137e-26 +1.7429e+21 +1.4346e-24 +1.2996e-24 Timestep 0575 57.5000 +4.8915e-27 -2.1144e+22 +1.1829e-25 +1.0668e-25 Timestep 0600 60.0000 +3.5116e-28 +2.5653e+23 +9.7532e-27 +8.7565e-27 Timestep 0625 62.5000 +2.5210e-29 -3.1122e+24 +8.0420e-28 +7.1878e-28 Timestep 0650 65.0000 +1.8098e-30 +3.7757e+25 +6.6310e-29 +5.9001e-29 Timestep 0675 67.5000 +1.2993e-31 -4.5807e+26 +5.4675e-30 +4.8431e-30 Timestep 0700 70.0000 +9.3273e-33 +5.5574e+27 +4.5082e-31 +3.9754e-31 Timestep 0725 72.5000 +6.6961e-34 -6.7423e+28 +3.7172e-32 +3.2632e-32 Timestep 0750 75.0000 +4.8071e-35 +8.1797e+29 +3.0650e-33 +2.6786e-33 Timestep 0775 77.5000 +3.4510e-36 -9.9237e+30 +2.5273e-34 +2.1988e-34 Timestep 0800 80.0000 +2.4775e-37 +1.2040e+32 +2.0838e-35 +1.8049e-35 Timestep 0825 82.5000 +1.7786e-38 -1.4606e+33 +1.7182e-36 +1.4815e-36 Timestep 0850 85.0000 +1.2768e-39 +1.7721e+34 +1.4167e-37 +1.2161e-37 Timestep 0875 87.5000 +9.1663e-41 -2.1499e+35 +1.1682e-38 +9.9824e-39 Timestep 0900 90.0000 +6.5805e-42 +2.6082e+36 +9.6321e-40 +8.1940e-40 Timestep 0925 92.5000 +4.7241e-43 -3.1643e+37 +7.9421e-41 +6.7261e-41 Timestep 0950 95.0000 +3.3914e-44 +3.8390e+38 +6.5486e-42 +5.5211e-42 Timestep 0975 97.5000 +2.4347e-45 -4.6575e+39 +5.3996e-43 +4.5320e-43 Timestep 1000 100.0000 +1.7479e-46 +5.6505e+40 +4.4522e-44 +3.7201e-44 \"\"\" Clearly, the quadratic expansion does not do a good job in this particular case, since the error is a growing instability which makes the solution useless almost immediately. However, the second order Runge-Kutta method is very accurate. This gives a clear warning that numerical solution of equations is partly an art (though the stability or otherwise of a given method can actually be proven formally in many cases).","title":"Our First Code"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-2.html#explicit-versus-implicit-methods","text":"The use of implicit solution methods is crucial when we come to the mantle flow problem (and to elasticity). It is a deceptively simple concept which we use to eliminate oscillatory errors and other problems associated with stiff systems, and in other cases where we want to take large timesteps. Again, Numerical Recipes provides a good example of the way in which implicit methods avoid the shuffling timesteps which result when an otherwise unimportant component of the system blows up. To obtain an implicit solution, we express our updated timestep in terms of values of derivatives etc which are evaluated at that timestep. This may result in dependencies which mean large systems of simultaneous equations need to be solved. But our simple example can also be solved implicitly as follows \\[ \\theta(t+\\Delta t) = \\left. \\frac{d \\theta}{d t} \\right| _ {t+\\Delta t} + \\theta(t) \\] Easy to write down, but \\(\\left. d \\theta/ dt \\right| _ {t+\\Delta t}\\) is not known until \\(\\theta(t+\\Delta t) \\) is known. In this case, however, we simply substitute \\(\\left. d \\theta/ dt \\right| _ {t+\\Delta t}= -k \\theta(t+\\Delta t) \\) from the differential equation and write \\[ \\begin{split} \\theta(t+\\Delta t) & = -k \\theta(t+\\Delta t) \\Delta t + \\theta(t) \\ & = \\frac{\\theta(t) }{ 1 + k\\Delta t} \\end{split} \\] A higher order implicit method can be found by considering how the second order Runge-Kutta method constructs its estimates. In place of the first order approximation above, we try \\[ \\theta(t+\\Delta t) = \\left. \\frac{d \\theta}{d t} \\right| _ {t+\\Delta t/2} + \\theta(t) \\] where we assume \\[ \\left. \\frac{d \\theta}{d t} \\right| _ {t+\\Delta t/2} = \\frac{1}{2} \\left. \\frac{d \\theta}{d t} \\right| _ {t+\\Delta t} + \\frac{1}{2} \\left. \\frac{d \\theta}{d t} \\right| _ {t} \\] Substituting from our original differential equation now gives \\[ \\begin{split} \\theta(t+\\Delta t) &= -k\\frac{\\theta(t+\\Delta t) + \\theta(t)}{2} \\Delta t + \\theta(t) \\ &= \\theta(t) \\frac{1-k\\Delta t / 2}{1+k\\Delta t / 2} \\end{split} \\] \\(t\\) \\(^1\\theta_i\\) \\(^2\\theta_i\\) Exact 0.0 1.0000 1.0000 1.0000 0.1 0.9091 0.9048 0.9048 0.2 0.8264 0.8186 0.8187 0.3 0.7513 0.7406 0.7408 0.4 0.6830 0.6701 0.6703 0.5 0.6209 0.6063 0.6063 Results of the two implicit schemes derived above for the numerical integration of the decay equation The first order scheme is of approximately the same accuracy as the explicit scheme, not surprisingly, but the mid-point implicit method has very impressive results considering that there is very little extra work involved in deriving the formulation. Can this strategy stabilize the quadratic expansion which proved to be so unstable in explicit form ? Evaluating the derivatives at time \\(t=(n+1)\\Delta t\\) gives \\[ \\left. \\frac{d \\theta}{d t} \\right| _ {(n+1)\\Delta t} = 3 \\theta _ {n+1} - 4\\theta _ n + \\theta _ {n-1} \\] which we substitute into the differential equation at \\(t=(n+1)\\Delta t\\) to give (eventually) \\[ \\theta _ {n+1} = \\frac{4\\theta _ n - \\theta _ {n-1}}{3+2k\\Delta t} \\] This method is stable although not as accurate as the second order Runge-Kutta Scheme. It is trivial to modify the Python script to demonstrate this.","title":"Explicit versus Implicit Methods}"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-2.html#references","text":"...","title":"References"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-3.html","text":"\\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\] We now move on to look at a different problem which also brings a few surprises when we attempt to find a straightforward numerical treatment. Specifically, the equations that govern transport of a quantity by a moving fluid (advection). This seems pretty straightforward as we simply have to account for a concentration being moved around by a velocity vector field. There are multiple tricks involved in doing this accurately. Generalizing from the Simplest Example We have learned a number of things --- in particular We have to know something about the governing equations of the system before modeling can proceed (i.e. a conceptual model, and then a mathematical model). Before the equations can be solved in the computer, it is necessary to render the problem finite. The discretization method chosen may not be effective for a particular problem. Numerical modeling can be an art since experience with different differential equations is often needed to avoid pitfalls. The Problem with Advection Advection is a fundamental concept in fluid mechanics, get it makes the lives of fluid dynamicists much more difficult. It can be particularly problematic in numerical modeling. This is worth having in mind when we discuss different numerical methods because in application to solid Earth dynamics, advection will be a major issue with any method we choose. Advection pure and simple --- with no diffusion, fluid motion winds up initially local regions into long, convoluted tendrils As we discussed previously in dealing with approximate analytic solutions, the solution to all our advection woes is to deal with a coordinate system locked to the fluid. Unfortunately, while this approach works well in some situations -- predominantly solid mechanics and engineering applications where total deformation is generally no more than a few percent strain -- in fluids, the local coordinate system becomes quite hard to track. In the figure above, a small, square region of fluid has been tagged and is followed during the deformation induced by a simple convection roll. It is clear that a coordinate system based on initially orthogonal sets of axes rapidly becomes unrecognizably distorted. Advection, in the absence of any diffusion terms, represents a transport of information about the state of an individual parcel of fluid which is different from the state of its neighbours. For example, it might be a dye which tells us whether a parcel of fluid started in on half of the domain or the other. We are dealing with a chaotic system in the sense that parcels of material which start abitrarily close together will wind up exponentially far apart as time progresses. Thus, the dye will become ever more stretched and filamented without ever being mixed (at least in laminar flow). If the dye can diffuse then, the finer scales of the tendrils will in fact be mixed because they are associated with enormous spatial gradients (e.g. compare this with boundary layers). If the dye cannot diffuse then the density of information needed to characterize the system increases without limit. Numerically this is impossible to represent since at some stage, the stored problem has to be kept finite. This can be imagined as an effective diffusion process, although it has an anisotropic and discretization dependent form. The rule of thumb, that arises from this observation is that the real diffusion coefficient must be larger than the numerical one if the method is to give a true representation of the problem. Numerical Example in 1D Setup for a first attempt at a numerical advection scheme on a fixed discretization. After two timesteps, the sharp front has become smoothed despite introducing no genuine diffusion Let us follow the usual strategy and consider the simplest imaginable advection equation: \\[ \\frac{\\partial \\phi}{\\partial t} = -v \\frac{\\partial \\phi}{\\partial x} \\] in which \\(v \\) is a constant velocity. Obviously we need to introduce some notation as a warm-up for solving the problem. We break up the spatial domain into a series of points separated by \\(\\delta x \\) as shown above and, as we did in the earlier examples, break up time into a discrete set separated by \\(\\delta t \\). The values of \\(\\phi\\) at various times and places are denoted by \\[ \\begin{align} \\nonumber _ {i-1} & \\phi _ {n-1} & _ {i} & \\phi _ {n-1} & _ {i+1} & \\phi _ {n-1} \\newline \\nonumber _ {i-1} & \\phi _ {n} & _ {i} & \\phi _ {n} & _ {i+1} & \\phi _ {n} \\nonumber \\newline \\nonumber _ {i-1} & \\phi _ {n+1} & _ {i} & \\phi _ {n+1} & _ {i+1} & \\phi _ {n+1} \\nonumber \\end{align} \\] where the \\(i \\) subscript is the \\(x \\) position and \\(n\\) denotes the timestep: \\[ \\phi(x _ i,n\\Delta t) = { _ {i}\\phi} _ n \\] A simple discretization gives \\[ \\begin{split} _ {i}\\phi _ {n+1} &= \\frac{\\partial \\phi}{\\partial t} \\Delta t + { _ {i}\\phi _ n } \\ & = -v \\frac{ { _ {i+1}\\phi _ n} - { _ {i-1}\\phi _ n}}{2 \\delta x} + { _ {i}\\phi _ n} \\end{split} \\] For simplicity, we set \\(v\\delta t = \\delta x /2\\) and write \\[ { _ {i}\\phi} _ {n+1} = \\frac{1}{4}\\left( { _ {i+1}\\phi} _ n - { _ {i-1}\\phi} _ n \\right) \\] time i-2 i-1 i i+1 i+2 i+3 \\( \\int \\phi dx \\) Centred t=0 1 1 1 0 0 0 3 t=\\(\\delta x/2v\\) 1 1 1.25 0.25 0 0 3.5 t=\\(\\delta x/v \\) 1 0.938 1.438 0.563 0.0625 0 4.0 Upwind t=0 1 1 1 0 0 0 3 t=\\(\\delta x/2v\\) 1 1 1 0.5 0 0 3.5 t=\\(\\delta x/v \\) 1 1 1 0.75 0.25 0 4.0 Table: Hand calculation of low order advection schemes We compute the first few timesteps for a step function in \\(\\phi\\) initially on the location \\(x_i\\) as shown in the diagram. These are written out in the first section of the table above. There are some oddities immediately visible from the table entries. The step has a large overshoot to the left, and its influence gradually propogates in the upstream direction. However, it does preserve the integral value of \\(\\phi\\) on the domain (allowing for the fact that the step is advancing into the domain). These effects can be minimized if we use \"upwind differencing\". This involves replacing the advection term with a non-centred difference scheme instead of the symmetrical term that we used above. \\[ \\begin{split} _ {i}\\phi _ {n+1} &= \\frac{\\partial \\phi}{\\partial t} \\Delta t + { _ {i}\\phi _ n} \\ & = -v \\frac{ { _ {i}\\phi _ n} - { _ {i-1}\\phi_n}}{\\delta x} + { _ {i}\\phi _ n} \\end{split} \\] Where we now take a difference only in the upstream direction. The results of this advection operator are clearly superior to the centred difference version. Now the step has no influence at all in the upstream direction, and the value does not overshoot the maximum. Again, the total quantity of \\(\\phi\\) is conserved. Why does this apparently ad hoc modification make such an improvement to the solution ? We need to remember that the fluid is moving. In the time it takes to make the update at a particular spatial location, the material at that location is swept downstream. Consider where the effective location of the derivative is computed at the beginning of the timestep - by the end of the timestep the fluid has carried this point to the place where the update will occur. This has some similarity to the implicit methods used earlier to produce stable results. Node/Particle Advection Contrary to the difficulty in advecting a continuum field, discrete particle paths can be integrated very easily. For example a Runge-Kutta integration scheme can be applied to advance the old positions to the new based on the known velocity field. It is only when the information needs to be recovered back to some regular grid points that the interpolation degradation of information becomes important. Courant condition For stability, the maximum value of \\(\\delta t\\) should not exceed the time taken for material to travel a distance \\(\\delta x\\). This makes sense as the derivatives come from local information (between a point and its immediate neighbour) and information cannot propogate faster than \\(\\delta x / \\delta t \\). If the physical velocity exceeds the maximum information velocity, then the procedure must fail. This is known as the Courant (or Courant-Friedrichs-Lewy) condition. In multidimensional applications it takes the form \\[ \\delta t \\le \\frac{\\delta x}{\\sqrt{N} |v|} \\] where \\(N \\) is the number of dimensions, and a uniform spacing in all directions, \\( \\delta x \\) is presumed. The exact details of such maximum timestep restrictions for explicit methods vary from problem to problem. It is, however, important to be aware that such restrictions exist so as to be able to search them out before trouble strikes. One of the ugliest problems from advection appears when viscoelasticity is introduced. In this case we need to track a tensor quantity (stress-rate) without diffusion or other distorting effects. Obviously this is not easy, especially in a situation where very large deformations are being tracked elsewhere in the system - e.g. the lithosphere floating about on the mantle as it is being stressed and storing elastic stress. References ...","title":"NumericalMethodsPrimer 3"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-3.html#generalizing-from-the-simplest-example","text":"We have learned a number of things --- in particular We have to know something about the governing equations of the system before modeling can proceed (i.e. a conceptual model, and then a mathematical model). Before the equations can be solved in the computer, it is necessary to render the problem finite. The discretization method chosen may not be effective for a particular problem. Numerical modeling can be an art since experience with different differential equations is often needed to avoid pitfalls.","title":"Generalizing from the Simplest Example"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-3.html#the-problem-with-advection","text":"Advection is a fundamental concept in fluid mechanics, get it makes the lives of fluid dynamicists much more difficult. It can be particularly problematic in numerical modeling. This is worth having in mind when we discuss different numerical methods because in application to solid Earth dynamics, advection will be a major issue with any method we choose. Advection pure and simple --- with no diffusion, fluid motion winds up initially local regions into long, convoluted tendrils As we discussed previously in dealing with approximate analytic solutions, the solution to all our advection woes is to deal with a coordinate system locked to the fluid. Unfortunately, while this approach works well in some situations -- predominantly solid mechanics and engineering applications where total deformation is generally no more than a few percent strain -- in fluids, the local coordinate system becomes quite hard to track. In the figure above, a small, square region of fluid has been tagged and is followed during the deformation induced by a simple convection roll. It is clear that a coordinate system based on initially orthogonal sets of axes rapidly becomes unrecognizably distorted. Advection, in the absence of any diffusion terms, represents a transport of information about the state of an individual parcel of fluid which is different from the state of its neighbours. For example, it might be a dye which tells us whether a parcel of fluid started in on half of the domain or the other. We are dealing with a chaotic system in the sense that parcels of material which start abitrarily close together will wind up exponentially far apart as time progresses. Thus, the dye will become ever more stretched and filamented without ever being mixed (at least in laminar flow). If the dye can diffuse then, the finer scales of the tendrils will in fact be mixed because they are associated with enormous spatial gradients (e.g. compare this with boundary layers). If the dye cannot diffuse then the density of information needed to characterize the system increases without limit. Numerically this is impossible to represent since at some stage, the stored problem has to be kept finite. This can be imagined as an effective diffusion process, although it has an anisotropic and discretization dependent form. The rule of thumb, that arises from this observation is that the real diffusion coefficient must be larger than the numerical one if the method is to give a true representation of the problem.","title":"The Problem with Advection"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-3.html#numerical-example-in-1d","text":"Setup for a first attempt at a numerical advection scheme on a fixed discretization. After two timesteps, the sharp front has become smoothed despite introducing no genuine diffusion Let us follow the usual strategy and consider the simplest imaginable advection equation: \\[ \\frac{\\partial \\phi}{\\partial t} = -v \\frac{\\partial \\phi}{\\partial x} \\] in which \\(v \\) is a constant velocity. Obviously we need to introduce some notation as a warm-up for solving the problem. We break up the spatial domain into a series of points separated by \\(\\delta x \\) as shown above and, as we did in the earlier examples, break up time into a discrete set separated by \\(\\delta t \\). The values of \\(\\phi\\) at various times and places are denoted by \\[ \\begin{align} \\nonumber _ {i-1} & \\phi _ {n-1} & _ {i} & \\phi _ {n-1} & _ {i+1} & \\phi _ {n-1} \\newline \\nonumber _ {i-1} & \\phi _ {n} & _ {i} & \\phi _ {n} & _ {i+1} & \\phi _ {n} \\nonumber \\newline \\nonumber _ {i-1} & \\phi _ {n+1} & _ {i} & \\phi _ {n+1} & _ {i+1} & \\phi _ {n+1} \\nonumber \\end{align} \\] where the \\(i \\) subscript is the \\(x \\) position and \\(n\\) denotes the timestep: \\[ \\phi(x _ i,n\\Delta t) = { _ {i}\\phi} _ n \\] A simple discretization gives \\[ \\begin{split} _ {i}\\phi _ {n+1} &= \\frac{\\partial \\phi}{\\partial t} \\Delta t + { _ {i}\\phi _ n } \\ & = -v \\frac{ { _ {i+1}\\phi _ n} - { _ {i-1}\\phi _ n}}{2 \\delta x} + { _ {i}\\phi _ n} \\end{split} \\] For simplicity, we set \\(v\\delta t = \\delta x /2\\) and write \\[ { _ {i}\\phi} _ {n+1} = \\frac{1}{4}\\left( { _ {i+1}\\phi} _ n - { _ {i-1}\\phi} _ n \\right) \\] time i-2 i-1 i i+1 i+2 i+3 \\( \\int \\phi dx \\) Centred t=0 1 1 1 0 0 0 3 t=\\(\\delta x/2v\\) 1 1 1.25 0.25 0 0 3.5 t=\\(\\delta x/v \\) 1 0.938 1.438 0.563 0.0625 0 4.0 Upwind t=0 1 1 1 0 0 0 3 t=\\(\\delta x/2v\\) 1 1 1 0.5 0 0 3.5 t=\\(\\delta x/v \\) 1 1 1 0.75 0.25 0 4.0 Table: Hand calculation of low order advection schemes We compute the first few timesteps for a step function in \\(\\phi\\) initially on the location \\(x_i\\) as shown in the diagram. These are written out in the first section of the table above. There are some oddities immediately visible from the table entries. The step has a large overshoot to the left, and its influence gradually propogates in the upstream direction. However, it does preserve the integral value of \\(\\phi\\) on the domain (allowing for the fact that the step is advancing into the domain). These effects can be minimized if we use \"upwind differencing\". This involves replacing the advection term with a non-centred difference scheme instead of the symmetrical term that we used above. \\[ \\begin{split} _ {i}\\phi _ {n+1} &= \\frac{\\partial \\phi}{\\partial t} \\Delta t + { _ {i}\\phi _ n} \\ & = -v \\frac{ { _ {i}\\phi _ n} - { _ {i-1}\\phi_n}}{\\delta x} + { _ {i}\\phi _ n} \\end{split} \\] Where we now take a difference only in the upstream direction. The results of this advection operator are clearly superior to the centred difference version. Now the step has no influence at all in the upstream direction, and the value does not overshoot the maximum. Again, the total quantity of \\(\\phi\\) is conserved. Why does this apparently ad hoc modification make such an improvement to the solution ? We need to remember that the fluid is moving. In the time it takes to make the update at a particular spatial location, the material at that location is swept downstream. Consider where the effective location of the derivative is computed at the beginning of the timestep - by the end of the timestep the fluid has carried this point to the place where the update will occur. This has some similarity to the implicit methods used earlier to produce stable results.","title":"Numerical Example in 1D"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-3.html#nodeparticle-advection","text":"Contrary to the difficulty in advecting a continuum field, discrete particle paths can be integrated very easily. For example a Runge-Kutta integration scheme can be applied to advance the old positions to the new based on the known velocity field. It is only when the information needs to be recovered back to some regular grid points that the interpolation degradation of information becomes important.","title":"Node/Particle Advection"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-3.html#courant-condition","text":"For stability, the maximum value of \\(\\delta t\\) should not exceed the time taken for material to travel a distance \\(\\delta x\\). This makes sense as the derivatives come from local information (between a point and its immediate neighbour) and information cannot propogate faster than \\(\\delta x / \\delta t \\). If the physical velocity exceeds the maximum information velocity, then the procedure must fail. This is known as the Courant (or Courant-Friedrichs-Lewy) condition. In multidimensional applications it takes the form \\[ \\delta t \\le \\frac{\\delta x}{\\sqrt{N} |v|} \\] where \\(N \\) is the number of dimensions, and a uniform spacing in all directions, \\( \\delta x \\) is presumed. The exact details of such maximum timestep restrictions for explicit methods vary from problem to problem. It is, however, important to be aware that such restrictions exist so as to be able to search them out before trouble strikes. One of the ugliest problems from advection appears when viscoelasticity is introduced. In this case we need to track a tensor quantity (stress-rate) without diffusion or other distorting effects. Obviously this is not easy, especially in a situation where very large deformations are being tracked elsewhere in the system - e.g. the lithosphere floating about on the mantle as it is being stressed and storing elastic stress.","title":"Courant condition"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-3.html#references","text":"...","title":"References"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html","text":"\\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\] Prelude to Finite Elements: The Variational Calculus This is something of an aside but it is absolutely necessary to understand the variational method in order to follow how Finite Element Methods work Example: How Short is a Straight Line ? It's intuitively obvious that the shortest distance between two points on a plane is just a straight line. To demonstrate this mathematically is more tricky. In words, the procedure goes like this: of all possible curves between the two points, find one (if it exists) which always becomes longer if it is altered in any way. (Thinking physically, if the points were linked by a rubber band, then to disturb it from the shortest curve would require additional energy, no matter what that disturbance looked like). {. width=\"75%\"} What curve gives the shortest distance between two points in a plane ? And how can we prove it ! We decide, arbitrarily, that we will make x x the independent variable and find a curve y(x) y(x) which satisfies the minimum distance requirement. The distance along a curve is a path integral : \\[ S = \\int_{x_1}^{x_2} \\frac{ds}{dx} dx \\] where \\[ \\frac{ds}{dx} = \\sqrt{1 + \\left( \\frac{dy}{dx} \\right)^2} \\] We seek to find y(x) y(x) which minimizes S S . First consider the function \\begin{equation} \\begin{split} Y(x,\\alpha) &= y(x) + \\alpha \\eta(x) \\\\ \\frac{\\partial Y}{\\partial x} \\equiv Y' &= y'(x) + \\alpha \\eta'(x) \\end{split} \\nonumber \\end{equation} \\begin{equation} \\begin{split} Y(x,\\alpha) &= y(x) + \\alpha \\eta(x) \\\\ \\frac{\\partial Y}{\\partial x} \\equiv Y' &= y'(x) + \\alpha \\eta'(x) \\end{split} \\nonumber \\end{equation} where \\eta(x) \\eta(x) is an arbitrary function which is differentiable and vanishes at x=x_1,x_2 x=x_1,x_2 . This is the variation which we apply to some curve y(x) y(x) to see if it gets shorter or longer. Note the notation for the derivative which will be useful as we procede. The optimal path will minimize S(\\alpha) S(\\alpha) , when \\alpha=0 \\alpha=0 , i.e. \\begin{equation} \\left. \\frac{\\partial S(\\alpha)}{\\partial \\alpha} \\right| _ {\\alpha = 0} = 0 \\nonumber \\end{equation} \\begin{equation} \\left. \\frac{\\partial S(\\alpha)}{\\partial \\alpha} \\right| _ {\\alpha = 0} = 0 \\nonumber \\end{equation} In our current notation \\begin{equation} S(\\alpha) = \\int_{x_1}^{x_2} \\sqrt{1 + (Y')^2} \\nonumber \\end{equation} and \\begin{equation} \\frac{\\partial S(\\alpha)}{\\partial \\alpha} = \\int_{x_1}^{x_2} \\frac{1}{2} \\frac{1}{\\sqrt{1 + (Y')^2}} . 2 Y' \\frac{\\partial Y'}{\\partial \\alpha} dx \\nonumber \\end{equation} from the definition of Y'(x,\\alpha) Y'(x,\\alpha) \\begin{equation} \\frac{\\partial Y'}{\\partial \\alpha} = \\eta'(x) \\nonumber \\end{equation} \\begin{equation} \\frac{\\partial Y'}{\\partial \\alpha} = \\eta'(x) \\nonumber \\end{equation} So we now must solve \\begin{equation} \\left. \\frac{\\partial S(\\alpha)}{\\partial \\alpha} \\right| _ {\\alpha = 0} = \\int_{x_1}^{x_2} \\frac{y'(x)\\eta'(x)}{\\sqrt{1 + (y') ^ 2}} dx = 0 \\nonumber \\end{equation} We integrate by parts to give \\begin{equation} \\left. \\frac{\\partial S(\\alpha)}{\\partial \\alpha} \\right| _ {\\alpha = 0} = \\left[ \\frac{y'(x)\\eta(x)}{\\sqrt{1 + (y')^2}} \\right] _ {x_1}^{x_2} - \\int_{x_1}^{x_2} \\eta{x} \\frac{d}{dx} \\frac{y'(x)}{\\sqrt{1 + (y')^2}} dx = 0 \\nonumber \\end{equation} \\begin{equation} \\left. \\frac{\\partial S(\\alpha)}{\\partial \\alpha} \\right| _ {\\alpha = 0} = \\left[ \\frac{y'(x)\\eta(x)}{\\sqrt{1 + (y')^2}} \\right] _ {x_1}^{x_2} - \\int_{x_1}^{x_2} \\eta{x} \\frac{d}{dx} \\frac{y'(x)}{\\sqrt{1 + (y')^2}} dx = 0 \\nonumber \\end{equation} The first term on the RHS vanishes because \\eta \\eta vanishes at the boundaries. The second term is valid for arbitrary \\eta(x) \\eta(x) which implies \\begin{equation} \\frac{d}{dx} \\frac{y'(x)}{\\sqrt{1 + (y')^2}} dx = 0 \\nonumber \\end{equation} \\begin{equation} \\frac{d}{dx} \\frac{y'(x)}{\\sqrt{1 + (y')^2}} dx = 0 \\nonumber \\end{equation} which in turn implies y'= y'= constant, i.e. the equation of a straight line. The important things to note here are that an integral method can be used to solve a simple geometrical problem and that the method itself includes the boundary conditions as a natural consequence of the way it is set up. The variational method can be generalized to solve more important problems. In particular, instead of solving each problem as we have for the straight line / distance question above, we solve a generic problem whose solutions we can apply immediately. Generalisation The general form works like this. To find the function y(x) y(x) which produces a stationary value of the functional \\begin{equation} J=\\int_{x_1}^{x_2} F(x,y,y') dx \\nonumber \\end{equation} \\begin{equation} J=\\int_{x_1}^{x_2} F(x,y,y') dx \\nonumber \\end{equation} we work through the same procedure as above, and use the same arguments concerning the arbitrary nature of the variation to obtain \\begin{equation} \\frac{d}{dx}\\frac{\\partial F}{\\partial y'} - \\frac{\\partial F}{\\partial y} = 0 \\nonumber \\end{equation} This is known as the Euler equation. Hamilton's principle states that mechanical systems evolve such that the integral \\begin{equation} J=\\int_{t_1}^{t_2} L dt \\nonumber \\end{equation} \\begin{equation} J=\\int_{t_1}^{t_2} L dt \\nonumber \\end{equation} is stationary. Here L L is the Lagrangian of the system which is identified with a combination of the work done on the system and the kinetic energy of the system, e.g. potential energy - kinetic energy. Application of the Euler equation to each direction independently recovers Newton's law ( F=ma F=ma ). In complex geometries and with difficult boundary conditions, the variational form may be easier to solve than the differential or \"strong\" form. This shows us that there are equivalent integral representations for the standard mechanical equations we are accustomed to using - variational or weak forms versus differential or strong forms . Although this may seem complicated and of rather theoretical interest, in fact it runs throughout finite element methods, and the concept must be familiar in order to follow how FEM works. Advantages of using variational forms of the equations include: The simplification of the construction of the governing equations in the sense that scalar quantities --- energies, potentials --- are considered in place of forces, displacements etc. There is also the possibility that such formulations can be derived more-or-less automatically for previously unknown systems. Governing equations may be more directly accessible since \"unimportant\" variables such as internal forces doing no net work do not appear in the variational form. When dealing with approximate solutions, the variational form often allows a broader range of trial functions than for the standard differential form. This happens because some boundary conditions are implicit in the formulation and hence are not imposed on the trial functions themselves. Example of Variational Forms for FEM See Klaus-J\u00fcrgen Bathe's book for a more complete outline of this approach to FEM. Although this is not the best text to explain how to implement a finite element code, he does a great job of explaining the link between variational methods and weak forms of various equations of progressively increasing complexity. A Slab of Material Subjected to an sudden onset of heating at Q Q on one side at time t=0 t=0 The functional governing the temperature in the block of material is \\begin{equation} \\Pi = \\int_0^L \\frac{1}{2} k \\left(\\frac{\\partial \\theta}{\\partial x} \\right)^2 dx - \\int_0^L \\theta q^B dx - \\theta(0,t) Q \\nonumber \\end{equation} where q^B q^B is an internal heat generation rate. The fixed boundary condition is \\theta(L,t) =\\theta_i \\theta(L,t) =\\theta_i . This is our generalized problem with \\begin{equation} F = \\frac{1}{2}k \\left(\\frac{\\partial \\theta}{\\partial x} \\right)^2 - \\theta q^B = \\frac{1}{2}k {\\theta'}^2 - \\theta q^B \\nonumber \\end{equation} which produces a stationary functional if \\begin{equation} \\frac{d}{dx}\\frac{\\partial F}{\\partial \\theta'} - \\frac{\\partial F}{\\partial \\theta} = 0 \\nonumber \\end{equation} \\begin{equation} \\frac{d}{dx}\\frac{\\partial F}{\\partial \\theta'} - \\frac{\\partial F}{\\partial \\theta} = 0 \\nonumber \\end{equation} or, in other words \\begin{equation} k\\frac{d^2 \\theta}{d x^2} = q^B \\nonumber \\end{equation} Which we recognize to be the governing differential equation. The variational statement also contains the natural boundary condition \\begin{equation} k \\left. \\frac{\\partial \\theta}{\\partial x} \\right| _ {x=0} + Q = 0 \\nonumber \\end{equation} \\begin{equation} k \\left. \\frac{\\partial \\theta}{\\partial x} \\right| _ {x=0} + Q = 0 \\nonumber \\end{equation} This is a clear demonstration that the standard form of the equations plus certain boundary conditions can be fully wrapped up in integral form and are exactly equivalent to the standard form. The major difficulty is in how we produce the correct functional in the first place, especially if we want to avoid first deriving the differential form of the equations and back-calculating as we done above. Extension to Approximate Methods The problem above is simple enough that the integral or differential forms of the equations can be solved directly. In general, however, we anticipate dealing with problems where no closed form of solution exists. Under these circumstances approximate solutions are desirable. In particular, there is a class of approximation methods which use families of trial functions to obtain a best fit approximation to the solution. These naturally develop into finite element algorithms as we shall soon see. Formulation of a General Problem We consider a steady-state problem characterized by the following strong form \\begin{equation} {\\cal{L}} (\\phi) = f \\nonumber \\end{equation} where {\\cal{L}} {\\cal{L}} is a linear differential operator acting on the (unknown) state variable \\phi \\phi in responce to a forcing function f f . Boundary conditions are \\begin{equation} {\\cal{B}}_i [\\phi] = \\left. q_i \\right| _ {\\textrm{ \\small at boundary } S_i} \\;\\;\\; i=1,2,\\ldots \\nonumber \\end{equation} The operator should be symmetric \\begin{equation} \\int_\\Omega v {\\cal{L}}(u) d\\Omega = \\int_\\Omega u {\\cal{L}}(v) d\\Omega \\nonumber \\end{equation} and positive definite \\begin{equation} \\int_\\Omega u {\\cal{L}}(u) d\\Omega > 0 \\nonumber \\end{equation} \\Omega \\Omega is the domain of the operator and u u and v v are any functions which satisfy the boundary conditions. {. width=\"75%\" id=\"rod-end-load\"} A rod subject to end load --- Young's modulus, E E , density, \\rho \\rho , cross sectional area, A A Consider the 1D example of a bar subject to a steady end load. The response is the solution to \\begin{equation} -EA\\frac{\\partial^2 u}{\\partial x^2} = 0 \\nonumber \\end{equation} \\begin{equation} -EA\\frac{\\partial^2 u}{\\partial x^2} = 0 \\nonumber \\end{equation} subject to the boundary conditions \\begin{equation} \\begin{split} \\left. u \\right| _ {x=0} & = 0 \\\\ \\left. EA\\frac{\\partial u}{\\partial x} \\right| _ {x=L} = R \\end{split} \\end{equation} \\begin{equation} \\begin{split} \\left. u \\right| _ {x=0} & = 0 \\\\ \\left. EA\\frac{\\partial u}{\\partial x} \\right| _ {x=L} = R \\end{split} \\end{equation} We therefore identify \\begin{eqnarray} {\\cal{L}}= -EA\\frac{\\partial^2 u}{\\partial x^2} & \\phi = u & f = 0 \\ \\nonumber & B_1 = 1 ; q_1 = 0 & \\ \\nonumber & B_2 = EA\\frac{\\partial }{\\partial x}; q_2 = R & \\nonumber \\end{eqnarray} To check symmetry and positive definiteness of the operator we consider R=0 R=0 since the operator properties are independent of the actual load. Integrating by parts gives \\begin{equation} \\begin{split} \\int_0^L-EA\\frac{\\partial^2 u}{\\partial x^2} v dx & = - \\left. EA\\frac{\\partial u}{\\partial x} v \\right| _ 0^L + \\int_0^L EA \\frac{\\partial u}{\\partial x} \\frac{\\partial v}{\\partial x} dx \\\\ &= - \\left. EA\\frac{\\partial u}{\\partial x} v \\right| _ 0^L - \\left. + EA u \\frac{\\partial v}{\\partial x} \\right| _ 0^L - \\int_0^L EA\\frac{\\partial^2 v}{\\partial x^2} u dx \\end{split} \\end{equation} \\begin{equation} \\begin{split} \\int_0^L-EA\\frac{\\partial^2 u}{\\partial x^2} v dx & = - \\left. EA\\frac{\\partial u}{\\partial x} v \\right| _ 0^L + \\int_0^L EA \\frac{\\partial u}{\\partial x} \\frac{\\partial v}{\\partial x} dx \\\\ &= - \\left. EA\\frac{\\partial u}{\\partial x} v \\right| _ 0^L - \\left. + EA u \\frac{\\partial v}{\\partial x} \\right| _ 0^L - \\int_0^L EA\\frac{\\partial^2 v}{\\partial x^2} u dx \\end{split} \\end{equation} Application of boundary conditions demonstrates that the operator is symmetric by our definition. Positive definiteness is also assured since \\begin{equation} \\int_0 L-EA\\frac{\\partial 2 u}{\\partial x^2} u dx = - \\left. EA\\frac{\\partial u}{\\partial x} u \\right| _ 0^L + \\int_0^L EA \\frac{\\partial u}{\\partial x} \\frac{\\partial u}{\\partial x} dx = 0 + \\int_0^L EA \\left(\\frac{\\partial u}{\\partial x}\\right)^2 dx \\end{equation} Suppose we now search for approximate solutions of the form \\begin{equation} \\bar{\\phi} = \\sum_{i=1}^{n} a_i \\Phi_i \\nonumber \\end{equation} where \\Phi_i \\Phi_i are linearly independent trial functions and the a_i a_i are the unknown weights for each of the functions. In weighted residuals methods, the expansion is used directly on the strong form of the equations. \\Phi_i \\Phi_i are chosen so as to satisfy all boundary conditions and then we seek to minimize a residual \\begin{equation} R = f - {\\cal{L}}( \\sum_{i=1}^{n} a_i \\Phi_i ) \\end{equation} \\begin{equation} R = f - {\\cal{L}}( \\sum_{i=1}^{n} a_i \\Phi_i ) \\end{equation} Least Squares Method Minimize the square of the residual with respect to a_i a_i \\begin{equation} \\frac{\\partial}{\\partial a_i} \\int_\\Omega R^2 d\\Omega = 0 \\;\\;\\; i = 1,2,\\ldots \\nonumber \\end{equation} This method produces a symmetric coefficient matrix regardless of the properties of the operator. Galerkin Method To determine a_i a_i , solve the n equation system \\begin{equation} \\int_\\Omega N_i R d\\Omega = 0 \\;\\;\\; i = 1,2,\\ldots \\end{equation} over the solution domain \\Omega \\Omega . This method produces a symmetric, positive definite coefficient matrix if the operator is symmetric and positive definite. Ritz Method The Ritz method does not operate on the residual of the strong problem, but minimizes the weak form of the problem with respect to each of the unknown parameters a_i a_i in the usual variational manner. The trial functions no longer need satisfy the natural boundary conditions of the problem as these are wrapped up in the variational form. (Again, see Bathe for discussion and examples). The Galerkin method can be extended to include a term which minimizes the violation of natural boundary conditions, and thus permits the use of a wider range of trial functions \\begin{equation} \\int_\\Omega N_i R d\\Omega + \\int_\\Gamma N_i R_B d\\Gamma = 0 \\;\\;\\; i = 1,2,\\ldots \\label{eq:galerk1} \\end{equation} However, it does not now necessarily produce a symmetric matrix even for a symmetric operator. However, if the equation (\\ref{eq:galerk1} ) is integrated once by parts, it yields a symmetric form and also reduces the order of derivatives inside the integral. This means that the trial functions need be of lower order, and it makes the Galerkin formulation equivalent to the Ritz formulation. Weighted residual formulations have one advantage: they can be used whether or not there exists a functional corresponding to the particular problem. As a result is used this method is used extensively in constructing finite element methods. Finite Element Theory The Genesis of a Matrix Method One of the dominant features of the Finite Element literature is that it is filled with matrix algebra. The fact that differential equations can be rendered into matrices seems at first to be mysterious. However, it results quite naturally from the discretization of the problem, and the parameterization of the discrete equations through a limited set of unknown parameters. Thus, before getting deeply involved in the arcane lore of Finite Elements, We give one example of a genuinely discrete system and show how it generates a matrix problem quite naturally. {. width=\"75%\" id=\"mine-carts\"} (a) A system of three carts interconnected by springs of different stiffnesses and in turn connected to an end wall. (b) The element equilibrium diagram for the spring k_2 k_2 Consider the system illustrated in the figure (a) above --- three freely rolling carts attached by springs. There are three loads applied R_1,R_2,R_3 R_1,R_2,R_3 , one to each cart, and we wish to determine the equilibrium displacements U_1,U_2,U_3 U_1,U_2,U_3 . We can illustrate graphically the equilibrium condition for one of the springs based on its internal degrees of freedom and effective external load. This equilibrium is: \\begin{equation} \\begin{split} k_2 (U_1 - U_2) &= {F_1}^{(2)} \\\\ k_2 (U_2 - U_1) &= {F_2}^{(2)} \\end{split} \\end{equation} \\begin{equation} \\begin{split} k_2 (U_1 - U_2) &= {F_1}^{(2)} \\\\ k_2 (U_2 - U_1) &= {F_2}^{(2)} \\end{split} \\end{equation} or \\begin{equation} k_2 \\left[ \\begin{array}{cc} 1 & -1 \\\\ -1 & 1 \\end{array} \\right] \\left[ \\begin{array}{c} U_1 \\\\ U_2 \\end{array} \\right] = \\left[ \\begin{array}{c} {F_1}^{(2)} \\\\ {F_2}^{(2)}\\end{array} \\right] \\end{equation} \\begin{equation} k_2 \\left[ \\begin{array}{cc} 1 & -1 \\\\ -1 & 1 \\end{array} \\right] \\left[ \\begin{array}{c} U_1 \\\\ U_2 \\end{array} \\right] = \\left[ \\begin{array}{c} {F_1}^{(2)} \\\\ {F_2}^{(2)}\\end{array} \\right] \\end{equation} This is essentially the same for all five spring elements except for k_1 k_1 which is anchored at one end and so satisfies \\begin{equation} k_1 U_1 = {F_1}^{(1)} \\nonumber \\end{equation} \\begin{equation} k_1 U_1 = {F_1}^{(1)} \\nonumber \\end{equation} The equilibrium relations for the system as a whole are \\[ \\begin{equation} \\begin{split} & {F_1}^{(1)} + {F_1}^{(2)} + {F_1}^{(3)} + {F_1}^{(4)} = R_1 \\ & {F_2}^{(2)} + {F_2}^{(3)} + {F_2}^{(5)} = R_2 \\ & {F_3}^{(4)} + {F_3}^{(5)} = R_3 \\end{split} \\label{eq:globeq} \\end{equation} \\] If we now write all five equilibrium relations in terms of all available degrees of freedom we obtain a form like this \\begin{equation} k_2 \\left[ \\begin{array}{ccc} k_2 & -k_2 & 0 \\\\ -k_2 & k_2 & 0 \\\\ 0 & 0 & 0 \\end{array} \\right] \\left[ \\begin{array}{c} U_1 \\\\ U_2 \\\\ U_3 \\end{array} \\right] = \\left[ \\begin{array}{c} {F_1}^{(2)} \\\\ {F_2}^{(2)} \\\\ 0\\end{array} \\right] \\nonumber \\end{equation} \\begin{equation} k_2 \\left[ \\begin{array}{ccc} k_2 & -k_2 & 0 \\\\ -k_2 & k_2 & 0 \\\\ 0 & 0 & 0 \\end{array} \\right] \\left[ \\begin{array}{c} U_1 \\\\ U_2 \\\\ U_3 \\end{array} \\right] = \\left[ \\begin{array}{c} {F_1}^{(2)} \\\\ {F_2}^{(2)} \\\\ 0\\end{array} \\right] \\nonumber \\end{equation} which can also be written \\begin{equation} \\mathbf{K}^{(2)}\\mathbf{U} = \\mathbf{F}^{(2)} \\nonumber \\end{equation} in each case. Thus the global equilibrium requirement of \\ref{eq:globeq} become \\begin{equation} \\mathbf{K}\\mathbf{U} = \\mathbf{R} \\nonumber \\end{equation} where \\begin{equation} \\mathbf{K} = \\left[ \\begin{array}{ccc} (k_1 + k_2 +k_3 + k_4) & -(k_2+k_3) & -k_4 \\ -(k_2+k_3) & (k_2 + k_3+k_4) & -k_5 \\ -k_4 & -k_5 & (k_4 + k_5) \\end{array} \\right] \\nonumber \\end{equation} Note, by the way, that $ \\mathbf{K}$ is symmetric. An important observation is that \\begin{equation} \\mathbf{K} = \\sum_{i=1}^{5} \\mathbf{K}^{(i)} \\nonumber \\end{equation} The individual element stiffnesses can be summed to form a global stiffness matrix. A symmetric, positive definite matrix problem can be solved in numerous different ways, many of which are easy to look up in textbooks ! A very simple problem like this has captured much of what we need to do in arbitrarily complex finite element computations. The construction of local element equilbrium problems based on the interaction of each available degree of freedom with every other is followed by the assembly into a global problem by summing the contributions of the individual elements. Note that the formulation of the local equilibrium conditions are done in a symmetric manner (if we change this degree of freedom how does the balance change, and then what if we change this degree of freedom ?) rather than trying to simplify the system. The degrees of freedom are related by elastic spring constants here. In our more abstract formulations we will replace spring constants by coefficients obtained from the variational method but the form is {\\em exactly the same}.","title":"NumericalMethodsPrimer 4"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#prelude-to-finite-elements-the-variational-calculus","text":"This is something of an aside but it is absolutely necessary to understand the variational method in order to follow how Finite Element Methods work","title":"Prelude to Finite Elements: The Variational Calculus"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#example-how-short-is-a-straight-line","text":"It's intuitively obvious that the shortest distance between two points on a plane is just a straight line. To demonstrate this mathematically is more tricky. In words, the procedure goes like this: of all possible curves between the two points, find one (if it exists) which always becomes longer if it is altered in any way. (Thinking physically, if the points were linked by a rubber band, then to disturb it from the shortest curve would require additional energy, no matter what that disturbance looked like). {. width=\"75%\"} What curve gives the shortest distance between two points in a plane ? And how can we prove it ! We decide, arbitrarily, that we will make x x the independent variable and find a curve y(x) y(x) which satisfies the minimum distance requirement. The distance along a curve is a path integral : \\[ S = \\int_{x_1}^{x_2} \\frac{ds}{dx} dx \\] where \\[ \\frac{ds}{dx} = \\sqrt{1 + \\left( \\frac{dy}{dx} \\right)^2} \\] We seek to find y(x) y(x) which minimizes S S . First consider the function \\begin{equation} \\begin{split} Y(x,\\alpha) &= y(x) + \\alpha \\eta(x) \\\\ \\frac{\\partial Y}{\\partial x} \\equiv Y' &= y'(x) + \\alpha \\eta'(x) \\end{split} \\nonumber \\end{equation} \\begin{equation} \\begin{split} Y(x,\\alpha) &= y(x) + \\alpha \\eta(x) \\\\ \\frac{\\partial Y}{\\partial x} \\equiv Y' &= y'(x) + \\alpha \\eta'(x) \\end{split} \\nonumber \\end{equation} where \\eta(x) \\eta(x) is an arbitrary function which is differentiable and vanishes at x=x_1,x_2 x=x_1,x_2 . This is the variation which we apply to some curve y(x) y(x) to see if it gets shorter or longer. Note the notation for the derivative which will be useful as we procede. The optimal path will minimize S(\\alpha) S(\\alpha) , when \\alpha=0 \\alpha=0 , i.e. \\begin{equation} \\left. \\frac{\\partial S(\\alpha)}{\\partial \\alpha} \\right| _ {\\alpha = 0} = 0 \\nonumber \\end{equation} \\begin{equation} \\left. \\frac{\\partial S(\\alpha)}{\\partial \\alpha} \\right| _ {\\alpha = 0} = 0 \\nonumber \\end{equation} In our current notation \\begin{equation} S(\\alpha) = \\int_{x_1}^{x_2} \\sqrt{1 + (Y')^2} \\nonumber \\end{equation} and \\begin{equation} \\frac{\\partial S(\\alpha)}{\\partial \\alpha} = \\int_{x_1}^{x_2} \\frac{1}{2} \\frac{1}{\\sqrt{1 + (Y')^2}} . 2 Y' \\frac{\\partial Y'}{\\partial \\alpha} dx \\nonumber \\end{equation} from the definition of Y'(x,\\alpha) Y'(x,\\alpha) \\begin{equation} \\frac{\\partial Y'}{\\partial \\alpha} = \\eta'(x) \\nonumber \\end{equation} \\begin{equation} \\frac{\\partial Y'}{\\partial \\alpha} = \\eta'(x) \\nonumber \\end{equation} So we now must solve \\begin{equation} \\left. \\frac{\\partial S(\\alpha)}{\\partial \\alpha} \\right| _ {\\alpha = 0} = \\int_{x_1}^{x_2} \\frac{y'(x)\\eta'(x)}{\\sqrt{1 + (y') ^ 2}} dx = 0 \\nonumber \\end{equation} We integrate by parts to give \\begin{equation} \\left. \\frac{\\partial S(\\alpha)}{\\partial \\alpha} \\right| _ {\\alpha = 0} = \\left[ \\frac{y'(x)\\eta(x)}{\\sqrt{1 + (y')^2}} \\right] _ {x_1}^{x_2} - \\int_{x_1}^{x_2} \\eta{x} \\frac{d}{dx} \\frac{y'(x)}{\\sqrt{1 + (y')^2}} dx = 0 \\nonumber \\end{equation} \\begin{equation} \\left. \\frac{\\partial S(\\alpha)}{\\partial \\alpha} \\right| _ {\\alpha = 0} = \\left[ \\frac{y'(x)\\eta(x)}{\\sqrt{1 + (y')^2}} \\right] _ {x_1}^{x_2} - \\int_{x_1}^{x_2} \\eta{x} \\frac{d}{dx} \\frac{y'(x)}{\\sqrt{1 + (y')^2}} dx = 0 \\nonumber \\end{equation} The first term on the RHS vanishes because \\eta \\eta vanishes at the boundaries. The second term is valid for arbitrary \\eta(x) \\eta(x) which implies \\begin{equation} \\frac{d}{dx} \\frac{y'(x)}{\\sqrt{1 + (y')^2}} dx = 0 \\nonumber \\end{equation} \\begin{equation} \\frac{d}{dx} \\frac{y'(x)}{\\sqrt{1 + (y')^2}} dx = 0 \\nonumber \\end{equation} which in turn implies y'= y'= constant, i.e. the equation of a straight line. The important things to note here are that an integral method can be used to solve a simple geometrical problem and that the method itself includes the boundary conditions as a natural consequence of the way it is set up. The variational method can be generalized to solve more important problems. In particular, instead of solving each problem as we have for the straight line / distance question above, we solve a generic problem whose solutions we can apply immediately.","title":"Example: How Short is a Straight Line ?"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#generalisation","text":"The general form works like this. To find the function y(x) y(x) which produces a stationary value of the functional \\begin{equation} J=\\int_{x_1}^{x_2} F(x,y,y') dx \\nonumber \\end{equation} \\begin{equation} J=\\int_{x_1}^{x_2} F(x,y,y') dx \\nonumber \\end{equation} we work through the same procedure as above, and use the same arguments concerning the arbitrary nature of the variation to obtain \\begin{equation} \\frac{d}{dx}\\frac{\\partial F}{\\partial y'} - \\frac{\\partial F}{\\partial y} = 0 \\nonumber \\end{equation} This is known as the Euler equation. Hamilton's principle states that mechanical systems evolve such that the integral \\begin{equation} J=\\int_{t_1}^{t_2} L dt \\nonumber \\end{equation} \\begin{equation} J=\\int_{t_1}^{t_2} L dt \\nonumber \\end{equation} is stationary. Here L L is the Lagrangian of the system which is identified with a combination of the work done on the system and the kinetic energy of the system, e.g. potential energy - kinetic energy. Application of the Euler equation to each direction independently recovers Newton's law ( F=ma F=ma ). In complex geometries and with difficult boundary conditions, the variational form may be easier to solve than the differential or \"strong\" form. This shows us that there are equivalent integral representations for the standard mechanical equations we are accustomed to using - variational or weak forms versus differential or strong forms . Although this may seem complicated and of rather theoretical interest, in fact it runs throughout finite element methods, and the concept must be familiar in order to follow how FEM works. Advantages of using variational forms of the equations include: The simplification of the construction of the governing equations in the sense that scalar quantities --- energies, potentials --- are considered in place of forces, displacements etc. There is also the possibility that such formulations can be derived more-or-less automatically for previously unknown systems. Governing equations may be more directly accessible since \"unimportant\" variables such as internal forces doing no net work do not appear in the variational form. When dealing with approximate solutions, the variational form often allows a broader range of trial functions than for the standard differential form. This happens because some boundary conditions are implicit in the formulation and hence are not imposed on the trial functions themselves.","title":"Generalisation"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#example-of-variational-forms-for-fem","text":"See Klaus-J\u00fcrgen Bathe's book for a more complete outline of this approach to FEM. Although this is not the best text to explain how to implement a finite element code, he does a great job of explaining the link between variational methods and weak forms of various equations of progressively increasing complexity. A Slab of Material Subjected to an sudden onset of heating at Q Q on one side at time t=0 t=0 The functional governing the temperature in the block of material is \\begin{equation} \\Pi = \\int_0^L \\frac{1}{2} k \\left(\\frac{\\partial \\theta}{\\partial x} \\right)^2 dx - \\int_0^L \\theta q^B dx - \\theta(0,t) Q \\nonumber \\end{equation} where q^B q^B is an internal heat generation rate. The fixed boundary condition is \\theta(L,t) =\\theta_i \\theta(L,t) =\\theta_i . This is our generalized problem with \\begin{equation} F = \\frac{1}{2}k \\left(\\frac{\\partial \\theta}{\\partial x} \\right)^2 - \\theta q^B = \\frac{1}{2}k {\\theta'}^2 - \\theta q^B \\nonumber \\end{equation} which produces a stationary functional if \\begin{equation} \\frac{d}{dx}\\frac{\\partial F}{\\partial \\theta'} - \\frac{\\partial F}{\\partial \\theta} = 0 \\nonumber \\end{equation} \\begin{equation} \\frac{d}{dx}\\frac{\\partial F}{\\partial \\theta'} - \\frac{\\partial F}{\\partial \\theta} = 0 \\nonumber \\end{equation} or, in other words \\begin{equation} k\\frac{d^2 \\theta}{d x^2} = q^B \\nonumber \\end{equation} Which we recognize to be the governing differential equation. The variational statement also contains the natural boundary condition \\begin{equation} k \\left. \\frac{\\partial \\theta}{\\partial x} \\right| _ {x=0} + Q = 0 \\nonumber \\end{equation} \\begin{equation} k \\left. \\frac{\\partial \\theta}{\\partial x} \\right| _ {x=0} + Q = 0 \\nonumber \\end{equation} This is a clear demonstration that the standard form of the equations plus certain boundary conditions can be fully wrapped up in integral form and are exactly equivalent to the standard form. The major difficulty is in how we produce the correct functional in the first place, especially if we want to avoid first deriving the differential form of the equations and back-calculating as we done above.","title":"Example of Variational Forms for FEM"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#extension-to-approximate-methods","text":"The problem above is simple enough that the integral or differential forms of the equations can be solved directly. In general, however, we anticipate dealing with problems where no closed form of solution exists. Under these circumstances approximate solutions are desirable. In particular, there is a class of approximation methods which use families of trial functions to obtain a best fit approximation to the solution. These naturally develop into finite element algorithms as we shall soon see.","title":"Extension to Approximate Methods"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#formulation-of-a-general-problem","text":"We consider a steady-state problem characterized by the following strong form \\begin{equation} {\\cal{L}} (\\phi) = f \\nonumber \\end{equation} where {\\cal{L}} {\\cal{L}} is a linear differential operator acting on the (unknown) state variable \\phi \\phi in responce to a forcing function f f . Boundary conditions are \\begin{equation} {\\cal{B}}_i [\\phi] = \\left. q_i \\right| _ {\\textrm{ \\small at boundary } S_i} \\;\\;\\; i=1,2,\\ldots \\nonumber \\end{equation} The operator should be symmetric \\begin{equation} \\int_\\Omega v {\\cal{L}}(u) d\\Omega = \\int_\\Omega u {\\cal{L}}(v) d\\Omega \\nonumber \\end{equation} and positive definite \\begin{equation} \\int_\\Omega u {\\cal{L}}(u) d\\Omega > 0 \\nonumber \\end{equation} \\Omega \\Omega is the domain of the operator and u u and v v are any functions which satisfy the boundary conditions. {. width=\"75%\" id=\"rod-end-load\"} A rod subject to end load --- Young's modulus, E E , density, \\rho \\rho , cross sectional area, A A Consider the 1D example of a bar subject to a steady end load. The response is the solution to \\begin{equation} -EA\\frac{\\partial^2 u}{\\partial x^2} = 0 \\nonumber \\end{equation} \\begin{equation} -EA\\frac{\\partial^2 u}{\\partial x^2} = 0 \\nonumber \\end{equation} subject to the boundary conditions \\begin{equation} \\begin{split} \\left. u \\right| _ {x=0} & = 0 \\\\ \\left. EA\\frac{\\partial u}{\\partial x} \\right| _ {x=L} = R \\end{split} \\end{equation} \\begin{equation} \\begin{split} \\left. u \\right| _ {x=0} & = 0 \\\\ \\left. EA\\frac{\\partial u}{\\partial x} \\right| _ {x=L} = R \\end{split} \\end{equation} We therefore identify \\begin{eqnarray} {\\cal{L}}= -EA\\frac{\\partial^2 u}{\\partial x^2} & \\phi = u & f = 0 \\ \\nonumber & B_1 = 1 ; q_1 = 0 & \\ \\nonumber & B_2 = EA\\frac{\\partial }{\\partial x}; q_2 = R & \\nonumber \\end{eqnarray} To check symmetry and positive definiteness of the operator we consider R=0 R=0 since the operator properties are independent of the actual load. Integrating by parts gives \\begin{equation} \\begin{split} \\int_0^L-EA\\frac{\\partial^2 u}{\\partial x^2} v dx & = - \\left. EA\\frac{\\partial u}{\\partial x} v \\right| _ 0^L + \\int_0^L EA \\frac{\\partial u}{\\partial x} \\frac{\\partial v}{\\partial x} dx \\\\ &= - \\left. EA\\frac{\\partial u}{\\partial x} v \\right| _ 0^L - \\left. + EA u \\frac{\\partial v}{\\partial x} \\right| _ 0^L - \\int_0^L EA\\frac{\\partial^2 v}{\\partial x^2} u dx \\end{split} \\end{equation} \\begin{equation} \\begin{split} \\int_0^L-EA\\frac{\\partial^2 u}{\\partial x^2} v dx & = - \\left. EA\\frac{\\partial u}{\\partial x} v \\right| _ 0^L + \\int_0^L EA \\frac{\\partial u}{\\partial x} \\frac{\\partial v}{\\partial x} dx \\\\ &= - \\left. EA\\frac{\\partial u}{\\partial x} v \\right| _ 0^L - \\left. + EA u \\frac{\\partial v}{\\partial x} \\right| _ 0^L - \\int_0^L EA\\frac{\\partial^2 v}{\\partial x^2} u dx \\end{split} \\end{equation} Application of boundary conditions demonstrates that the operator is symmetric by our definition. Positive definiteness is also assured since \\begin{equation} \\int_0 L-EA\\frac{\\partial 2 u}{\\partial x^2} u dx = - \\left. EA\\frac{\\partial u}{\\partial x} u \\right| _ 0^L + \\int_0^L EA \\frac{\\partial u}{\\partial x} \\frac{\\partial u}{\\partial x} dx = 0 + \\int_0^L EA \\left(\\frac{\\partial u}{\\partial x}\\right)^2 dx \\end{equation} Suppose we now search for approximate solutions of the form \\begin{equation} \\bar{\\phi} = \\sum_{i=1}^{n} a_i \\Phi_i \\nonumber \\end{equation} where \\Phi_i \\Phi_i are linearly independent trial functions and the a_i a_i are the unknown weights for each of the functions. In weighted residuals methods, the expansion is used directly on the strong form of the equations. \\Phi_i \\Phi_i are chosen so as to satisfy all boundary conditions and then we seek to minimize a residual \\begin{equation} R = f - {\\cal{L}}( \\sum_{i=1}^{n} a_i \\Phi_i ) \\end{equation} \\begin{equation} R = f - {\\cal{L}}( \\sum_{i=1}^{n} a_i \\Phi_i ) \\end{equation}","title":"Formulation of a General Problem"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#least-squares-method","text":"Minimize the square of the residual with respect to a_i a_i \\begin{equation} \\frac{\\partial}{\\partial a_i} \\int_\\Omega R^2 d\\Omega = 0 \\;\\;\\; i = 1,2,\\ldots \\nonumber \\end{equation} This method produces a symmetric coefficient matrix regardless of the properties of the operator.","title":"Least Squares Method"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#galerkin-method","text":"To determine a_i a_i , solve the n equation system \\begin{equation} \\int_\\Omega N_i R d\\Omega = 0 \\;\\;\\; i = 1,2,\\ldots \\end{equation} over the solution domain \\Omega \\Omega . This method produces a symmetric, positive definite coefficient matrix if the operator is symmetric and positive definite.","title":"Galerkin Method"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#ritz-method","text":"The Ritz method does not operate on the residual of the strong problem, but minimizes the weak form of the problem with respect to each of the unknown parameters a_i a_i in the usual variational manner. The trial functions no longer need satisfy the natural boundary conditions of the problem as these are wrapped up in the variational form. (Again, see Bathe for discussion and examples). The Galerkin method can be extended to include a term which minimizes the violation of natural boundary conditions, and thus permits the use of a wider range of trial functions \\begin{equation} \\int_\\Omega N_i R d\\Omega + \\int_\\Gamma N_i R_B d\\Gamma = 0 \\;\\;\\; i = 1,2,\\ldots \\label{eq:galerk1} \\end{equation} However, it does not now necessarily produce a symmetric matrix even for a symmetric operator. However, if the equation (\\ref{eq:galerk1} ) is integrated once by parts, it yields a symmetric form and also reduces the order of derivatives inside the integral. This means that the trial functions need be of lower order, and it makes the Galerkin formulation equivalent to the Ritz formulation. Weighted residual formulations have one advantage: they can be used whether or not there exists a functional corresponding to the particular problem. As a result is used this method is used extensively in constructing finite element methods.","title":"Ritz Method"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#finite-element-theory","text":"","title":"Finite Element Theory"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-4.html#the-genesis-of-a-matrix-method","text":"One of the dominant features of the Finite Element literature is that it is filled with matrix algebra. The fact that differential equations can be rendered into matrices seems at first to be mysterious. However, it results quite naturally from the discretization of the problem, and the parameterization of the discrete equations through a limited set of unknown parameters. Thus, before getting deeply involved in the arcane lore of Finite Elements, We give one example of a genuinely discrete system and show how it generates a matrix problem quite naturally. {. width=\"75%\" id=\"mine-carts\"} (a) A system of three carts interconnected by springs of different stiffnesses and in turn connected to an end wall. (b) The element equilibrium diagram for the spring k_2 k_2 Consider the system illustrated in the figure (a) above --- three freely rolling carts attached by springs. There are three loads applied R_1,R_2,R_3 R_1,R_2,R_3 , one to each cart, and we wish to determine the equilibrium displacements U_1,U_2,U_3 U_1,U_2,U_3 . We can illustrate graphically the equilibrium condition for one of the springs based on its internal degrees of freedom and effective external load. This equilibrium is: \\begin{equation} \\begin{split} k_2 (U_1 - U_2) &= {F_1}^{(2)} \\\\ k_2 (U_2 - U_1) &= {F_2}^{(2)} \\end{split} \\end{equation} \\begin{equation} \\begin{split} k_2 (U_1 - U_2) &= {F_1}^{(2)} \\\\ k_2 (U_2 - U_1) &= {F_2}^{(2)} \\end{split} \\end{equation} or \\begin{equation} k_2 \\left[ \\begin{array}{cc} 1 & -1 \\\\ -1 & 1 \\end{array} \\right] \\left[ \\begin{array}{c} U_1 \\\\ U_2 \\end{array} \\right] = \\left[ \\begin{array}{c} {F_1}^{(2)} \\\\ {F_2}^{(2)}\\end{array} \\right] \\end{equation} \\begin{equation} k_2 \\left[ \\begin{array}{cc} 1 & -1 \\\\ -1 & 1 \\end{array} \\right] \\left[ \\begin{array}{c} U_1 \\\\ U_2 \\end{array} \\right] = \\left[ \\begin{array}{c} {F_1}^{(2)} \\\\ {F_2}^{(2)}\\end{array} \\right] \\end{equation} This is essentially the same for all five spring elements except for k_1 k_1 which is anchored at one end and so satisfies \\begin{equation} k_1 U_1 = {F_1}^{(1)} \\nonumber \\end{equation} \\begin{equation} k_1 U_1 = {F_1}^{(1)} \\nonumber \\end{equation} The equilibrium relations for the system as a whole are \\[ \\begin{equation} \\begin{split} & {F_1}^{(1)} + {F_1}^{(2)} + {F_1}^{(3)} + {F_1}^{(4)} = R_1 \\ & {F_2}^{(2)} + {F_2}^{(3)} + {F_2}^{(5)} = R_2 \\ & {F_3}^{(4)} + {F_3}^{(5)} = R_3 \\end{split} \\label{eq:globeq} \\end{equation} \\] If we now write all five equilibrium relations in terms of all available degrees of freedom we obtain a form like this \\begin{equation} k_2 \\left[ \\begin{array}{ccc} k_2 & -k_2 & 0 \\\\ -k_2 & k_2 & 0 \\\\ 0 & 0 & 0 \\end{array} \\right] \\left[ \\begin{array}{c} U_1 \\\\ U_2 \\\\ U_3 \\end{array} \\right] = \\left[ \\begin{array}{c} {F_1}^{(2)} \\\\ {F_2}^{(2)} \\\\ 0\\end{array} \\right] \\nonumber \\end{equation} \\begin{equation} k_2 \\left[ \\begin{array}{ccc} k_2 & -k_2 & 0 \\\\ -k_2 & k_2 & 0 \\\\ 0 & 0 & 0 \\end{array} \\right] \\left[ \\begin{array}{c} U_1 \\\\ U_2 \\\\ U_3 \\end{array} \\right] = \\left[ \\begin{array}{c} {F_1}^{(2)} \\\\ {F_2}^{(2)} \\\\ 0\\end{array} \\right] \\nonumber \\end{equation} which can also be written \\begin{equation} \\mathbf{K}^{(2)}\\mathbf{U} = \\mathbf{F}^{(2)} \\nonumber \\end{equation} in each case. Thus the global equilibrium requirement of \\ref{eq:globeq} become \\begin{equation} \\mathbf{K}\\mathbf{U} = \\mathbf{R} \\nonumber \\end{equation} where \\begin{equation} \\mathbf{K} = \\left[ \\begin{array}{ccc} (k_1 + k_2 +k_3 + k_4) & -(k_2+k_3) & -k_4 \\ -(k_2+k_3) & (k_2 + k_3+k_4) & -k_5 \\ -k_4 & -k_5 & (k_4 + k_5) \\end{array} \\right] \\nonumber \\end{equation} Note, by the way, that $ \\mathbf{K}$ is symmetric. An important observation is that \\begin{equation} \\mathbf{K} = \\sum_{i=1}^{5} \\mathbf{K}^{(i)} \\nonumber \\end{equation} The individual element stiffnesses can be summed to form a global stiffness matrix. A symmetric, positive definite matrix problem can be solved in numerous different ways, many of which are easy to look up in textbooks ! A very simple problem like this has captured much of what we need to do in arbitrarily complex finite element computations. The construction of local element equilbrium problems based on the interaction of each available degree of freedom with every other is followed by the assembly into a global problem by summing the contributions of the individual elements. Note that the formulation of the local equilibrium conditions are done in a symmetric manner (if we change this degree of freedom how does the balance change, and then what if we change this degree of freedom ?) rather than trying to simplify the system. The degrees of freedom are related by elastic spring constants here. In our more abstract formulations we will replace spring constants by coefficients obtained from the variational method but the form is {\\em exactly the same}.","title":"The Genesis of a Matrix Method"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html","text":"\\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\] Weak Forms of Some Useful Equations As we discovered earlier, the Galerkin form of weighted residual approximate method can be made equivalent to a full variational problem if integration by parts is employed in the correct manner. This then allows the development of fully automatic variational methods for arbitrary strong forms of the governing equations. From here on we follow the notation of Hughes (The Finite Element Method) which is reasonably clear. 1D Heat Conduction The differential or strong form of the equation we have encountered a number of times is \\begin{equation} \\begin{split} \\frac{d^2 u}{d x^2} - {\\curly f} &= 0 \\;\\;\\; \\text{on} \\;\\;\\; \\Omega \\ u(1) & = {\\curly g} \\ -\\frac{d u}{d x} (0) &= {\\curly h} \\end{split} \\nonumber \\end{equation} where the boundary conditions are supplied at either end of a domain of unit length. To find the weak form of the equation we need to have a set of functions \\curly S \\curly S to use for the trial functions which satisfy the boundary condition \\curly g \\curly g on u u at x=1 x=1 . We also need a set of weighting functions which are zero at x=1 x=1 which we call \\curly V \\curly V . The weighting functions play the role of the variations. The statement of the weak form of the problem (as distinct from its solution which will wait till later) is then to find u \\in {\\curly S} u \\in {\\curly S} such that for all w \\in {\\curly V} w \\in {\\curly V} \\begin{equation} {\\int_0^1 \\frac{d w }{d x} \\frac{d u}{d x} dx - \\int_0^1 w f dx + w(0) h = 0} \\nonumber \\end{equation} \\begin{equation} {\\int_0^1 \\frac{d w }{d x} \\frac{d u}{d x} dx - \\int_0^1 w f dx + w(0) h = 0} \\nonumber \\end{equation} This looks a lot like the variational solutions we found before. How do we get to it ? The procedure is relatively general (except that in higher dimensions it becomes more time consuming). First assume we have found the solution to the strong form, u u . This must satisfy \\begin{equation} 0 = \\int_0^1 w( \\frac{d^2 u}{d x^2} - {\\curly f} )dx \\nonumber \\end{equation} by the definition of the problem, as long as w w is well behaved, which is ensured by a sensible choice of weighting functions. This starts to look a lot like the Galerkin approach although we are not yet seeking an approximate solution. Now integrate by parts to give \\begin{equation} 0 = \\int_0^1 \\frac{d w }{d x} \\frac{d u}{d x} dx - \\int_0^1 w f dx - \\left[ w \\frac{d u}{d x} \\right] _ 0^1 \\nonumber \\end{equation} \\begin{equation} 0 = \\int_0^1 \\frac{d w }{d x} \\frac{d u}{d x} dx - \\int_0^1 w f dx - \\left[ w \\frac{d u}{d x} \\right] _ 0^1 \\nonumber \\end{equation} the boundary conditions on du / dx du / dx are substituted to give the weak form as above. We write the equation in terms of the symmetric operators: \\begin{equation} \\begin{split} a(w,u) & \\equiv \\int_0^1 \\frac{d w }{d x} \\frac{d u}{d x} dx \\ (w,{\\curly f}) & \\equiv \\int_0^1 w f dx \\end{split} \\nonumber \\end{equation} to obtain an abstract form which can be used for all the finite element formulations we derive here \\begin{equation} a(w,u) = (w,{\\curly f}) + w(0){\\curly h} \\label{eq:FEabst} \\end{equation} \\begin{equation} a(w,u) = (w,{\\curly f}) + w(0){\\curly h} \\label{eq:FEabst} \\end{equation} Note, that the partial differentiation step produces a symmetric operator from the non symmetric initial form. This approach also works in 2D as follows. 2D Heat Conduction Define a heat flux (vector), \\mathbf{q} \\mathbf{q} related to temperature, u u as follows \\begin{equation} q_i = -K_{ij} \\frac{\\partial u}{\\partial x_j} \\nonumber \\end{equation} \\begin{equation} q_i = -K_{ij} \\frac{\\partial u}{\\partial x_j} \\nonumber \\end{equation} where \\mathbf{K} \\mathbf{K} is the symmetric conductivity tensor. (This is the generalized version of Fourier's law). A volumetric heating rate of \\curly f \\curly f gives the following strong form \\begin{equation} \\begin{split} \\nabla q - {\\curly f} &= 0 \\;\\;\\; \\text{on} \\;\\;\\; \\Omega \\\\ u & = {\\curly g} \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{\\curly g} \\\\ -q_i n_i &= {\\curly h} \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{\\curly h} \\end{split} \\nonumber \\end{equation} \\begin{equation} \\begin{split} \\nabla q - {\\curly f} &= 0 \\;\\;\\; \\text{on} \\;\\;\\; \\Omega \\\\ u & = {\\curly g} \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{\\curly g} \\\\ -q_i n_i &= {\\curly h} \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{\\curly h} \\end{split} \\nonumber \\end{equation} where \\Gamma_ {\\curly g} \\Gamma_ {\\curly g} and \\Gamma_{\\curly h} \\Gamma_{\\curly h} are the regions of the boundary with fixed temperatures and fixed fluxes respectively, \\mathbf{n} \\mathbf{n} is the unit normal over \\Gamma_{\\curly h} \\Gamma_{\\curly h} . The corresponding weak form (found in the usual way) is \\begin{equation} -\\int_\\Omega \\frac{\\partial w}{\\partial x_j} q_i d\\Omega = \\int_\\Omega w {\\curly f} d\\Omega + \\int_{\\Gamma_{\\curly h}} w{\\curly h} d\\Gamma \\nonumber \\end{equation} \\begin{equation} -\\int_\\Omega \\frac{\\partial w}{\\partial x_j} q_i d\\Omega = \\int_\\Omega w {\\curly f} d\\Omega + \\int_{\\Gamma_{\\curly h}} w{\\curly h} d\\Gamma \\nonumber \\end{equation} The symmetry of the operator is not obvious here until we substitute the constitutive law to obtain \\begin{equation} \\int_\\Omega \\frac{\\partial w}{\\partial x_j} q_i d\\Omega = \\int_\\Omega \\frac{\\partial w}{\\partial x_j} K_{ij} \\frac{\\partial u}{\\partial x_j} d\\Omega = \\int_\\Omega (\\nabla w)^T \\mathbf{K} (\\nabla u) d\\Omega \\nonumber \\end{equation} 2D/3D Fluid Flow Now we finally come around to the problem we really need to solve. The strong form of the Stokes' flow problem is identical to that of linear elasticity (which is what all the finite element textbooks deal with). The general constitutive law is \\begin{equation} \\sigma_{ij} = c_{ijkl} \\epsilon_{kl} \\nonumber \\end{equation} \\begin{equation} \\sigma_{ij} = c_{ijkl} \\epsilon_{kl} \\nonumber \\end{equation} which reduces to \\sigma_{ij} = \\eta \\epsilon_{ij} \\sigma_{ij} = \\eta \\epsilon_{ij} for homogeneous fluid. \\epsilon_{ij} \\epsilon_{ij} is defined by \\begin{equation} \\epsilon_{ij} = \\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i} \\nonumber \\end{equation} \\begin{equation} \\epsilon_{ij} = \\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i} \\nonumber \\end{equation} where \\mathbf{u} \\mathbf{u} now represents the fluid velocity. The strong form of the equation is \\begin{equation} \\frac{\\partial \\sigma _ {ij}}{\\partial x _ j} + {\\curly f} _ i = 0 \\;\\;\\; \\text{on} \\;\\;\\; \\Omega \\\\ \\nonumber \\end{equation} \\begin{equation} u _ i = {\\curly g} _ i \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{ {\\curly g} _ i} \\\\ \\nonumber \\end{equation} \\begin{equation} \\sigma _ {ij} n _ i = {\\curly h} _ i \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{ {\\curly h} _ i} \\nonumber \\end{equation} \\begin{equation} \\frac{\\partial \\sigma _ {ij}}{\\partial x _ j} + {\\curly f} _ i = 0 \\;\\;\\; \\text{on} \\;\\;\\; \\Omega \\\\ \\nonumber \\end{equation} \\begin{equation} u _ i = {\\curly g} _ i \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{ {\\curly g} _ i} \\\\ \\nonumber \\end{equation} \\begin{equation} \\sigma _ {ij} n _ i = {\\curly h} _ i \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{ {\\curly h} _ i} \\nonumber \\end{equation} with a corresponding weak form \\begin{equation} \\int_\\Omega \\left( \\frac{\\partial w _ i}{\\partial x _ j} + \\frac{\\partial w _ j}{\\partial x _ i} \\right) \\sigma _ {ij} d \\Omega = \\int _ \\Omega w _ i {\\curly f} _ i d\\Omega + \\sum _ {d=1} ^ {n _ {\\rm dim}} \\left[ \\int _ {\\Gamma _ { {\\curly h} _ i}} w _ i h _ i d\\Gamma \\right] \\nonumber \\end{equation} Which is symmetric in the trial functions and the unknown velocities since \\begin{equation} \\int_\\Omega \\left( \\frac{\\partial w_i}{\\partial x_j} + \\frac{\\partial w_j}{\\partial x_i} \\right) \\sigma_{ij} d \\Omega = \\int_\\Omega \\left( \\frac{\\partial w_i}{\\partial x_j} + \\frac{\\partial w_j}{\\partial x_i} \\right) c_{ijkl} \\left( \\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i} \\right) d \\Omega \\nonumber \\end{equation} Galerkin Approximate Weak Form We have one last task before we can develop a real numerical method. How do we get away from the highly abstract notion of trial functions, weighting functions, and weak forms to produce a compact system of equations which can be written out in a nice tidy matrix form. The answer lies in the Galerkin approximate solution method. Let us go back to the 1D problem for a moment. We have to express our solution in terms of a sum over basis functions as before \\begin{equation} w^h = \\sum_{A=1}^{n} c_A N_A \\nonumber \\end{equation} \\begin{equation} w^h = \\sum_{A=1}^{n} c_A N_A \\nonumber \\end{equation} where the h h superscript indicates that we have moved from an infinite choice of functions to a finite one based on the discrete functions N_A N_A which we will define in a minute, although for now we need to ensure that they are zero where the boundary conditions apply in the same way as \\curly V \\curly V . We define a corresponding expansion for u u \\begin{equation} \\begin{split} u^h &= \\sum_{A=1}^{n} d_A N_A + {\\curly g}N_{n+1} \\\\ {\\curly g}^h &= {\\curly g} N_{n+1} \\end{split} \\nonumber \\end{equation} \\begin{equation} \\begin{split} u^h &= \\sum_{A=1}^{n} d_A N_A + {\\curly g}N_{n+1} \\\\ {\\curly g}^h &= {\\curly g} N_{n+1} \\end{split} \\nonumber \\end{equation} which includes an extra basis function to allow the boundary condition ( u(1)={\\curly g} u(1)={\\curly g} ) to be satisfied. Substituting into (\\ref{eq:FEabst}) gives \\begin{equation} a\\left( \\sum_{A=1}^n c_A N_A , \\sum_{B=1}^n d_B N_B \\right) = \\left( \\sum_{A=1}^n c_A N_A , {\\curly f} \\right) + \\left[ \\sum_{A=1}^n c_A N_A(0) \\right] {\\curly h} -a\\left( \\sum_{A=1}^n c_A N_A , {\\curly g} N_{n+1} \\right) \\nonumber \\end{equation} \\begin{equation} a\\left( \\sum_{A=1}^n c_A N_A , \\sum_{B=1}^n d_B N_B \\right) = \\left( \\sum_{A=1}^n c_A N_A , {\\curly f} \\right) + \\left[ \\sum_{A=1}^n c_A N_A(0) \\right] {\\curly h} -a\\left( \\sum_{A=1}^n c_A N_A , {\\curly g} N_{n+1} \\right) \\nonumber \\end{equation} Our operators are symmetric and wholly linear, so the order of summation and application of the operator can be interchanged freely to give: \\begin{equation} 0 = \\sum_{A=1}^n c_A G_A \\nonumber \\end{equation} \\begin{equation} 0 = \\sum_{A=1}^n c_A G_A \\nonumber \\end{equation} where \\begin{equation} G_A = \\sum_{B=1}^n a(N_A,N_B)d_B - (N_A,{\\curly f}) - N_A(0){\\curly h} + a(N_A,A_{n+1}){\\curly g} \\nonumber \\end{equation} \\begin{equation} G_A = \\sum_{B=1}^n a(N_A,N_B)d_B - (N_A,{\\curly f}) - N_A(0){\\curly h} + a(N_A,A_{n+1}){\\curly g} \\nonumber \\end{equation} As we have done many times before, we appeal to the argument that the particular choice of variation must be totally arbitrary: this equation must hold no matter what w^h \\in {\\curly S}^h w^h \\in {\\curly S}^h we choose, and hence no matter what the combination of c_A c_A may be. This then means that G_A G_A must be identically zero independent of c_A c_A , i.e. \\begin{equation} \\sum_{B=1}^n a(N_A,N_B)d_B = (N_A,{\\curly f} ) - N_A(0){\\curly h} + a(N_A,A_{n+1}) {\\curly g} \\label{eq:gal2} \\end{equation} \\begin{equation} \\sum_{B=1}^n a(N_A,N_B)d_B = (N_A,{\\curly f} ) - N_A(0){\\curly h} + a(N_A,A_{n+1}) {\\curly g} \\label{eq:gal2} \\end{equation} As in the standard variational method, we have eliminated all references to the actual variation and left unknowns which only related to the physical variables (i.e. the coefficients of the expansion for u u ). If we simply write \\begin{equation} \\begin{split} K_{AB} &= a(N_A,N_B)\\ F_A &= (N_A,{\\curly f}) + N_A(0)h - a(N_A,N_n+1){\\curly g} \\end{split} \\label{eq:FEstdform} \\end{equation} then we have a matrix formulation immediately since (\\ref{eq:gal2}) now becomes \\begin{equation} \\sum_{B=1}^n K_{AB} d_B = F_A \\;\\;\\;\\;\\;\\; A=1,2,3,\\ldots \\nonumber \\end{equation} \\begin{equation} \\sum_{B=1}^n K_{AB} d_B = F_A \\;\\;\\;\\;\\;\\; A=1,2,3,\\ldots \\nonumber \\end{equation} or \\begin{equation} \\mathbf{K d} = \\mathbf{F} \\nonumber \\end{equation} \\begin{equation} \\mathbf{K d} = \\mathbf{F} \\nonumber \\end{equation} The matrix \\mathbf{K} \\mathbf{K} is known as the stiffness matrix - the association with the matrix of stiffnesses from our discrete set of springs being obvious. Generalization The same argument can be applied to higher dimensions, and to problems with different constitutive laws, and vector unknowns, however, the identification of the components of the stiffness matrix with the operator a(\\cdot,\\cdot) a(\\cdot,\\cdot) acting on every possible combination of the Galerkin basis functions still holds. When the unknowns are vectors, the basis functions are used in each respective direction which complicates the notation even more than before. Essentially, though it means that the entries to \\mathbf{K} \\mathbf{K} from each A A and B B are actually matrices which are n_{\\rm dim} \\times n_{\\rm dim} n_{\\rm dim} \\times n_{\\rm dim} . For example, in the constant viscosity Stokes' flow problem, we now have \\begin{equation} \\left. K _ {AB}\\right| _ {ij} = \\int _ \\Omega \\left( \\frac{\\partial N _ A}{\\partial x _ j} + \\frac{\\partial N _ A}{\\partial x _ i} \\right) \\eta \\left( \\frac{\\partial N _ B}{\\partial x _ j} + \\frac{\\partial N _ B}{\\partial x _ i} \\right) d \\Omega \\label{eq:festokes} \\end{equation} \\begin{equation} \\left. K _ {AB}\\right| _ {ij} = \\int _ \\Omega \\left( \\frac{\\partial N _ A}{\\partial x _ j} + \\frac{\\partial N _ A}{\\partial x _ i} \\right) \\eta \\left( \\frac{\\partial N _ B}{\\partial x _ j} + \\frac{\\partial N _ B}{\\partial x _ i} \\right) d \\Omega \\label{eq:festokes} \\end{equation} Discretization & Shape Functions Although we have now nominally made our problem finite by expressing everything in terms of Galerkin basis functions, we have yet to decide what those functions ought to be. Whatever our choice, it should make the problem easy to compute numerically. {. width=\"75%\" id=\"domain-decomp\"} Splitting an arbitrary domain into patches is trivial in an integral formulation. Additionally, the irregular shape of the individual subdomains is easy to handle with standard changes of variables (Jacobians) The choice of basis functions is very broad but we narrow it down by tying it very closely to the way we choose to split up the domain of our problem. With an integral method, the domain decomposition is simple since \\begin{equation} \\int_\\Omega d\\Omega = \\int_{\\Omega_1} d {\\Omega_1} + \\int_{\\Omega_2} d {\\Omega_2} + \\int_{\\Omega_3} d {\\Omega_3} + \\ldots \\label{eq:decomp} \\end{equation} \\begin{equation} \\int_\\Omega d\\Omega = \\int_{\\Omega_1} d {\\Omega_1} + \\int_{\\Omega_2} d {\\Omega_2} + \\int_{\\Omega_3} d {\\Omega_3} + \\ldots \\label{eq:decomp} \\end{equation} which means that the decomposition shown above is as accurate as the representation of the boundary allows. The functions which interpolate the node points at the corners of subdomains are a suitable set for use as an approximate basis as we require for the Galerkin formulation. This is easy to see if we consider a one dimensional problem. {. width=\"75%\" id=\"discretisation-1d\"} Representing a piecewise linear function as a sum of pointy functions localized at the nodes In the one dimensional case, the choice of subdomains is limited to breaking up the line into a number of segments, though not necessarily of equal length. If the approximation to the continuum field, u u is made by linear interpolation to give u^h u^h , then u^h u^h can also be expressed as a sum on the local triangular functions N_A N_A \\begin{equation} u^h = \\sum_{A} u(x_A) N_A(x) \\nonumber \\end{equation} \\begin{equation} u^h = \\sum_{A} u(x_A) N_A(x) \\nonumber \\end{equation} This is an exact representation of the interpolation provided the N_A N_A takes the value one at node A A and zero at all other nodes, and varies in a linear manner in between. Note that the functions all take the same form apart from scaling. Also, because the basis functions are localized around a node and its neighbours, the direct interaction between a node and its immediate neighbours is non-zero but the interaction with more distant nodes is zero. This makes the stiffness matrix banded --- more importantly it is sparse and therefore the potentially enormous number of interactions (the square of the number of unknowns) is contained. This procedure can be extended to higher dimensions and to higher order interpolations as shown in the Figure below Naming of Things and What They Look Like The interpolation functions are known as shape functions. The subdomains are known as elements. The elements correspond to the individual springs of our discrete example. The shape functions are pure interpolation functions of the required order within the element and can be differentiated the appropriate number of times. This means that the order of interpolation must match the order of derivatives in the FE operator. Crossing the element boundaries, the shape functions have discontinuous derivatives (as do the piecewise interpolations). Continuous derivative functions are possible (e.g. splines) but add significant complexity. Shape functions in 2D and 3D can be formed from the product of 1D shape functions in each direction. {. width=\"75%\" id=\"shape-functions\"} Extension of simple shape function concept (a) to higher order functions and (b) to higher dimensions Element Matrices The domain decomposition illustrated above produces a minature version of the matrix problem on the individual elements themselves. \\begin{equation} {k^e}_{ij} {d^e}_j = {f^e}_i \\nonumber \\end{equation} \\begin{equation} {k^e}_{ij} {d^e}_j = {f^e}_i \\nonumber \\end{equation} The interpretation of the local problem is similar to that of the individual springs in the discrete example we derived earlier. By assembling the individual under-constrained element problems into the full problem we obtain a soluble system with the correct number of boundary conditions etc. The local equilibrium problem is identical with that of the global problem. That is, we use the same variational form and the same operators to build the local matrices. In general, finite element methods are designed around the local element frame of reference and large problems assembled from smaller ones. The book-keeping involved is to track which degrees of freedom in the local problem are associated with which others in the global problem. Clearly the interaction coefficient between one degree of freedom and another is derived from the local equilibrium relationships of a number of elements because the shape function for a given nodal point is spread across a number of elements. Numerical Integration The real power of finite elements comes from its ability to handle remarkably distorted geometries with ease. (Obviously this only applies in two or more dimensions). This is possible because simple mappings can be derived to convert a distorted element into a regular one (square, cubic, regular-tetrahedral). This takes the form of a change of variables which can be done {\\em fully automatically} if we make minor restrictions on the choice of distortions we allow the elements to have. This restriction takes the form of ensuring that the mappings which transform the element shape can be described by the shape functions of the element. This means that only linear mappings can be applied to linear elements. If we wish to map a curved boundary exactly, then elements which have the appropriate order shape functions have to be used even if they are not needed for the representation of the differential operator. (e.g. quadratic elements for a circular boundary). This concept produces \"isoparametric\" elements. The mapping shown in the domain decomposition figure is achieved by a change of variables \\begin{equation} \\int \\int_{\\Omega^e} \\phi dx dy = \\int \\int_{\\rm square} \\phi {\\curly j} d \\xi d \\eta \\nonumber \\end{equation} \\begin{equation} \\int \\int_{\\Omega^e} \\phi dx dy = \\int \\int_{\\rm square} \\phi {\\curly j} d \\xi d \\eta \\nonumber \\end{equation} where {\\curly j} {\\curly j} is the jacobian of the transformation defined by \\begin{equation} {\\curly j} = {\\rm det} \\frac{\\partial \\mathbf{x}}{\\partial \\boldsymbol{\\xi}} = {\\rm det} \\left[ \\begin{array}{cc} \\frac{\\partial x_1}{\\partial \\xi_1} & \\frac{\\partial x_1}{\\partial \\xi_2} \\ \\frac{\\partial x_2}{\\partial \\xi_1} & \\frac{\\partial x_2}{\\partial \\xi_2} \\end{array} \\right] \\nonumber \\end{equation} In the isoparametric concept, the components of the jacobian can be written as, for example, \\begin{equation} \\frac{\\partial x_1}{\\partial \\xi_1} (\\xi_1,\\xi_2) = \\sum_{a=1}^{n_en} \\frac{\\partial N_a}{\\partial \\xi_1} {x_1}_a \\nonumber \\end{equation} Now we have the integral of a number of things defined over a regular, square domain (or cube etc). A number of ways to estimate these integrals is available. For example, in 1D the familiar trapezium rule can integrate linearly interpolated functions exactly. Given that our approximate method has already reduced the degree of polynomial within the element to something manageable, it is possible to find integration schemes which produce the {\\em exact} integral for our approximation --- in other words introducing no further error. One possibility which is commonly used is Gaussian quadrature. This is a textbook method which can be made exact for any order of interpolation function desired. For linear interpolation in 2D, the following rule applies (assuming that we have transformed to a square element) \\begin{equation} \\int\\limits_{-1}^{1} \\int\\limits_{-1}^{1} \\phi(\\xi,\\eta) d\\xi d\\eta \\ \\ \\ \\cong \\ \\ \\ \\sum_{l=1}^{n_{\\rm int}} \\phi( \\tilde{ \\xi_l}, \\tilde{ \\eta_l}) W_l \\nonumber \\end{equation} \\begin{equation} \\int\\limits_{-1}^{1} \\int\\limits_{-1}^{1} \\phi(\\xi,\\eta) d\\xi d\\eta \\ \\ \\ \\cong \\ \\ \\ \\sum_{l=1}^{n_{\\rm int}} \\phi( \\tilde{ \\xi_l}, \\tilde{ \\eta_l}) W_l \\nonumber \\end{equation} In which n_{\\rm int} n_{\\rm int} is the number of points in the quadrature rule with co-ordinates (\\tilde{ \\xi_l}, \\tilde{ \\eta_l}) (\\tilde{ \\xi_l}, \\tilde{ \\eta_l}) . Each point has a weight associated with it of W_l W_l . For the four point rule: $ l $ \\tilde{\\xi _ l} \\tilde{\\xi _ l} \\tilde{\\eta _ l} \\tilde{\\eta _ l} W _ l W _ l 1 $ -1/ \\surd{3} $ \\( -1/ \\sqrt{3} \\) 1 2 $ +1/ \\surd{3} $ \\( -1/ \\sqrt{3} \\) 1 3 $ -1/ \\surd{3} $ \\( +1/ \\sqrt{3} \\) 1 4 $ +1 /\\surd{3} $ \\( +1/ \\sqrt{3} \\) 1 The four-point rule in two dimensions is constructed by applying a two point, one dimensional rule to each of the coordinates in turn. The integrals along the edges of the elements which are required to construct the force vectors are therefore should be calculated using the one dimensional, two point rule which, along an edge of the bi-unit master element is l l \\tilde{\\xi _ l} \\tilde{\\xi _ l} W _ l W _ l 1 -1/\\sqrt\\{3\\} -1/\\sqrt\\{3\\} 1 2 +1/\\sqrt\\{3\\} +1/\\sqrt\\{3\\} 1 This naturally extends to three dimensional elements and their two dimensional boundaries. Standard Form for Everything Consider the heat 1D conduction problem one more time. We can rewrite the generalized Fourier law as \\begin{equation} q_i = - \\kappa_{ij} \\frac{\\partial u}{\\partial x_j} = -\\kappa_{ij} \\left( \\begin{array}{c} \\frac{\\partial}{\\partial x_1} \\\\ \\frac{\\partial }{\\partial x_2} \\end{array} \\right) u \\nonumber \\end{equation} \\begin{equation} q_i = - \\kappa_{ij} \\frac{\\partial u}{\\partial x_j} = -\\kappa_{ij} \\left( \\begin{array}{c} \\frac{\\partial}{\\partial x_1} \\\\ \\frac{\\partial }{\\partial x_2} \\end{array} \\right) u \\nonumber \\end{equation} If we express u u in terms of the shape functions \\begin{equation} u = \\sum_A d_A N_A (x) \\rightarrow q_i = -\\kappa _ {ij} \\sum_A \\left( \\begin{array}{c} {\\displaystyle \\frac{\\partial N _ A}{\\partial x _ 1}} \\\\ { \\displaystyle \\frac{\\partial N _ A}{\\partial x _ 2}} \\end{array} \\right) d _ A q_i = - \\sum_A \\kappa _ {ij} \\left. B _ A \\right| _ {j} d _ A \\nonumber \\end{equation} \\begin{equation} u = \\sum_A d_A N_A (x) \\rightarrow q_i = -\\kappa _ {ij} \\sum_A \\left( \\begin{array}{c} {\\displaystyle \\frac{\\partial N _ A}{\\partial x _ 1}} \\\\ { \\displaystyle \\frac{\\partial N _ A}{\\partial x _ 2}} \\end{array} \\right) d _ A q_i = - \\sum_A \\kappa _ {ij} \\left. B _ A \\right| _ {j} d _ A \\nonumber \\end{equation} This allows us to define a matrix \\mathbf{B _ A} \\mathbf{B _ A} which comes directly from the operators contained in the constitutive law. Compare this with equation ( \\ref{eq:FEstdform}) and, the more concrete example (\\ref{eq:festokes}) and we see that the stiffness matrix coefficients can be obtained from \\begin{equation} K_{AB} = a(N_A,N_B) = \\int_{\\Omega} \\mathbf{B}_A^T \\mathbf{D} \\mathbf{B}_B d\\Omega \\nonumber \\end{equation} \\begin{equation} K_{AB} = a(N_A,N_B) = \\int_{\\Omega} \\mathbf{B}_A^T \\mathbf{D} \\mathbf{B}_B d\\Omega \\nonumber \\end{equation} where \\mathbf{D} \\mathbf{D} is a matrix of material properties. This form can be used for all problems and may greatly simplify both programming and the automation of the development of the equations since now we simply need to be able to express the constitutive law in terms of the unknowns, build a material property matrix and plug this into standard machinery. Constraints We generally want to solve problems where there is a constraint on the unknowns. For example we wish to find a flow solution which also satisfies mass conservation (not unreasonable !). To see how this constraint may be applied we go back to the variational method and regard the equation {\\bf Kd = f} {\\bf Kd = f} in the light of the functional \\begin{equation} {\\cal F} ({\\bf d}) = \\frac{ {\\bf d}^T{\\bf Kd}}{2} - {\\bf d}^T {\\bf f} \\nonumber \\end{equation} \\begin{equation} {\\cal F} ({\\bf d}) = \\frac{ {\\bf d}^T{\\bf Kd}}{2} - {\\bf d}^T {\\bf f} \\nonumber \\end{equation} The vector which minimizes {\\cal F} ({\\bf d}) {\\cal F} ({\\bf d}) is the solution to {\\bf Kd = f} {\\bf Kd = f} - this can be shown in the usual way. (Start with $ {\\cal F}({\\bf d + \\varepsilon c}) $, where \\varepsilon \\varepsilon is a free, real parameter and \\bf c \\bf c is arbitrary). Consider a single constraint, \\begin{equation} d_Q = {\\curly g} \\nonumber \\end{equation} \\begin{equation} d_Q = {\\curly g} \\nonumber \\end{equation} which corresponds to specifying a value for one of the velocities in the problem having the index Q Q in the global numbering system. This constraint should be written as a function of \\bf d \\bf d in the following way: \\begin{equation} 0 = {\\cal G}({\\bf d}) = {\\bf 1}_Q^T{\\bf d} - {\\curly g} \\nonumber \\end{equation} \\begin{equation} 0 = {\\cal G}({\\bf d}) = {\\bf 1}_Q^T{\\bf d} - {\\curly g} \\nonumber \\end{equation} \\begin{eqnarray} {\\bf 1}^T_Q = \\left\\langle 0 \\ldots 0 \\right. & 1 & \\left. 0 \\ldots 0 \\right\\rangle \\\\ \\nonumber & \\uparrow & \\! \\\\ \\nonumber & \\text{ Q'th column} & \\nonumber \\end{eqnarray} \\begin{eqnarray} {\\bf 1}^T_Q = \\left\\langle 0 \\ldots 0 \\right. & 1 & \\left. 0 \\ldots 0 \\right\\rangle \\\\ \\nonumber & \\uparrow & \\! \\\\ \\nonumber & \\text{ Q'th column} & \\nonumber \\end{eqnarray} Then finding the stationary value for the following function is equivalent to solving the constrained problem: \\begin{equation} {\\cal H}({\\bf d},m) = {\\cal F}({\\bf d}) + m{\\cal G}({\\bf d}) \\nonumber \\end{equation} \\begin{equation} {\\cal H}({\\bf d},m) = {\\cal F}({\\bf d}) + m{\\cal G}({\\bf d}) \\nonumber \\end{equation} The condition that \\bf d \\bf d renders \\cal H \\cal H stationary is \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} {\\cal H}({\\bf d}+\\varepsilon{\\bf c},m + \\varepsilon l) \\right| _ {\\varepsilon=0} \\;\\;\\; \\forall {\\bf c},l \\nonumber \\end{equation} \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} {\\cal H}({\\bf d}+\\varepsilon{\\bf c},m + \\varepsilon l) \\right| _ {\\varepsilon=0} \\;\\;\\; \\forall {\\bf c},l \\nonumber \\end{equation} Substituting for \\cal H \\cal H gives: \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} \\left[ {\\cal F}({\\bf d}+\\varepsilon{\\bf c} )+ (m+\\varepsilon l){\\cal G}({\\bf d}+\\varepsilon{\\bf c} )\\right] \\right| _ {\\varepsilon=0} \\nonumber \\end{equation} \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} \\left[ {\\cal F}({\\bf d}+\\varepsilon{\\bf c} )+ (m+\\varepsilon l){\\cal G}({\\bf d}+\\varepsilon{\\bf c} )\\right] \\right| _ {\\varepsilon=0} \\nonumber \\end{equation} \\begin{equation} 0= {\\bf c}^T({\\bf Kd -F) }+l {\\cal G}({\\bf d}) + m{\\bf 1}_Q^T {\\bf c} \\nonumber \\end{equation} \\begin{equation} 0= {\\bf c}^T({\\bf Kd -F) }+l {\\cal G}({\\bf d}) + m{\\bf 1}_Q^T {\\bf c} \\nonumber \\end{equation} \\begin{equation} 0= {\\bf c}^T({\\bf Kd} + m{\\bf 1} _ Q {\\bf -f}) + l ({\\bf 1}_Q^T {\\bf d} -\\mbox{\\curly g}) \\nonumber \\end{equation} \\begin{equation} 0= {\\bf c}^T({\\bf Kd} + m{\\bf 1} _ Q {\\bf -f}) + l ({\\bf 1}_Q^T {\\bf d} -\\mbox{\\curly g}) \\nonumber \\end{equation} Since \\bf c \\bf c and l l are strictly arbitrary, \\begin{eqnarray} {\\bf Kd }+m{\\bf 1} _ Q &=& {\\bf f}\\\\ \\nonumber {\\bf 1} _ Q^T {\\bf d} & = & {\\curly g} \\nonumber \\end{eqnarray} \\begin{eqnarray} {\\bf Kd }+m{\\bf 1} _ Q &=& {\\bf f}\\\\ \\nonumber {\\bf 1} _ Q^T {\\bf d} & = & {\\curly g} \\nonumber \\end{eqnarray} Equivalently \\begin{equation} \\left[ \\begin{array}{cc} {\\bf K} & {\\bf 1} _ Q \\\\\\\\ {\\bf 1} _ Q^T & 0 \\end{array} \\right] \\left\\\\{ \\begin{array}{c} {\\bf d} \\\\\\\\ m \\end{array} \\right\\\\} = \\left\\\\{ \\begin{array}{c} {\\bf f} \\\\\\\\ {\\curly g} \\end{array} \\right\\\\} \\end{equation} \\begin{equation} \\left[ \\begin{array}{cc} {\\bf K} & {\\bf 1} _ Q \\\\\\\\ {\\bf 1} _ Q^T & 0 \\end{array} \\right] \\left\\\\{ \\begin{array}{c} {\\bf d} \\\\\\\\ m \\end{array} \\right\\\\} = \\left\\\\{ \\begin{array}{c} {\\bf f} \\\\\\\\ {\\curly g} \\end{array} \\right\\\\} \\end{equation} which establishes the pattern expected for adding constraints to the physical problem: augmentation of all the matrices with some forces ( \\bf m \\bf m ) as well as velocities ( \\bf d \\bf d ) unknown. The coefficient matrix remains symmetric but is no longer positive definite. An alternative way to enforce constraints and one which proves slightly simpler to implement, is to make an approximation to the Lagrange-multiplier as follows: \\begin{equation} m \\cong k {\\cal G}({\\bf d}) \\nonumber \\end{equation} \\begin{equation} m \\cong k {\\cal G}({\\bf d}) \\nonumber \\end{equation} in which k k is a large positive number. Form the following functional, \\begin{equation} {\\cal J}({\\bf d}) = {\\cal F}({\\bf d}) + \\frac{k}{2} \\left[{\\cal G} ({\\bf d})\\right]^2 \\nonumber \\end{equation} \\begin{equation} {\\cal J}({\\bf d}) = {\\cal F}({\\bf d}) + \\frac{k}{2} \\left[{\\cal G} ({\\bf d})\\right]^2 \\nonumber \\end{equation} and consider the extremal values for \\cal J \\cal J which are defined by the following expression \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} \\left( {\\cal F}({\\bf d}+\\varepsilon {\\bf c} )+ \\frac{k}{2} \\left[ {\\cal G} ({\\bf d} + \\varepsilon{\\bf c}) \\right]^2 \\right) \\right| _ {\\varepsilon = 0} \\nonumber \\end{equation} \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} \\left( {\\cal F}({\\bf d}+\\varepsilon {\\bf c} )+ \\frac{k}{2} \\left[ {\\cal G} ({\\bf d} + \\varepsilon{\\bf c}) \\right]^2 \\right) \\right| _ {\\varepsilon = 0} \\nonumber \\end{equation} \\begin{equation} {\\bf c}^T({\\bf Kd - f}) + k {\\bf G}({\\bf d}) {\\bf 1}_Q^T {\\bf c} \\nonumber \\end{equation} \\begin{equation} {\\bf c}^T({\\bf Kd - f}) + k {\\bf G}({\\bf d}) {\\bf 1}_Q^T {\\bf c} \\nonumber \\end{equation} Substituting for \\cal G \\cal G gives \\begin{equation} {\\bf c}^T \\left[ ({\\bf K} + k{\\bf 1}_Q{\\bf 1}_Q^T){\\bf d} - ({\\bf f} + k {\\curly g} {\\bf 1}_Q) \\right] = 0 \\nonumber \\end{equation} \\begin{equation} {\\bf c}^T \\left[ ({\\bf K} + k{\\bf 1}_Q{\\bf 1}_Q^T){\\bf d} - ({\\bf f} + k {\\curly g} {\\bf 1}_Q) \\right] = 0 \\nonumber \\end{equation} Since \\bf c \\bf c is arbitrary, the following matrix equation is implied \\begin{equation} ({\\bf K} + k{\\bf 1}_Q{\\bf 1}_Q^T){\\bf d} = ({\\bf f} + k {\\curly g} {\\bf 1}_Q) \\nonumber \\end{equation} \\begin{equation} ({\\bf K} + k{\\bf 1}_Q{\\bf 1}_Q^T){\\bf d} = ({\\bf f} + k {\\curly g} {\\bf 1}_Q) \\nonumber \\end{equation} Now the dimension of the problem is not changed; the constraints are introduced by addition of another matrix equation. In the limit k \\rightarrow \\infty k \\rightarrow \\infty , d_Q \\rightarrow {\\curly g} d_Q \\rightarrow {\\curly g} and the constraint is applied exactly. The constraint of incompressibility is applied as the limiting case of slight compressibility. The large constant is known as the penalty parameter and gives its name to the method; its value is chosen according to the accuracy of the machine used to compute the solution to the problem. A suitable value is somewhere between 10^7 10^7 and 10^9 10^9 , any smaller and the fluid volume may not be conserved during hydrostatic loading. The penalty method is computationally very simple, but can be ill conditioned. This becomes a serious issue when the matrix equation is not solved by direct methods. In the latter case, it is preferable to introduce additional variables. These take the form of pressures which, in the slightly compressible flow case can always be eliminated in favour of the constraint, and then recovered as \\begin{equation} p = - \\lambda \\nabla . {\\bf u} \\nonumber \\end{equation} \\begin{equation} p = - \\lambda \\nabla . {\\bf u} \\nonumber \\end{equation} The augmented matrix equation then becomes \\begin{equation} \\left( \\begin{array}{cc} \\mathbf{K} & \\mathbf{G} \\\\\\\\ \\mathbf{G}^T & 0 \\end{array} \\right) \\left( \\begin{array}{c} \\mathbf{u} \\\\\\\\ \\mathbf{p} \\end{array} \\right)= \\left( \\begin{array}{c} \\mathbf{f} \\\\\\\\ \\mathbf{0} \\end{array} \\right) \\nonumber \\end{equation} \\begin{equation} \\left( \\begin{array}{cc} \\mathbf{K} & \\mathbf{G} \\\\\\\\ \\mathbf{G}^T & 0 \\end{array} \\right) \\left( \\begin{array}{c} \\mathbf{u} \\\\\\\\ \\mathbf{p} \\end{array} \\right)= \\left( \\begin{array}{c} \\mathbf{f} \\\\\\\\ \\mathbf{0} \\end{array} \\right) \\nonumber \\end{equation} The components of the \\mathbf{G} \\mathbf{G} matrix are, at the element level found from \\begin{equation} \\left. g^e_{a\\tilde{a}} \\right| _ {i} = -\\int_{\\Omega^e} {\\rm div}(N_a) (\\tilde{N}) _ {\\tilde{a}} d\\Omega \\nonumber \\end{equation} \\begin{equation} \\left. g^e_{a\\tilde{a}} \\right| _ {i} = -\\int_{\\Omega^e} {\\rm div}(N_a) (\\tilde{N}) _ {\\tilde{a}} d\\Omega \\nonumber \\end{equation} The tilde indicates a pressure degree of freedom / shape function as constrasted with the standard velocity shape fucntions. The independent pressures require their own space of trial functions which can be of lower order than the velocity since the derivatives in the equations are also lower. In fact, it is necessary to use lower order functions for the pressure otherwise the problem is overconstrained and no non-trivial solution can be found. This is more of a difficulty when the penalty formulation is used since there are no additional variables and hence no associated trial functions. Instead a trick is used - the constraint terms are integrated using an order lower integration scheme than that required for exact solutions. The result turns out to be equivalent (usually) to the introduction of additional variables and lower order shape functions but the proof of this is another tricky matter. Stresses Note Derivatives of the shape functions are commonly not defined at the node points and consequently it is not possible to compute stresses etc at the nodal points. They are defined on the interiors of the elements and must be extrapolated to the nodes using some form of best fit procedure.","title":"NumericalMethodsPrimer 5"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#weak-forms-of-some-useful-equations","text":"As we discovered earlier, the Galerkin form of weighted residual approximate method can be made equivalent to a full variational problem if integration by parts is employed in the correct manner. This then allows the development of fully automatic variational methods for arbitrary strong forms of the governing equations. From here on we follow the notation of Hughes (The Finite Element Method) which is reasonably clear.","title":"Weak Forms of Some Useful Equations"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#1d-heat-conduction","text":"The differential or strong form of the equation we have encountered a number of times is \\begin{equation} \\begin{split} \\frac{d^2 u}{d x^2} - {\\curly f} &= 0 \\;\\;\\; \\text{on} \\;\\;\\; \\Omega \\ u(1) & = {\\curly g} \\ -\\frac{d u}{d x} (0) &= {\\curly h} \\end{split} \\nonumber \\end{equation} where the boundary conditions are supplied at either end of a domain of unit length. To find the weak form of the equation we need to have a set of functions \\curly S \\curly S to use for the trial functions which satisfy the boundary condition \\curly g \\curly g on u u at x=1 x=1 . We also need a set of weighting functions which are zero at x=1 x=1 which we call \\curly V \\curly V . The weighting functions play the role of the variations. The statement of the weak form of the problem (as distinct from its solution which will wait till later) is then to find u \\in {\\curly S} u \\in {\\curly S} such that for all w \\in {\\curly V} w \\in {\\curly V} \\begin{equation} {\\int_0^1 \\frac{d w }{d x} \\frac{d u}{d x} dx - \\int_0^1 w f dx + w(0) h = 0} \\nonumber \\end{equation} \\begin{equation} {\\int_0^1 \\frac{d w }{d x} \\frac{d u}{d x} dx - \\int_0^1 w f dx + w(0) h = 0} \\nonumber \\end{equation} This looks a lot like the variational solutions we found before. How do we get to it ? The procedure is relatively general (except that in higher dimensions it becomes more time consuming). First assume we have found the solution to the strong form, u u . This must satisfy \\begin{equation} 0 = \\int_0^1 w( \\frac{d^2 u}{d x^2} - {\\curly f} )dx \\nonumber \\end{equation} by the definition of the problem, as long as w w is well behaved, which is ensured by a sensible choice of weighting functions. This starts to look a lot like the Galerkin approach although we are not yet seeking an approximate solution. Now integrate by parts to give \\begin{equation} 0 = \\int_0^1 \\frac{d w }{d x} \\frac{d u}{d x} dx - \\int_0^1 w f dx - \\left[ w \\frac{d u}{d x} \\right] _ 0^1 \\nonumber \\end{equation} \\begin{equation} 0 = \\int_0^1 \\frac{d w }{d x} \\frac{d u}{d x} dx - \\int_0^1 w f dx - \\left[ w \\frac{d u}{d x} \\right] _ 0^1 \\nonumber \\end{equation} the boundary conditions on du / dx du / dx are substituted to give the weak form as above. We write the equation in terms of the symmetric operators: \\begin{equation} \\begin{split} a(w,u) & \\equiv \\int_0^1 \\frac{d w }{d x} \\frac{d u}{d x} dx \\ (w,{\\curly f}) & \\equiv \\int_0^1 w f dx \\end{split} \\nonumber \\end{equation} to obtain an abstract form which can be used for all the finite element formulations we derive here \\begin{equation} a(w,u) = (w,{\\curly f}) + w(0){\\curly h} \\label{eq:FEabst} \\end{equation} \\begin{equation} a(w,u) = (w,{\\curly f}) + w(0){\\curly h} \\label{eq:FEabst} \\end{equation} Note, that the partial differentiation step produces a symmetric operator from the non symmetric initial form. This approach also works in 2D as follows.","title":"1D Heat Conduction"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#2d-heat-conduction","text":"Define a heat flux (vector), \\mathbf{q} \\mathbf{q} related to temperature, u u as follows \\begin{equation} q_i = -K_{ij} \\frac{\\partial u}{\\partial x_j} \\nonumber \\end{equation} \\begin{equation} q_i = -K_{ij} \\frac{\\partial u}{\\partial x_j} \\nonumber \\end{equation} where \\mathbf{K} \\mathbf{K} is the symmetric conductivity tensor. (This is the generalized version of Fourier's law). A volumetric heating rate of \\curly f \\curly f gives the following strong form \\begin{equation} \\begin{split} \\nabla q - {\\curly f} &= 0 \\;\\;\\; \\text{on} \\;\\;\\; \\Omega \\\\ u & = {\\curly g} \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{\\curly g} \\\\ -q_i n_i &= {\\curly h} \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{\\curly h} \\end{split} \\nonumber \\end{equation} \\begin{equation} \\begin{split} \\nabla q - {\\curly f} &= 0 \\;\\;\\; \\text{on} \\;\\;\\; \\Omega \\\\ u & = {\\curly g} \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{\\curly g} \\\\ -q_i n_i &= {\\curly h} \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{\\curly h} \\end{split} \\nonumber \\end{equation} where \\Gamma_ {\\curly g} \\Gamma_ {\\curly g} and \\Gamma_{\\curly h} \\Gamma_{\\curly h} are the regions of the boundary with fixed temperatures and fixed fluxes respectively, \\mathbf{n} \\mathbf{n} is the unit normal over \\Gamma_{\\curly h} \\Gamma_{\\curly h} . The corresponding weak form (found in the usual way) is \\begin{equation} -\\int_\\Omega \\frac{\\partial w}{\\partial x_j} q_i d\\Omega = \\int_\\Omega w {\\curly f} d\\Omega + \\int_{\\Gamma_{\\curly h}} w{\\curly h} d\\Gamma \\nonumber \\end{equation} \\begin{equation} -\\int_\\Omega \\frac{\\partial w}{\\partial x_j} q_i d\\Omega = \\int_\\Omega w {\\curly f} d\\Omega + \\int_{\\Gamma_{\\curly h}} w{\\curly h} d\\Gamma \\nonumber \\end{equation} The symmetry of the operator is not obvious here until we substitute the constitutive law to obtain \\begin{equation} \\int_\\Omega \\frac{\\partial w}{\\partial x_j} q_i d\\Omega = \\int_\\Omega \\frac{\\partial w}{\\partial x_j} K_{ij} \\frac{\\partial u}{\\partial x_j} d\\Omega = \\int_\\Omega (\\nabla w)^T \\mathbf{K} (\\nabla u) d\\Omega \\nonumber \\end{equation}","title":"2D Heat Conduction"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#2d3d-fluid-flow","text":"Now we finally come around to the problem we really need to solve. The strong form of the Stokes' flow problem is identical to that of linear elasticity (which is what all the finite element textbooks deal with). The general constitutive law is \\begin{equation} \\sigma_{ij} = c_{ijkl} \\epsilon_{kl} \\nonumber \\end{equation} \\begin{equation} \\sigma_{ij} = c_{ijkl} \\epsilon_{kl} \\nonumber \\end{equation} which reduces to \\sigma_{ij} = \\eta \\epsilon_{ij} \\sigma_{ij} = \\eta \\epsilon_{ij} for homogeneous fluid. \\epsilon_{ij} \\epsilon_{ij} is defined by \\begin{equation} \\epsilon_{ij} = \\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i} \\nonumber \\end{equation} \\begin{equation} \\epsilon_{ij} = \\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i} \\nonumber \\end{equation} where \\mathbf{u} \\mathbf{u} now represents the fluid velocity. The strong form of the equation is \\begin{equation} \\frac{\\partial \\sigma _ {ij}}{\\partial x _ j} + {\\curly f} _ i = 0 \\;\\;\\; \\text{on} \\;\\;\\; \\Omega \\\\ \\nonumber \\end{equation} \\begin{equation} u _ i = {\\curly g} _ i \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{ {\\curly g} _ i} \\\\ \\nonumber \\end{equation} \\begin{equation} \\sigma _ {ij} n _ i = {\\curly h} _ i \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{ {\\curly h} _ i} \\nonumber \\end{equation} \\begin{equation} \\frac{\\partial \\sigma _ {ij}}{\\partial x _ j} + {\\curly f} _ i = 0 \\;\\;\\; \\text{on} \\;\\;\\; \\Omega \\\\ \\nonumber \\end{equation} \\begin{equation} u _ i = {\\curly g} _ i \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{ {\\curly g} _ i} \\\\ \\nonumber \\end{equation} \\begin{equation} \\sigma _ {ij} n _ i = {\\curly h} _ i \\;\\;\\; \\text{on} \\;\\;\\; \\Gamma_{ {\\curly h} _ i} \\nonumber \\end{equation} with a corresponding weak form \\begin{equation} \\int_\\Omega \\left( \\frac{\\partial w _ i}{\\partial x _ j} + \\frac{\\partial w _ j}{\\partial x _ i} \\right) \\sigma _ {ij} d \\Omega = \\int _ \\Omega w _ i {\\curly f} _ i d\\Omega + \\sum _ {d=1} ^ {n _ {\\rm dim}} \\left[ \\int _ {\\Gamma _ { {\\curly h} _ i}} w _ i h _ i d\\Gamma \\right] \\nonumber \\end{equation} Which is symmetric in the trial functions and the unknown velocities since \\begin{equation} \\int_\\Omega \\left( \\frac{\\partial w_i}{\\partial x_j} + \\frac{\\partial w_j}{\\partial x_i} \\right) \\sigma_{ij} d \\Omega = \\int_\\Omega \\left( \\frac{\\partial w_i}{\\partial x_j} + \\frac{\\partial w_j}{\\partial x_i} \\right) c_{ijkl} \\left( \\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i} \\right) d \\Omega \\nonumber \\end{equation}","title":"2D/3D Fluid Flow"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#galerkin-approximate-weak-form","text":"We have one last task before we can develop a real numerical method. How do we get away from the highly abstract notion of trial functions, weighting functions, and weak forms to produce a compact system of equations which can be written out in a nice tidy matrix form. The answer lies in the Galerkin approximate solution method. Let us go back to the 1D problem for a moment. We have to express our solution in terms of a sum over basis functions as before \\begin{equation} w^h = \\sum_{A=1}^{n} c_A N_A \\nonumber \\end{equation} \\begin{equation} w^h = \\sum_{A=1}^{n} c_A N_A \\nonumber \\end{equation} where the h h superscript indicates that we have moved from an infinite choice of functions to a finite one based on the discrete functions N_A N_A which we will define in a minute, although for now we need to ensure that they are zero where the boundary conditions apply in the same way as \\curly V \\curly V . We define a corresponding expansion for u u \\begin{equation} \\begin{split} u^h &= \\sum_{A=1}^{n} d_A N_A + {\\curly g}N_{n+1} \\\\ {\\curly g}^h &= {\\curly g} N_{n+1} \\end{split} \\nonumber \\end{equation} \\begin{equation} \\begin{split} u^h &= \\sum_{A=1}^{n} d_A N_A + {\\curly g}N_{n+1} \\\\ {\\curly g}^h &= {\\curly g} N_{n+1} \\end{split} \\nonumber \\end{equation} which includes an extra basis function to allow the boundary condition ( u(1)={\\curly g} u(1)={\\curly g} ) to be satisfied. Substituting into (\\ref{eq:FEabst}) gives \\begin{equation} a\\left( \\sum_{A=1}^n c_A N_A , \\sum_{B=1}^n d_B N_B \\right) = \\left( \\sum_{A=1}^n c_A N_A , {\\curly f} \\right) + \\left[ \\sum_{A=1}^n c_A N_A(0) \\right] {\\curly h} -a\\left( \\sum_{A=1}^n c_A N_A , {\\curly g} N_{n+1} \\right) \\nonumber \\end{equation} \\begin{equation} a\\left( \\sum_{A=1}^n c_A N_A , \\sum_{B=1}^n d_B N_B \\right) = \\left( \\sum_{A=1}^n c_A N_A , {\\curly f} \\right) + \\left[ \\sum_{A=1}^n c_A N_A(0) \\right] {\\curly h} -a\\left( \\sum_{A=1}^n c_A N_A , {\\curly g} N_{n+1} \\right) \\nonumber \\end{equation} Our operators are symmetric and wholly linear, so the order of summation and application of the operator can be interchanged freely to give: \\begin{equation} 0 = \\sum_{A=1}^n c_A G_A \\nonumber \\end{equation} \\begin{equation} 0 = \\sum_{A=1}^n c_A G_A \\nonumber \\end{equation} where \\begin{equation} G_A = \\sum_{B=1}^n a(N_A,N_B)d_B - (N_A,{\\curly f}) - N_A(0){\\curly h} + a(N_A,A_{n+1}){\\curly g} \\nonumber \\end{equation} \\begin{equation} G_A = \\sum_{B=1}^n a(N_A,N_B)d_B - (N_A,{\\curly f}) - N_A(0){\\curly h} + a(N_A,A_{n+1}){\\curly g} \\nonumber \\end{equation} As we have done many times before, we appeal to the argument that the particular choice of variation must be totally arbitrary: this equation must hold no matter what w^h \\in {\\curly S}^h w^h \\in {\\curly S}^h we choose, and hence no matter what the combination of c_A c_A may be. This then means that G_A G_A must be identically zero independent of c_A c_A , i.e. \\begin{equation} \\sum_{B=1}^n a(N_A,N_B)d_B = (N_A,{\\curly f} ) - N_A(0){\\curly h} + a(N_A,A_{n+1}) {\\curly g} \\label{eq:gal2} \\end{equation} \\begin{equation} \\sum_{B=1}^n a(N_A,N_B)d_B = (N_A,{\\curly f} ) - N_A(0){\\curly h} + a(N_A,A_{n+1}) {\\curly g} \\label{eq:gal2} \\end{equation} As in the standard variational method, we have eliminated all references to the actual variation and left unknowns which only related to the physical variables (i.e. the coefficients of the expansion for u u ). If we simply write \\begin{equation} \\begin{split} K_{AB} &= a(N_A,N_B)\\ F_A &= (N_A,{\\curly f}) + N_A(0)h - a(N_A,N_n+1){\\curly g} \\end{split} \\label{eq:FEstdform} \\end{equation} then we have a matrix formulation immediately since (\\ref{eq:gal2}) now becomes \\begin{equation} \\sum_{B=1}^n K_{AB} d_B = F_A \\;\\;\\;\\;\\;\\; A=1,2,3,\\ldots \\nonumber \\end{equation} \\begin{equation} \\sum_{B=1}^n K_{AB} d_B = F_A \\;\\;\\;\\;\\;\\; A=1,2,3,\\ldots \\nonumber \\end{equation} or \\begin{equation} \\mathbf{K d} = \\mathbf{F} \\nonumber \\end{equation} \\begin{equation} \\mathbf{K d} = \\mathbf{F} \\nonumber \\end{equation} The matrix \\mathbf{K} \\mathbf{K} is known as the stiffness matrix - the association with the matrix of stiffnesses from our discrete set of springs being obvious.","title":"Galerkin Approximate Weak Form"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#generalization","text":"The same argument can be applied to higher dimensions, and to problems with different constitutive laws, and vector unknowns, however, the identification of the components of the stiffness matrix with the operator a(\\cdot,\\cdot) a(\\cdot,\\cdot) acting on every possible combination of the Galerkin basis functions still holds. When the unknowns are vectors, the basis functions are used in each respective direction which complicates the notation even more than before. Essentially, though it means that the entries to \\mathbf{K} \\mathbf{K} from each A A and B B are actually matrices which are n_{\\rm dim} \\times n_{\\rm dim} n_{\\rm dim} \\times n_{\\rm dim} . For example, in the constant viscosity Stokes' flow problem, we now have \\begin{equation} \\left. K _ {AB}\\right| _ {ij} = \\int _ \\Omega \\left( \\frac{\\partial N _ A}{\\partial x _ j} + \\frac{\\partial N _ A}{\\partial x _ i} \\right) \\eta \\left( \\frac{\\partial N _ B}{\\partial x _ j} + \\frac{\\partial N _ B}{\\partial x _ i} \\right) d \\Omega \\label{eq:festokes} \\end{equation} \\begin{equation} \\left. K _ {AB}\\right| _ {ij} = \\int _ \\Omega \\left( \\frac{\\partial N _ A}{\\partial x _ j} + \\frac{\\partial N _ A}{\\partial x _ i} \\right) \\eta \\left( \\frac{\\partial N _ B}{\\partial x _ j} + \\frac{\\partial N _ B}{\\partial x _ i} \\right) d \\Omega \\label{eq:festokes} \\end{equation}","title":"Generalization"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#discretization-shape-functions","text":"Although we have now nominally made our problem finite by expressing everything in terms of Galerkin basis functions, we have yet to decide what those functions ought to be. Whatever our choice, it should make the problem easy to compute numerically. {. width=\"75%\" id=\"domain-decomp\"} Splitting an arbitrary domain into patches is trivial in an integral formulation. Additionally, the irregular shape of the individual subdomains is easy to handle with standard changes of variables (Jacobians) The choice of basis functions is very broad but we narrow it down by tying it very closely to the way we choose to split up the domain of our problem. With an integral method, the domain decomposition is simple since \\begin{equation} \\int_\\Omega d\\Omega = \\int_{\\Omega_1} d {\\Omega_1} + \\int_{\\Omega_2} d {\\Omega_2} + \\int_{\\Omega_3} d {\\Omega_3} + \\ldots \\label{eq:decomp} \\end{equation} \\begin{equation} \\int_\\Omega d\\Omega = \\int_{\\Omega_1} d {\\Omega_1} + \\int_{\\Omega_2} d {\\Omega_2} + \\int_{\\Omega_3} d {\\Omega_3} + \\ldots \\label{eq:decomp} \\end{equation} which means that the decomposition shown above is as accurate as the representation of the boundary allows. The functions which interpolate the node points at the corners of subdomains are a suitable set for use as an approximate basis as we require for the Galerkin formulation. This is easy to see if we consider a one dimensional problem. {. width=\"75%\" id=\"discretisation-1d\"} Representing a piecewise linear function as a sum of pointy functions localized at the nodes In the one dimensional case, the choice of subdomains is limited to breaking up the line into a number of segments, though not necessarily of equal length. If the approximation to the continuum field, u u is made by linear interpolation to give u^h u^h , then u^h u^h can also be expressed as a sum on the local triangular functions N_A N_A \\begin{equation} u^h = \\sum_{A} u(x_A) N_A(x) \\nonumber \\end{equation} \\begin{equation} u^h = \\sum_{A} u(x_A) N_A(x) \\nonumber \\end{equation} This is an exact representation of the interpolation provided the N_A N_A takes the value one at node A A and zero at all other nodes, and varies in a linear manner in between. Note that the functions all take the same form apart from scaling. Also, because the basis functions are localized around a node and its neighbours, the direct interaction between a node and its immediate neighbours is non-zero but the interaction with more distant nodes is zero. This makes the stiffness matrix banded --- more importantly it is sparse and therefore the potentially enormous number of interactions (the square of the number of unknowns) is contained. This procedure can be extended to higher dimensions and to higher order interpolations as shown in the Figure below","title":"Discretization &amp; Shape Functions"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#naming-of-things-and-what-they-look-like","text":"The interpolation functions are known as shape functions. The subdomains are known as elements. The elements correspond to the individual springs of our discrete example. The shape functions are pure interpolation functions of the required order within the element and can be differentiated the appropriate number of times. This means that the order of interpolation must match the order of derivatives in the FE operator. Crossing the element boundaries, the shape functions have discontinuous derivatives (as do the piecewise interpolations). Continuous derivative functions are possible (e.g. splines) but add significant complexity. Shape functions in 2D and 3D can be formed from the product of 1D shape functions in each direction. {. width=\"75%\" id=\"shape-functions\"} Extension of simple shape function concept (a) to higher order functions and (b) to higher dimensions","title":"Naming of Things and What They Look Like"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#element-matrices","text":"The domain decomposition illustrated above produces a minature version of the matrix problem on the individual elements themselves. \\begin{equation} {k^e}_{ij} {d^e}_j = {f^e}_i \\nonumber \\end{equation} \\begin{equation} {k^e}_{ij} {d^e}_j = {f^e}_i \\nonumber \\end{equation} The interpretation of the local problem is similar to that of the individual springs in the discrete example we derived earlier. By assembling the individual under-constrained element problems into the full problem we obtain a soluble system with the correct number of boundary conditions etc. The local equilibrium problem is identical with that of the global problem. That is, we use the same variational form and the same operators to build the local matrices. In general, finite element methods are designed around the local element frame of reference and large problems assembled from smaller ones. The book-keeping involved is to track which degrees of freedom in the local problem are associated with which others in the global problem. Clearly the interaction coefficient between one degree of freedom and another is derived from the local equilibrium relationships of a number of elements because the shape function for a given nodal point is spread across a number of elements.","title":"Element Matrices"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#numerical-integration","text":"The real power of finite elements comes from its ability to handle remarkably distorted geometries with ease. (Obviously this only applies in two or more dimensions). This is possible because simple mappings can be derived to convert a distorted element into a regular one (square, cubic, regular-tetrahedral). This takes the form of a change of variables which can be done {\\em fully automatically} if we make minor restrictions on the choice of distortions we allow the elements to have. This restriction takes the form of ensuring that the mappings which transform the element shape can be described by the shape functions of the element. This means that only linear mappings can be applied to linear elements. If we wish to map a curved boundary exactly, then elements which have the appropriate order shape functions have to be used even if they are not needed for the representation of the differential operator. (e.g. quadratic elements for a circular boundary). This concept produces \"isoparametric\" elements. The mapping shown in the domain decomposition figure is achieved by a change of variables \\begin{equation} \\int \\int_{\\Omega^e} \\phi dx dy = \\int \\int_{\\rm square} \\phi {\\curly j} d \\xi d \\eta \\nonumber \\end{equation} \\begin{equation} \\int \\int_{\\Omega^e} \\phi dx dy = \\int \\int_{\\rm square} \\phi {\\curly j} d \\xi d \\eta \\nonumber \\end{equation} where {\\curly j} {\\curly j} is the jacobian of the transformation defined by \\begin{equation} {\\curly j} = {\\rm det} \\frac{\\partial \\mathbf{x}}{\\partial \\boldsymbol{\\xi}} = {\\rm det} \\left[ \\begin{array}{cc} \\frac{\\partial x_1}{\\partial \\xi_1} & \\frac{\\partial x_1}{\\partial \\xi_2} \\ \\frac{\\partial x_2}{\\partial \\xi_1} & \\frac{\\partial x_2}{\\partial \\xi_2} \\end{array} \\right] \\nonumber \\end{equation} In the isoparametric concept, the components of the jacobian can be written as, for example, \\begin{equation} \\frac{\\partial x_1}{\\partial \\xi_1} (\\xi_1,\\xi_2) = \\sum_{a=1}^{n_en} \\frac{\\partial N_a}{\\partial \\xi_1} {x_1}_a \\nonumber \\end{equation} Now we have the integral of a number of things defined over a regular, square domain (or cube etc). A number of ways to estimate these integrals is available. For example, in 1D the familiar trapezium rule can integrate linearly interpolated functions exactly. Given that our approximate method has already reduced the degree of polynomial within the element to something manageable, it is possible to find integration schemes which produce the {\\em exact} integral for our approximation --- in other words introducing no further error. One possibility which is commonly used is Gaussian quadrature. This is a textbook method which can be made exact for any order of interpolation function desired. For linear interpolation in 2D, the following rule applies (assuming that we have transformed to a square element) \\begin{equation} \\int\\limits_{-1}^{1} \\int\\limits_{-1}^{1} \\phi(\\xi,\\eta) d\\xi d\\eta \\ \\ \\ \\cong \\ \\ \\ \\sum_{l=1}^{n_{\\rm int}} \\phi( \\tilde{ \\xi_l}, \\tilde{ \\eta_l}) W_l \\nonumber \\end{equation} \\begin{equation} \\int\\limits_{-1}^{1} \\int\\limits_{-1}^{1} \\phi(\\xi,\\eta) d\\xi d\\eta \\ \\ \\ \\cong \\ \\ \\ \\sum_{l=1}^{n_{\\rm int}} \\phi( \\tilde{ \\xi_l}, \\tilde{ \\eta_l}) W_l \\nonumber \\end{equation} In which n_{\\rm int} n_{\\rm int} is the number of points in the quadrature rule with co-ordinates (\\tilde{ \\xi_l}, \\tilde{ \\eta_l}) (\\tilde{ \\xi_l}, \\tilde{ \\eta_l}) . Each point has a weight associated with it of W_l W_l . For the four point rule: $ l $ \\tilde{\\xi _ l} \\tilde{\\xi _ l} \\tilde{\\eta _ l} \\tilde{\\eta _ l} W _ l W _ l 1 $ -1/ \\surd{3} $ \\( -1/ \\sqrt{3} \\) 1 2 $ +1/ \\surd{3} $ \\( -1/ \\sqrt{3} \\) 1 3 $ -1/ \\surd{3} $ \\( +1/ \\sqrt{3} \\) 1 4 $ +1 /\\surd{3} $ \\( +1/ \\sqrt{3} \\) 1 The four-point rule in two dimensions is constructed by applying a two point, one dimensional rule to each of the coordinates in turn. The integrals along the edges of the elements which are required to construct the force vectors are therefore should be calculated using the one dimensional, two point rule which, along an edge of the bi-unit master element is l l \\tilde{\\xi _ l} \\tilde{\\xi _ l} W _ l W _ l 1 -1/\\sqrt\\{3\\} -1/\\sqrt\\{3\\} 1 2 +1/\\sqrt\\{3\\} +1/\\sqrt\\{3\\} 1 This naturally extends to three dimensional elements and their two dimensional boundaries.","title":"Numerical Integration"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#standard-form-for-everything","text":"Consider the heat 1D conduction problem one more time. We can rewrite the generalized Fourier law as \\begin{equation} q_i = - \\kappa_{ij} \\frac{\\partial u}{\\partial x_j} = -\\kappa_{ij} \\left( \\begin{array}{c} \\frac{\\partial}{\\partial x_1} \\\\ \\frac{\\partial }{\\partial x_2} \\end{array} \\right) u \\nonumber \\end{equation} \\begin{equation} q_i = - \\kappa_{ij} \\frac{\\partial u}{\\partial x_j} = -\\kappa_{ij} \\left( \\begin{array}{c} \\frac{\\partial}{\\partial x_1} \\\\ \\frac{\\partial }{\\partial x_2} \\end{array} \\right) u \\nonumber \\end{equation} If we express u u in terms of the shape functions \\begin{equation} u = \\sum_A d_A N_A (x) \\rightarrow q_i = -\\kappa _ {ij} \\sum_A \\left( \\begin{array}{c} {\\displaystyle \\frac{\\partial N _ A}{\\partial x _ 1}} \\\\ { \\displaystyle \\frac{\\partial N _ A}{\\partial x _ 2}} \\end{array} \\right) d _ A q_i = - \\sum_A \\kappa _ {ij} \\left. B _ A \\right| _ {j} d _ A \\nonumber \\end{equation} \\begin{equation} u = \\sum_A d_A N_A (x) \\rightarrow q_i = -\\kappa _ {ij} \\sum_A \\left( \\begin{array}{c} {\\displaystyle \\frac{\\partial N _ A}{\\partial x _ 1}} \\\\ { \\displaystyle \\frac{\\partial N _ A}{\\partial x _ 2}} \\end{array} \\right) d _ A q_i = - \\sum_A \\kappa _ {ij} \\left. B _ A \\right| _ {j} d _ A \\nonumber \\end{equation} This allows us to define a matrix \\mathbf{B _ A} \\mathbf{B _ A} which comes directly from the operators contained in the constitutive law. Compare this with equation ( \\ref{eq:FEstdform}) and, the more concrete example (\\ref{eq:festokes}) and we see that the stiffness matrix coefficients can be obtained from \\begin{equation} K_{AB} = a(N_A,N_B) = \\int_{\\Omega} \\mathbf{B}_A^T \\mathbf{D} \\mathbf{B}_B d\\Omega \\nonumber \\end{equation} \\begin{equation} K_{AB} = a(N_A,N_B) = \\int_{\\Omega} \\mathbf{B}_A^T \\mathbf{D} \\mathbf{B}_B d\\Omega \\nonumber \\end{equation} where \\mathbf{D} \\mathbf{D} is a matrix of material properties. This form can be used for all problems and may greatly simplify both programming and the automation of the development of the equations since now we simply need to be able to express the constitutive law in terms of the unknowns, build a material property matrix and plug this into standard machinery.","title":"Standard Form for Everything"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#constraints","text":"We generally want to solve problems where there is a constraint on the unknowns. For example we wish to find a flow solution which also satisfies mass conservation (not unreasonable !). To see how this constraint may be applied we go back to the variational method and regard the equation {\\bf Kd = f} {\\bf Kd = f} in the light of the functional \\begin{equation} {\\cal F} ({\\bf d}) = \\frac{ {\\bf d}^T{\\bf Kd}}{2} - {\\bf d}^T {\\bf f} \\nonumber \\end{equation} \\begin{equation} {\\cal F} ({\\bf d}) = \\frac{ {\\bf d}^T{\\bf Kd}}{2} - {\\bf d}^T {\\bf f} \\nonumber \\end{equation} The vector which minimizes {\\cal F} ({\\bf d}) {\\cal F} ({\\bf d}) is the solution to {\\bf Kd = f} {\\bf Kd = f} - this can be shown in the usual way. (Start with $ {\\cal F}({\\bf d + \\varepsilon c}) $, where \\varepsilon \\varepsilon is a free, real parameter and \\bf c \\bf c is arbitrary). Consider a single constraint, \\begin{equation} d_Q = {\\curly g} \\nonumber \\end{equation} \\begin{equation} d_Q = {\\curly g} \\nonumber \\end{equation} which corresponds to specifying a value for one of the velocities in the problem having the index Q Q in the global numbering system. This constraint should be written as a function of \\bf d \\bf d in the following way: \\begin{equation} 0 = {\\cal G}({\\bf d}) = {\\bf 1}_Q^T{\\bf d} - {\\curly g} \\nonumber \\end{equation} \\begin{equation} 0 = {\\cal G}({\\bf d}) = {\\bf 1}_Q^T{\\bf d} - {\\curly g} \\nonumber \\end{equation} \\begin{eqnarray} {\\bf 1}^T_Q = \\left\\langle 0 \\ldots 0 \\right. & 1 & \\left. 0 \\ldots 0 \\right\\rangle \\\\ \\nonumber & \\uparrow & \\! \\\\ \\nonumber & \\text{ Q'th column} & \\nonumber \\end{eqnarray} \\begin{eqnarray} {\\bf 1}^T_Q = \\left\\langle 0 \\ldots 0 \\right. & 1 & \\left. 0 \\ldots 0 \\right\\rangle \\\\ \\nonumber & \\uparrow & \\! \\\\ \\nonumber & \\text{ Q'th column} & \\nonumber \\end{eqnarray} Then finding the stationary value for the following function is equivalent to solving the constrained problem: \\begin{equation} {\\cal H}({\\bf d},m) = {\\cal F}({\\bf d}) + m{\\cal G}({\\bf d}) \\nonumber \\end{equation} \\begin{equation} {\\cal H}({\\bf d},m) = {\\cal F}({\\bf d}) + m{\\cal G}({\\bf d}) \\nonumber \\end{equation} The condition that \\bf d \\bf d renders \\cal H \\cal H stationary is \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} {\\cal H}({\\bf d}+\\varepsilon{\\bf c},m + \\varepsilon l) \\right| _ {\\varepsilon=0} \\;\\;\\; \\forall {\\bf c},l \\nonumber \\end{equation} \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} {\\cal H}({\\bf d}+\\varepsilon{\\bf c},m + \\varepsilon l) \\right| _ {\\varepsilon=0} \\;\\;\\; \\forall {\\bf c},l \\nonumber \\end{equation} Substituting for \\cal H \\cal H gives: \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} \\left[ {\\cal F}({\\bf d}+\\varepsilon{\\bf c} )+ (m+\\varepsilon l){\\cal G}({\\bf d}+\\varepsilon{\\bf c} )\\right] \\right| _ {\\varepsilon=0} \\nonumber \\end{equation} \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} \\left[ {\\cal F}({\\bf d}+\\varepsilon{\\bf c} )+ (m+\\varepsilon l){\\cal G}({\\bf d}+\\varepsilon{\\bf c} )\\right] \\right| _ {\\varepsilon=0} \\nonumber \\end{equation} \\begin{equation} 0= {\\bf c}^T({\\bf Kd -F) }+l {\\cal G}({\\bf d}) + m{\\bf 1}_Q^T {\\bf c} \\nonumber \\end{equation} \\begin{equation} 0= {\\bf c}^T({\\bf Kd -F) }+l {\\cal G}({\\bf d}) + m{\\bf 1}_Q^T {\\bf c} \\nonumber \\end{equation} \\begin{equation} 0= {\\bf c}^T({\\bf Kd} + m{\\bf 1} _ Q {\\bf -f}) + l ({\\bf 1}_Q^T {\\bf d} -\\mbox{\\curly g}) \\nonumber \\end{equation} \\begin{equation} 0= {\\bf c}^T({\\bf Kd} + m{\\bf 1} _ Q {\\bf -f}) + l ({\\bf 1}_Q^T {\\bf d} -\\mbox{\\curly g}) \\nonumber \\end{equation} Since \\bf c \\bf c and l l are strictly arbitrary, \\begin{eqnarray} {\\bf Kd }+m{\\bf 1} _ Q &=& {\\bf f}\\\\ \\nonumber {\\bf 1} _ Q^T {\\bf d} & = & {\\curly g} \\nonumber \\end{eqnarray} \\begin{eqnarray} {\\bf Kd }+m{\\bf 1} _ Q &=& {\\bf f}\\\\ \\nonumber {\\bf 1} _ Q^T {\\bf d} & = & {\\curly g} \\nonumber \\end{eqnarray} Equivalently \\begin{equation} \\left[ \\begin{array}{cc} {\\bf K} & {\\bf 1} _ Q \\\\\\\\ {\\bf 1} _ Q^T & 0 \\end{array} \\right] \\left\\\\{ \\begin{array}{c} {\\bf d} \\\\\\\\ m \\end{array} \\right\\\\} = \\left\\\\{ \\begin{array}{c} {\\bf f} \\\\\\\\ {\\curly g} \\end{array} \\right\\\\} \\end{equation} \\begin{equation} \\left[ \\begin{array}{cc} {\\bf K} & {\\bf 1} _ Q \\\\\\\\ {\\bf 1} _ Q^T & 0 \\end{array} \\right] \\left\\\\{ \\begin{array}{c} {\\bf d} \\\\\\\\ m \\end{array} \\right\\\\} = \\left\\\\{ \\begin{array}{c} {\\bf f} \\\\\\\\ {\\curly g} \\end{array} \\right\\\\} \\end{equation} which establishes the pattern expected for adding constraints to the physical problem: augmentation of all the matrices with some forces ( \\bf m \\bf m ) as well as velocities ( \\bf d \\bf d ) unknown. The coefficient matrix remains symmetric but is no longer positive definite. An alternative way to enforce constraints and one which proves slightly simpler to implement, is to make an approximation to the Lagrange-multiplier as follows: \\begin{equation} m \\cong k {\\cal G}({\\bf d}) \\nonumber \\end{equation} \\begin{equation} m \\cong k {\\cal G}({\\bf d}) \\nonumber \\end{equation} in which k k is a large positive number. Form the following functional, \\begin{equation} {\\cal J}({\\bf d}) = {\\cal F}({\\bf d}) + \\frac{k}{2} \\left[{\\cal G} ({\\bf d})\\right]^2 \\nonumber \\end{equation} \\begin{equation} {\\cal J}({\\bf d}) = {\\cal F}({\\bf d}) + \\frac{k}{2} \\left[{\\cal G} ({\\bf d})\\right]^2 \\nonumber \\end{equation} and consider the extremal values for \\cal J \\cal J which are defined by the following expression \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} \\left( {\\cal F}({\\bf d}+\\varepsilon {\\bf c} )+ \\frac{k}{2} \\left[ {\\cal G} ({\\bf d} + \\varepsilon{\\bf c}) \\right]^2 \\right) \\right| _ {\\varepsilon = 0} \\nonumber \\end{equation} \\begin{equation} 0 = \\left. \\frac{d}{d\\varepsilon} \\left( {\\cal F}({\\bf d}+\\varepsilon {\\bf c} )+ \\frac{k}{2} \\left[ {\\cal G} ({\\bf d} + \\varepsilon{\\bf c}) \\right]^2 \\right) \\right| _ {\\varepsilon = 0} \\nonumber \\end{equation} \\begin{equation} {\\bf c}^T({\\bf Kd - f}) + k {\\bf G}({\\bf d}) {\\bf 1}_Q^T {\\bf c} \\nonumber \\end{equation} \\begin{equation} {\\bf c}^T({\\bf Kd - f}) + k {\\bf G}({\\bf d}) {\\bf 1}_Q^T {\\bf c} \\nonumber \\end{equation} Substituting for \\cal G \\cal G gives \\begin{equation} {\\bf c}^T \\left[ ({\\bf K} + k{\\bf 1}_Q{\\bf 1}_Q^T){\\bf d} - ({\\bf f} + k {\\curly g} {\\bf 1}_Q) \\right] = 0 \\nonumber \\end{equation} \\begin{equation} {\\bf c}^T \\left[ ({\\bf K} + k{\\bf 1}_Q{\\bf 1}_Q^T){\\bf d} - ({\\bf f} + k {\\curly g} {\\bf 1}_Q) \\right] = 0 \\nonumber \\end{equation} Since \\bf c \\bf c is arbitrary, the following matrix equation is implied \\begin{equation} ({\\bf K} + k{\\bf 1}_Q{\\bf 1}_Q^T){\\bf d} = ({\\bf f} + k {\\curly g} {\\bf 1}_Q) \\nonumber \\end{equation} \\begin{equation} ({\\bf K} + k{\\bf 1}_Q{\\bf 1}_Q^T){\\bf d} = ({\\bf f} + k {\\curly g} {\\bf 1}_Q) \\nonumber \\end{equation} Now the dimension of the problem is not changed; the constraints are introduced by addition of another matrix equation. In the limit k \\rightarrow \\infty k \\rightarrow \\infty , d_Q \\rightarrow {\\curly g} d_Q \\rightarrow {\\curly g} and the constraint is applied exactly. The constraint of incompressibility is applied as the limiting case of slight compressibility. The large constant is known as the penalty parameter and gives its name to the method; its value is chosen according to the accuracy of the machine used to compute the solution to the problem. A suitable value is somewhere between 10^7 10^7 and 10^9 10^9 , any smaller and the fluid volume may not be conserved during hydrostatic loading. The penalty method is computationally very simple, but can be ill conditioned. This becomes a serious issue when the matrix equation is not solved by direct methods. In the latter case, it is preferable to introduce additional variables. These take the form of pressures which, in the slightly compressible flow case can always be eliminated in favour of the constraint, and then recovered as \\begin{equation} p = - \\lambda \\nabla . {\\bf u} \\nonumber \\end{equation} \\begin{equation} p = - \\lambda \\nabla . {\\bf u} \\nonumber \\end{equation} The augmented matrix equation then becomes \\begin{equation} \\left( \\begin{array}{cc} \\mathbf{K} & \\mathbf{G} \\\\\\\\ \\mathbf{G}^T & 0 \\end{array} \\right) \\left( \\begin{array}{c} \\mathbf{u} \\\\\\\\ \\mathbf{p} \\end{array} \\right)= \\left( \\begin{array}{c} \\mathbf{f} \\\\\\\\ \\mathbf{0} \\end{array} \\right) \\nonumber \\end{equation} \\begin{equation} \\left( \\begin{array}{cc} \\mathbf{K} & \\mathbf{G} \\\\\\\\ \\mathbf{G}^T & 0 \\end{array} \\right) \\left( \\begin{array}{c} \\mathbf{u} \\\\\\\\ \\mathbf{p} \\end{array} \\right)= \\left( \\begin{array}{c} \\mathbf{f} \\\\\\\\ \\mathbf{0} \\end{array} \\right) \\nonumber \\end{equation} The components of the \\mathbf{G} \\mathbf{G} matrix are, at the element level found from \\begin{equation} \\left. g^e_{a\\tilde{a}} \\right| _ {i} = -\\int_{\\Omega^e} {\\rm div}(N_a) (\\tilde{N}) _ {\\tilde{a}} d\\Omega \\nonumber \\end{equation} \\begin{equation} \\left. g^e_{a\\tilde{a}} \\right| _ {i} = -\\int_{\\Omega^e} {\\rm div}(N_a) (\\tilde{N}) _ {\\tilde{a}} d\\Omega \\nonumber \\end{equation} The tilde indicates a pressure degree of freedom / shape function as constrasted with the standard velocity shape fucntions. The independent pressures require their own space of trial functions which can be of lower order than the velocity since the derivatives in the equations are also lower. In fact, it is necessary to use lower order functions for the pressure otherwise the problem is overconstrained and no non-trivial solution can be found. This is more of a difficulty when the penalty formulation is used since there are no additional variables and hence no associated trial functions. Instead a trick is used - the constraint terms are integrated using an order lower integration scheme than that required for exact solutions. The result turns out to be equivalent (usually) to the introduction of additional variables and lower order shape functions but the proof of this is another tricky matter.","title":"Constraints"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-5.html#stresses","text":"Note Derivatives of the shape functions are commonly not defined at the node points and consequently it is not possible to compute stresses etc at the nodal points. They are defined on the interiors of the elements and must be extrapolated to the nodes using some form of best fit procedure.","title":"Stresses"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-6.html","text":"\\[ \\require{color} \\newcommand{\\dGamma}{\\mathbf{d}\\boldsymbol{\\Gamma}} \\newcommand{\\erfc}{\\mbox{\\rm erfc}} \\newcommand{\\curly}{\\sf } \\newcommand{\\Red}[1]{\\textcolor[rgb]{0.7,0.0,0.0}{#1}} \\newcommand{\\Green}[1]{\\textcolor[rgb]{0.0,0.7,0.0}{ #1}} \\newcommand{\\Blue}[1]{\\textcolor[rgb]{0.0,0.0,0.7}{ #1}} \\newcommand{\\Emerald}[1]{\\textcolor[rgb]{0.0,0.7,0.3}{ #1}} \\newcommand{\\curly} \\] Solving the matrix problem Once the problem has been rendered into a matrix form, the finite element part is over ! The rest is matrix algebra. There are a number of standard techniques for solving such systems which we briefly outline in the context of the Earth dynamics problem. In most dynamic systems, the FEM can be formulated in a time-explicit manner (cf discrete element methods) that is robust and simple to implement, or using implicit methods which are more elaborate and are often more temperamental but which are capable of covering much larger time increments at each step. In our case, however, the fact that inertia is negligible, leaves the equation of motion independent of time and it can only be solved implicitly. The original implicit solver in FEM was to build the matrix equation and solve it directly using an method such as Crout elimination. This can be done relatively efficiently by exploiting the fact that the finite element matrices are quite tightly banded. Unfortunately, direct solution methods are limited to \"small\" problems since the solution-time scales very rapidly with the number of unknowns (in the worst case, as N^3 N^3 where N N is the number of unknowns, and even in the best case as N^2 N^2 ). Iterative methods can achieve much better performance than this once the problems start to become larger. For example, preconditioned conjugate gradient methods can obtain a solution to a given accuracy in a time proportional to N\\log N N\\log N . However, preconditioning can be highly time consuming and may need careful tailoring for different problems as the iterations may not converge (compare this to our need to carefully choose how to discretise problems in our finite difference examples earlier) The optimal method, in theory, for the Stokes problem is multigrid which, when properly formulated, can find a solution in a time proportional to N N . An additional problem arises with direct methods. The number of unknowns in the vector \\mathbf{d} \\mathbf{d} is slightly less than n_{\\rm dim} n_{\\rm dim} times the number of nodal points once the prescribed boundary velocities have been included. For a well-resolved problem the stiffness matrix \\mathbf{K} \\mathbf{K} is often too large to be stored in full, even accounting for its sparsity. Some iterative methods can operate with only the element stiffness matrices which can be built and used on the fly. Although this requires significantly more computations, it can make some large problems soluble where otherwise memory limitations would block solution. It is also possible that computation can be more efficient than retrieving carefully-packed matrix coefficents and, in this case, element-by-element methods can be competitive for speed in their own right. Some simple iterative schemes for positive definite systems are described next. Far more that this exist and many are more efficient but this usually depends on the actual application area. Jacobi relaxation One of the simplest iterative methods of residual reduction is the Jacobi iteration. Given an approximate solution, \\mathbf{d}^{(0)} \\mathbf{d}^{(0)} , to \\mathbf{Kd}=\\mathbf{f} \\mathbf{Kd}=\\mathbf{f} , an improved solution, $ \\mathbf{d}^{(1)}$ is found by \\begin{equation} d _ i^{(1)} = \\left( f _ i - \\sum^{n} _ {j=1;j\\not=i} k _ {ij}d_j^{(0)} \\right) / k _ {ii} \\nonumber \\end{equation} \\begin{equation} d _ i^{(1)} = \\left( f _ i - \\sum^{n} _ {j=1;j\\not=i} k _ {ij}d_j^{(0)} \\right) / k _ {ii} \\nonumber \\end{equation} It is clear that each iteration cycle requires only one matrix-vector multiplication. However, the convergence rate for this algorithm is usually poor and a large number of cycles is required. Gauss-Seidel relaxation A trivial modification to the Jacobi iteration is to use updated information on the \\mathbf{d} \\mathbf{d} componenents immediately they become available. This results in the Gauss-Seidel iteration which is considerably more efficient than Jacobi. It can, however, be harder to code efficiently because the independence of each degree of freedom during one iteration disappears. Conjugate Gradients A more sophisticated iterative procedure for obtaining solutions to linear systems is the conjugate gradient method. This is a development of the method of steepest descent with better convergence properties. The 'search directions' along which the solution is improved are denoted by \\mathbf{s} \\mathbf{s} and do not coincide with the residual vectors. In the following algorithm an inner product is denoted by (\\cdot,\\cdot) (\\cdot,\\cdot) . k=0; \\mathbf{u}_0 = {\\bf 0}; \\mathbf{r}_0 = \\mathbf{f} k=0; \\mathbf{u}_0 = {\\bf 0}; \\mathbf{r}_0 = \\mathbf{f} while (\\mathbf{r}_k \\not= {\\bf 0}) (\\mathbf{r}_k \\not= {\\bf 0}) \u2003 k = k + 1 k = k + 1 \u2003 if (k=1) (k=1) \u2003 \u2003 \\mathbf{s}_1 = \\mathbf{r}_0 \\mathbf{s}_1 = \\mathbf{r}_0 \u2003 else \u2003 \u2003 \\beta = (\\mathbf{r} _ {k-1},\\mathbf{r} _ {k-1})/(\\mathbf{r} _ {k-2},\\mathbf{r} _ {k-2}) \\beta = (\\mathbf{r} _ {k-1},\\mathbf{r} _ {k-1})/(\\mathbf{r} _ {k-2},\\mathbf{r} _ {k-2}) \u2003 \u2003 \\mathbf{s} _ k = \\mathbf{r} _ {k-1} + \\beta \\mathbf{s} _ {k-1} \\mathbf{s} _ k = \\mathbf{r} _ {k-1} + \\beta \\mathbf{s} _ {k-1} \u2003 end \u2003 \\alpha = (\\mathbf{r} _ {k},\\mathbf{r} _ {k})/(\\mathbf{s} _ {k},{\\bf A s} _ {k}) \\alpha = (\\mathbf{r} _ {k},\\mathbf{r} _ {k})/(\\mathbf{s} _ {k},{\\bf A s} _ {k}) \u2003 \\mathbf{u} _ k = \\mathbf{u} _ {k-1} + \\alpha \\mathbf{s} _ {k} \\mathbf{u} _ k = \\mathbf{u} _ {k-1} + \\alpha \\mathbf{s} _ {k} \u2003 \\mathbf{r} _ k = \\mathbf{r} _ {k-1} - \\alpha {\\bf As} _ {k} \\mathbf{r} _ k = \\mathbf{r} _ {k-1} - \\alpha {\\bf As} _ {k} end \\mathbf{u} = \\mathbf{u} _ k \\mathbf{u} = \\mathbf{u} _ k In this case the use of residual vectors from previous iteration steps is necessary to ensure that there is only one matrix-vector multiplication. The convergence of the conjugate gradient algorithm is fastest when \\mathbf{K} \\mathbf{K} is close to the identity matrix. For a general matrix the reduction of the residual may be slow through the first few iterations then picks up speed as the procedure progresses. This property makes this iterative loop inefficient for relatively small numbers of iterations. Note that the reduction of the residual is a measure of \"improvement\" of the solution which is internal to the iteration. Although the magnitude of the residual is usually reduced monotonically, the true error, when it can be calculated, may increase as well as decrease. In order to improve the rate of convergence it is necessary to reformulate the problem such that the solution to a nearly diagonal system is sought. This procedure is known as preconditioning. The system \\mathbf{Kd} = \\mathbf{f} \\mathbf{Kd} = \\mathbf{f} is transformed to a case which can be solved more rapidly by the conjugate gradient algorithm: \\begin{equation} \\tilde{\\mathbf{K}}\\tilde{\\mathbf{u}} = \\tilde{\\mathbf{f}} \\nonumber \\end{equation} \\begin{equation} \\tilde{\\mathbf{K}}\\tilde{\\mathbf{u}} = \\tilde{\\mathbf{f}} \\nonumber \\end{equation} where \\begin{align*} \\tilde{\\mathbf{K}} &= {\\bf C}^{-1}\\mathbf{K} {\\bf C}^{-1} \\\\ \\tilde{\\mathbf{u}} &= {\\bf Cu} \\\\ \\tilde{\\mathbf{f}} &= {\\bf C}^{-1} {\\bf b} \\end{align*} The preconditioning matrix \\bf M \\bf M is defined by {\\bf M} = {\\bf C}^2 {\\bf M} = {\\bf C}^2 , and the preconditioned conjugate gradient algorithm is then: $ k = 0; \\mathbf{u}_0 = {\\bf 0}; \\mathbf{r}_0 = \\mathbf{f}$ while (\\mathbf{r}_k \\not= {\\bf 0}) (\\mathbf{r}_k \\not= {\\bf 0}) \u2003 solve {\\bf M z}_k = \\mathbf{r}_k {\\bf M z}_k = \\mathbf{r}_k \u2003 $ k = k + 1 $ \u2003 if (k=1) (k=1) \u2003 \u2003 $ \\mathbf{s}_1 = {\\bf z}_0$ \u2003 else \u2003 \u2003 \\beta = (\\mathbf{r} _ {k-1},{\\bf z} _ {k-1})/(\\mathbf{r} _ {k-2},{\\bf z} _ {k-2}) \\beta = (\\mathbf{r} _ {k-1},{\\bf z} _ {k-1})/(\\mathbf{r} _ {k-2},{\\bf z} _ {k-2}) \u2003 \u2003 \\mathbf{s} _ k = {\\bf z} _ {k-1} + \\beta \\mathbf{s} _ {k-1} \\mathbf{s} _ k = {\\bf z} _ {k-1} + \\beta \\mathbf{s} _ {k-1} \u2003 end \u2003 \\alpha = (\\mathbf{r} _ {k},{\\bf z} _ {k})/(\\mathbf{s} _ {k},{\\bf A s} _ {k}) \\alpha = (\\mathbf{r} _ {k},{\\bf z} _ {k})/(\\mathbf{s} _ {k},{\\bf A s} _ {k}) \u2003 \\mathbf{d} _ k = \\mathbf{d} _ {k-1} + \\alpha \\mathbf{s} _ {k} \\mathbf{d} _ k = \\mathbf{d} _ {k-1} + \\alpha \\mathbf{s} _ {k} \u2003 \\mathbf{r} _ k = \\mathbf{r} _ {k-1} - \\alpha \\mathbf{Ks} _ {k} \\mathbf{r} _ k = \\mathbf{r} _ {k-1} - \\alpha \\mathbf{Ks} _ {k} end \\mathbf{d} = \\mathbf{d}_k \\mathbf{d} = \\mathbf{d}_k The choice of \\bf M \\bf M has a dramatic effect on the rate of convergence of the method. Not only must the preconditioner improve the convergence properties of the system, but the solution to {\\bf Mz} = \\mathbf{r} {\\bf Mz} = \\mathbf{r} must be inexpensive. Clearly if {\\bf M} = \\mathbf{K} {\\bf M} = \\mathbf{K} then exact convergence is obtained in one iteration but the solution of the preconditioning step becomes impossible. The simplest preconditioner, and the one employed here, is to set {\\bf M} = \\mathrm{diag}(\\mathbf{K}) {\\bf M} = \\mathrm{diag}(\\mathbf{K}) . This is not only simple to calculate (and invert) but requires very little storage. For simple systems the reduction of the residual is noticeably improved during the initial iterations. However, it is not clear how well this simple preconditioner will work when strong viscosity contrasts are present and off-diagonal terms in \\mathbf{K} \\mathbf{K} may be large. Multigrid {. width=\"50%\" id=\"multigrid-diagram\"} Multigrid: (a) Error reduction during simple two level V cycle, (b) Shape functions on different grid scales, (c) V cycle on four grids, (d) W cycle on four grids, (e) Schematic of Full Multigrid V cycles The multigrid method works by formulating the finite element problem on a number of different scales - usually a set of grids which are nested one within the other sharing common nodes. The solution progresses on all of the grids at the same time with each grid eliminating errors at a different scale. The effect is to propagate information very rapidly between different nodes in the grid which would otherwise be prevented by the local support of the element shape functions. In fact, by a single traverse from fine to coarse grid and back, all nodes in the mesh can be directly connected to every other - allowing nodes which are physically coupled but remote in the mesh to communicate directly during each iteration cycle. The multigrid effect relies upon using an iterative solver on each of the grid resolutions which acts like a smoother on the residual error at the characteristic scale of that particular grid. People commonly use Gauss-Seidel iteration because it has exactly this property. On the coarsest grid it is possible to use a direct solver because the number of elements is usually very small. For an elliptic operator such as the Laplacian of the Stokes' problem encountered in the algorithm above the discretized problem is now written \\begin{equation*} {\\bf K}_h {\\bf d}_h = {\\bf f}_h \\end{equation*} where the h h subscript indicates that the problem has been discretized to a mesh of fineness h h . As before an initial estimate of the velocity can be improved by determining the solution to \\begin{equation*} {\\bf K}_h \\delta {\\bf d}_h = {\\bf r}_h \\end{equation*} where {\\bf r}_h {\\bf r}_h is the residual on this mesh, and \\delta {\\bf d} \\delta {\\bf d} is a correction to \\bf d \\bf d which reduces \\bf r \\bf r . In the iterative methods described above, the initial approximation and the correction are found by solving a simplified version of the problem at the same gridpoints in such a way that computation time is reduced dramatically. However, another approach to the problem is to obtain an approximate solution by solving the problem on a more coarse grid. The reduction of the number of degrees of freedom also leads to a more manageable problem which can be solved fast. The correction term is therefore: \\begin{equation*} {\\bf K}_H \\delta{\\bf d}_H = {\\bf r}_H \\end{equation*} where H H indicates a coarser level of discretization. The residual on the coarser mesh is determined by the use of a projection (restriction) operator: \\begin{equation*} {\\bf r}_H = {\\bf P}_H^h {\\bf r}_h \\end{equation*} and the approximate solution is then interpolated from the coarse to fine grid using an interpolation operator: \\begin{equation*} \\delta{\\bf d}_h = {\\bf I}_H^h \\delta{\\bf d}_H \\end{equation*} The power of the algorithm is in a recursive application. The coarse grid correction is also calculated through the use of a still-coarser grid and so on, until the problem is so small that an exact solution can be obtained very rapidly. One very simple, but instructive, algorithm for hierarchical residual reduction is the sawtooth cycle (the same logical layout as the multigrid V cycle ). 1. Obtain approximate solution, ${ \\ bf d } _h$ at highest level $h $ 2. Calculate residual: ${ \\ bf r } _h = {\\bf f}_h - {\\bf K}_h {\\bf d}_h$ 3. Project residual by N levels to level $h -N$ 4. &amp; emsp; ${ \\ bf r } _ {h-i} = {\\bf P} _ {h-i}^{h-i+1} {\\bf r} _ {h-i+1}$ 5. Solve exactly: $\\delta {\\bf u} _ {h-N} = {\\bf A} _ {h-N} {\\bf r} _ {h-N}$ 6. Interpolation steps: 7. &amp; emsp; ${ \\ bf r } _ {h-i+1} += {\\bf I} _ {h-i}^{h-i+1} {\\bf K} _ {h-i} \\delta {\\bf d} _ {h-i}$ 8. &amp; emsp; Improve $\\delta {\\bf d} _ {h-i+1}$ 9. ${ \\ bf d } _ h += \\delta {\\bf d} _ h$ The step in which the velocity correction is \"improved\" is an iterative method for reducing the residual at the current level such as those described above. Although many methods of residual reduction are available, the class of methods which often work best with the multigrid approach are relaxation iterations which are also effective smoothing operators. At each level the smoothing operators reduce the residual most strongly on the scale of the discretization - the hierarchical nesting of different mesh sizes allows the residual to be reduced at each scale very efficiently. (see Parsons and Hall). The Jacobi relaxation above is a suitable algorithm for multigrid enhancement but still converges too slowly to build into an efficient code. Preconditioned conjugate gradient methods can better reduce the residual for the same number of operations but may not possess the smoothing properties which benefit the multigrid approach. The local inverse method appears to have both the smoothing and rapid convergence properties for the Stokes' problem which are required for effective multigridding. The projection and interpolation operators have to be chosen fairly carefully to avoid poor approximations to the problem at the coarse levels and ineffectual corrections propogated to the fine levels. The interpolation operator is defined naturally from the shape functions at the coarse levels. The projection operator is then defined to complement this choice (the operators should be adjoint). The sawtooth cycle given in this section is the simplest multigrid algorithm. Developments include improving the residual at each level of the projection , known as a v-cycle , and cycles in which the residual is interpolated only part way through the hierarchy before being reprojected and subjected to another set of improvements (a w-cycle ). The Full Multigrid Algorithm (see Brandt) introduces a further level of complexity. Instead of simply casting the problem at a single level and projecting/improving the residual on a number of grids, the whole problem is defined for all the grids. In this way the initial fine-grid approximation is obtained by interpolating from the solution to the coarsest grid problem. The solution at each level is still obtained by projecting to the finest level and reducing the residual at each projection step. The result is some sort of \"Loch Ness Monster\" cycle. One of the major problems in multigrid is knowing how best to represent material properties at different levels of grid coarseness in order to obtain the optimal convergence rate. This has a simple solution in the particle in cell finite element methods which we will discuss later. \\begin{equation*} {\\bf Gd} + {\\bf G K}^{-1}{\\bf G}^T {\\bf p} = {\\bf G K}^{-1}{\\bf f} \\end{equation*} \\begin{equation*} {\\bf Gd} + {\\bf G K}^{-1}{\\bf G}^T {\\bf p} = {\\bf G K}^{-1}{\\bf f} \\end{equation*} where the first term is zero due to the incompressibility condition. Then the equation system is in the form \\hat{\\bf K}{\\bf p} = \\hat{\\bf f} \\hat{\\bf K}{\\bf p} = \\hat{\\bf f} , where \\hat{\\bf K} \\hat{\\bf K} is now positive definite, and so a conjugate gradient method can be used: 1. $k = 0; {\\bf p}_0 = {\\bf 0}; {\\bf r}_0 = \\hat{\\bf f}$ 2. while $({\\bf r}_k \\not= {\\bf 0})$ 3. &amp; emsp; $ k = k + 1 $ 4. &amp; emsp; if $(k=1)$ 5. &amp; emsp; &amp; emsp; ${ \\ bf s } _1 = {\\bf r}_0$ 6. &amp; emsp; else 7. &amp; emsp; &amp; emsp; $\\beta = ({\\bf r} _ {k-1},{\\bf r} _ {k-1})/({\\bf r} _ {k-2},{\\bf r} _ {k-2})$ 8. &amp; emsp; &amp; emsp; ${ \\ bf s } _ k = {\\bf r} _ {k-1} + \\beta {\\bf s} _ {k-1}$ 9. &amp; emsp; end 10. &amp; emsp; ${ \\ bf w } = \\hat{\\bf K} {\\bf s}$ 11. &amp; emsp; $\\alpha = ({\\bf r} _ {k},{\\bf r} _ {k})/({\\bf s}_{k},{\\bf w})$ 12. &amp; emsp; ${ \\ bf p } _ k = {\\bf p} _ {k-1} + \\alpha {\\bf s} _ {k}$ 13. &amp; emsp; ${ \\ bf r } _ k = {\\bf r} _ {k-1} - \\alpha {\\bf w}$ 14. end 15. ${ \\ bf d } = {\\bf d}_k$ However, it is not possible to obtain \\hat{\\bf K} \\hat{\\bf K} or \\hat{\\bf f} \\hat{\\bf f} directly because the form of {\\bf K}^{-1} {\\bf K}^{-1} is not known - the reason for the use of an iterative method in the first place. In order to obtain the initial residual, therefore, perform the following operation, k = 0 k = 0 ; {\\bf p}_0 = {\\bf 0} {\\bf p}_0 = {\\bf 0} ; \\cancel{ {\\bf r}_0 = \\hat{\\bf f} } \\cancel{ {\\bf r}_0 = \\hat{\\bf f} } (1.a) \u2003 {\\bf r}_0 = {\\bf G K}^{-1} {\\bf f} {\\bf r}_0 = {\\bf G K}^{-1} {\\bf f} : (1.b) \u2003 solve: {\\bf K d} = {\\bf f} {\\bf K d} = {\\bf f} for \\bf d \\bf d (1.c) \u2003 set: {\\bf r}_0 = {\\bf G d} = {\\bf G K}^{-1} {\\bf f} {\\bf r}_0 = {\\bf G d} = {\\bf G K}^{-1} {\\bf f} and to find the \\bf w \\bf w vector: $ \\cancel{ {\\bf w} = \\hat{\\bf K} {\\bf s} }$ (10.a) \u2003 {\\bf w} = \\hat{\\bf G K}^{-1}{\\bf G}^T {\\bf s} {\\bf w} = \\hat{\\bf G K}^{-1}{\\bf G}^T {\\bf s} : (10.b) \u2003 solve {\\bf K d} = {\\bf G}^T {\\bf s} {\\bf K d} = {\\bf G}^T {\\bf s} for \\bf d \\bf d (10.c) \u2003 set {\\bf w} = {\\bf K d} = {\\bf G K}^{-1}{\\bf G}^T {\\bf s} {\\bf w} = {\\bf K d} = {\\bf G K}^{-1}{\\bf G}^T {\\bf s} Time We have concentrated on the solution to equilibrium equations without really touching on the way time is evolved in elasticity and slow flow where the equations themselves are independent of time. In the mantle dynamics problem, the energy equation contains time derivatives which the equation of motion lacks so it is unsurprising that these equations require entirely different computational methods. We almost always want to know the time-history of our simulations, so we generally solve this equation explicitly or semi-implicitly in time. Variational methods are not generally applicable because the energy equation with its strong advection term does not follow an extremum of some functional. In this case even the weak forms of the equation are quite similar to finite difference equations.","title":"NumericalMethodsPrimer 6"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-6.html#solving-the-matrix-problem","text":"Once the problem has been rendered into a matrix form, the finite element part is over ! The rest is matrix algebra. There are a number of standard techniques for solving such systems which we briefly outline in the context of the Earth dynamics problem. In most dynamic systems, the FEM can be formulated in a time-explicit manner (cf discrete element methods) that is robust and simple to implement, or using implicit methods which are more elaborate and are often more temperamental but which are capable of covering much larger time increments at each step. In our case, however, the fact that inertia is negligible, leaves the equation of motion independent of time and it can only be solved implicitly. The original implicit solver in FEM was to build the matrix equation and solve it directly using an method such as Crout elimination. This can be done relatively efficiently by exploiting the fact that the finite element matrices are quite tightly banded. Unfortunately, direct solution methods are limited to \"small\" problems since the solution-time scales very rapidly with the number of unknowns (in the worst case, as N^3 N^3 where N N is the number of unknowns, and even in the best case as N^2 N^2 ). Iterative methods can achieve much better performance than this once the problems start to become larger. For example, preconditioned conjugate gradient methods can obtain a solution to a given accuracy in a time proportional to N\\log N N\\log N . However, preconditioning can be highly time consuming and may need careful tailoring for different problems as the iterations may not converge (compare this to our need to carefully choose how to discretise problems in our finite difference examples earlier) The optimal method, in theory, for the Stokes problem is multigrid which, when properly formulated, can find a solution in a time proportional to N N . An additional problem arises with direct methods. The number of unknowns in the vector \\mathbf{d} \\mathbf{d} is slightly less than n_{\\rm dim} n_{\\rm dim} times the number of nodal points once the prescribed boundary velocities have been included. For a well-resolved problem the stiffness matrix \\mathbf{K} \\mathbf{K} is often too large to be stored in full, even accounting for its sparsity. Some iterative methods can operate with only the element stiffness matrices which can be built and used on the fly. Although this requires significantly more computations, it can make some large problems soluble where otherwise memory limitations would block solution. It is also possible that computation can be more efficient than retrieving carefully-packed matrix coefficents and, in this case, element-by-element methods can be competitive for speed in their own right. Some simple iterative schemes for positive definite systems are described next. Far more that this exist and many are more efficient but this usually depends on the actual application area.","title":"Solving the matrix problem"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-6.html#jacobi-relaxation","text":"One of the simplest iterative methods of residual reduction is the Jacobi iteration. Given an approximate solution, \\mathbf{d}^{(0)} \\mathbf{d}^{(0)} , to \\mathbf{Kd}=\\mathbf{f} \\mathbf{Kd}=\\mathbf{f} , an improved solution, $ \\mathbf{d}^{(1)}$ is found by \\begin{equation} d _ i^{(1)} = \\left( f _ i - \\sum^{n} _ {j=1;j\\not=i} k _ {ij}d_j^{(0)} \\right) / k _ {ii} \\nonumber \\end{equation} \\begin{equation} d _ i^{(1)} = \\left( f _ i - \\sum^{n} _ {j=1;j\\not=i} k _ {ij}d_j^{(0)} \\right) / k _ {ii} \\nonumber \\end{equation} It is clear that each iteration cycle requires only one matrix-vector multiplication. However, the convergence rate for this algorithm is usually poor and a large number of cycles is required.","title":"Jacobi relaxation"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-6.html#gauss-seidel-relaxation","text":"A trivial modification to the Jacobi iteration is to use updated information on the \\mathbf{d} \\mathbf{d} componenents immediately they become available. This results in the Gauss-Seidel iteration which is considerably more efficient than Jacobi. It can, however, be harder to code efficiently because the independence of each degree of freedom during one iteration disappears.","title":"Gauss-Seidel relaxation"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-6.html#conjugate-gradients","text":"A more sophisticated iterative procedure for obtaining solutions to linear systems is the conjugate gradient method. This is a development of the method of steepest descent with better convergence properties. The 'search directions' along which the solution is improved are denoted by \\mathbf{s} \\mathbf{s} and do not coincide with the residual vectors. In the following algorithm an inner product is denoted by (\\cdot,\\cdot) (\\cdot,\\cdot) . k=0; \\mathbf{u}_0 = {\\bf 0}; \\mathbf{r}_0 = \\mathbf{f} k=0; \\mathbf{u}_0 = {\\bf 0}; \\mathbf{r}_0 = \\mathbf{f} while (\\mathbf{r}_k \\not= {\\bf 0}) (\\mathbf{r}_k \\not= {\\bf 0}) \u2003 k = k + 1 k = k + 1 \u2003 if (k=1) (k=1) \u2003 \u2003 \\mathbf{s}_1 = \\mathbf{r}_0 \\mathbf{s}_1 = \\mathbf{r}_0 \u2003 else \u2003 \u2003 \\beta = (\\mathbf{r} _ {k-1},\\mathbf{r} _ {k-1})/(\\mathbf{r} _ {k-2},\\mathbf{r} _ {k-2}) \\beta = (\\mathbf{r} _ {k-1},\\mathbf{r} _ {k-1})/(\\mathbf{r} _ {k-2},\\mathbf{r} _ {k-2}) \u2003 \u2003 \\mathbf{s} _ k = \\mathbf{r} _ {k-1} + \\beta \\mathbf{s} _ {k-1} \\mathbf{s} _ k = \\mathbf{r} _ {k-1} + \\beta \\mathbf{s} _ {k-1} \u2003 end \u2003 \\alpha = (\\mathbf{r} _ {k},\\mathbf{r} _ {k})/(\\mathbf{s} _ {k},{\\bf A s} _ {k}) \\alpha = (\\mathbf{r} _ {k},\\mathbf{r} _ {k})/(\\mathbf{s} _ {k},{\\bf A s} _ {k}) \u2003 \\mathbf{u} _ k = \\mathbf{u} _ {k-1} + \\alpha \\mathbf{s} _ {k} \\mathbf{u} _ k = \\mathbf{u} _ {k-1} + \\alpha \\mathbf{s} _ {k} \u2003 \\mathbf{r} _ k = \\mathbf{r} _ {k-1} - \\alpha {\\bf As} _ {k} \\mathbf{r} _ k = \\mathbf{r} _ {k-1} - \\alpha {\\bf As} _ {k} end \\mathbf{u} = \\mathbf{u} _ k \\mathbf{u} = \\mathbf{u} _ k In this case the use of residual vectors from previous iteration steps is necessary to ensure that there is only one matrix-vector multiplication. The convergence of the conjugate gradient algorithm is fastest when \\mathbf{K} \\mathbf{K} is close to the identity matrix. For a general matrix the reduction of the residual may be slow through the first few iterations then picks up speed as the procedure progresses. This property makes this iterative loop inefficient for relatively small numbers of iterations. Note that the reduction of the residual is a measure of \"improvement\" of the solution which is internal to the iteration. Although the magnitude of the residual is usually reduced monotonically, the true error, when it can be calculated, may increase as well as decrease. In order to improve the rate of convergence it is necessary to reformulate the problem such that the solution to a nearly diagonal system is sought. This procedure is known as preconditioning. The system \\mathbf{Kd} = \\mathbf{f} \\mathbf{Kd} = \\mathbf{f} is transformed to a case which can be solved more rapidly by the conjugate gradient algorithm: \\begin{equation} \\tilde{\\mathbf{K}}\\tilde{\\mathbf{u}} = \\tilde{\\mathbf{f}} \\nonumber \\end{equation} \\begin{equation} \\tilde{\\mathbf{K}}\\tilde{\\mathbf{u}} = \\tilde{\\mathbf{f}} \\nonumber \\end{equation} where \\begin{align*} \\tilde{\\mathbf{K}} &= {\\bf C}^{-1}\\mathbf{K} {\\bf C}^{-1} \\\\ \\tilde{\\mathbf{u}} &= {\\bf Cu} \\\\ \\tilde{\\mathbf{f}} &= {\\bf C}^{-1} {\\bf b} \\end{align*} The preconditioning matrix \\bf M \\bf M is defined by {\\bf M} = {\\bf C}^2 {\\bf M} = {\\bf C}^2 , and the preconditioned conjugate gradient algorithm is then: $ k = 0; \\mathbf{u}_0 = {\\bf 0}; \\mathbf{r}_0 = \\mathbf{f}$ while (\\mathbf{r}_k \\not= {\\bf 0}) (\\mathbf{r}_k \\not= {\\bf 0}) \u2003 solve {\\bf M z}_k = \\mathbf{r}_k {\\bf M z}_k = \\mathbf{r}_k \u2003 $ k = k + 1 $ \u2003 if (k=1) (k=1) \u2003 \u2003 $ \\mathbf{s}_1 = {\\bf z}_0$ \u2003 else \u2003 \u2003 \\beta = (\\mathbf{r} _ {k-1},{\\bf z} _ {k-1})/(\\mathbf{r} _ {k-2},{\\bf z} _ {k-2}) \\beta = (\\mathbf{r} _ {k-1},{\\bf z} _ {k-1})/(\\mathbf{r} _ {k-2},{\\bf z} _ {k-2}) \u2003 \u2003 \\mathbf{s} _ k = {\\bf z} _ {k-1} + \\beta \\mathbf{s} _ {k-1} \\mathbf{s} _ k = {\\bf z} _ {k-1} + \\beta \\mathbf{s} _ {k-1} \u2003 end \u2003 \\alpha = (\\mathbf{r} _ {k},{\\bf z} _ {k})/(\\mathbf{s} _ {k},{\\bf A s} _ {k}) \\alpha = (\\mathbf{r} _ {k},{\\bf z} _ {k})/(\\mathbf{s} _ {k},{\\bf A s} _ {k}) \u2003 \\mathbf{d} _ k = \\mathbf{d} _ {k-1} + \\alpha \\mathbf{s} _ {k} \\mathbf{d} _ k = \\mathbf{d} _ {k-1} + \\alpha \\mathbf{s} _ {k} \u2003 \\mathbf{r} _ k = \\mathbf{r} _ {k-1} - \\alpha \\mathbf{Ks} _ {k} \\mathbf{r} _ k = \\mathbf{r} _ {k-1} - \\alpha \\mathbf{Ks} _ {k} end \\mathbf{d} = \\mathbf{d}_k \\mathbf{d} = \\mathbf{d}_k The choice of \\bf M \\bf M has a dramatic effect on the rate of convergence of the method. Not only must the preconditioner improve the convergence properties of the system, but the solution to {\\bf Mz} = \\mathbf{r} {\\bf Mz} = \\mathbf{r} must be inexpensive. Clearly if {\\bf M} = \\mathbf{K} {\\bf M} = \\mathbf{K} then exact convergence is obtained in one iteration but the solution of the preconditioning step becomes impossible. The simplest preconditioner, and the one employed here, is to set {\\bf M} = \\mathrm{diag}(\\mathbf{K}) {\\bf M} = \\mathrm{diag}(\\mathbf{K}) . This is not only simple to calculate (and invert) but requires very little storage. For simple systems the reduction of the residual is noticeably improved during the initial iterations. However, it is not clear how well this simple preconditioner will work when strong viscosity contrasts are present and off-diagonal terms in \\mathbf{K} \\mathbf{K} may be large.","title":"Conjugate Gradients"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-6.html#multigrid","text":"{. width=\"50%\" id=\"multigrid-diagram\"} Multigrid: (a) Error reduction during simple two level V cycle, (b) Shape functions on different grid scales, (c) V cycle on four grids, (d) W cycle on four grids, (e) Schematic of Full Multigrid V cycles The multigrid method works by formulating the finite element problem on a number of different scales - usually a set of grids which are nested one within the other sharing common nodes. The solution progresses on all of the grids at the same time with each grid eliminating errors at a different scale. The effect is to propagate information very rapidly between different nodes in the grid which would otherwise be prevented by the local support of the element shape functions. In fact, by a single traverse from fine to coarse grid and back, all nodes in the mesh can be directly connected to every other - allowing nodes which are physically coupled but remote in the mesh to communicate directly during each iteration cycle. The multigrid effect relies upon using an iterative solver on each of the grid resolutions which acts like a smoother on the residual error at the characteristic scale of that particular grid. People commonly use Gauss-Seidel iteration because it has exactly this property. On the coarsest grid it is possible to use a direct solver because the number of elements is usually very small. For an elliptic operator such as the Laplacian of the Stokes' problem encountered in the algorithm above the discretized problem is now written \\begin{equation*} {\\bf K}_h {\\bf d}_h = {\\bf f}_h \\end{equation*} where the h h subscript indicates that the problem has been discretized to a mesh of fineness h h . As before an initial estimate of the velocity can be improved by determining the solution to \\begin{equation*} {\\bf K}_h \\delta {\\bf d}_h = {\\bf r}_h \\end{equation*} where {\\bf r}_h {\\bf r}_h is the residual on this mesh, and \\delta {\\bf d} \\delta {\\bf d} is a correction to \\bf d \\bf d which reduces \\bf r \\bf r . In the iterative methods described above, the initial approximation and the correction are found by solving a simplified version of the problem at the same gridpoints in such a way that computation time is reduced dramatically. However, another approach to the problem is to obtain an approximate solution by solving the problem on a more coarse grid. The reduction of the number of degrees of freedom also leads to a more manageable problem which can be solved fast. The correction term is therefore: \\begin{equation*} {\\bf K}_H \\delta{\\bf d}_H = {\\bf r}_H \\end{equation*} where H H indicates a coarser level of discretization. The residual on the coarser mesh is determined by the use of a projection (restriction) operator: \\begin{equation*} {\\bf r}_H = {\\bf P}_H^h {\\bf r}_h \\end{equation*} and the approximate solution is then interpolated from the coarse to fine grid using an interpolation operator: \\begin{equation*} \\delta{\\bf d}_h = {\\bf I}_H^h \\delta{\\bf d}_H \\end{equation*} The power of the algorithm is in a recursive application. The coarse grid correction is also calculated through the use of a still-coarser grid and so on, until the problem is so small that an exact solution can be obtained very rapidly. One very simple, but instructive, algorithm for hierarchical residual reduction is the sawtooth cycle (the same logical layout as the multigrid V cycle ). 1. Obtain approximate solution, ${ \\ bf d } _h$ at highest level $h $ 2. Calculate residual: ${ \\ bf r } _h = {\\bf f}_h - {\\bf K}_h {\\bf d}_h$ 3. Project residual by N levels to level $h -N$ 4. &amp; emsp; ${ \\ bf r } _ {h-i} = {\\bf P} _ {h-i}^{h-i+1} {\\bf r} _ {h-i+1}$ 5. Solve exactly: $\\delta {\\bf u} _ {h-N} = {\\bf A} _ {h-N} {\\bf r} _ {h-N}$ 6. Interpolation steps: 7. &amp; emsp; ${ \\ bf r } _ {h-i+1} += {\\bf I} _ {h-i}^{h-i+1} {\\bf K} _ {h-i} \\delta {\\bf d} _ {h-i}$ 8. &amp; emsp; Improve $\\delta {\\bf d} _ {h-i+1}$ 9. ${ \\ bf d } _ h += \\delta {\\bf d} _ h$ The step in which the velocity correction is \"improved\" is an iterative method for reducing the residual at the current level such as those described above. Although many methods of residual reduction are available, the class of methods which often work best with the multigrid approach are relaxation iterations which are also effective smoothing operators. At each level the smoothing operators reduce the residual most strongly on the scale of the discretization - the hierarchical nesting of different mesh sizes allows the residual to be reduced at each scale very efficiently. (see Parsons and Hall). The Jacobi relaxation above is a suitable algorithm for multigrid enhancement but still converges too slowly to build into an efficient code. Preconditioned conjugate gradient methods can better reduce the residual for the same number of operations but may not possess the smoothing properties which benefit the multigrid approach. The local inverse method appears to have both the smoothing and rapid convergence properties for the Stokes' problem which are required for effective multigridding. The projection and interpolation operators have to be chosen fairly carefully to avoid poor approximations to the problem at the coarse levels and ineffectual corrections propogated to the fine levels. The interpolation operator is defined naturally from the shape functions at the coarse levels. The projection operator is then defined to complement this choice (the operators should be adjoint). The sawtooth cycle given in this section is the simplest multigrid algorithm. Developments include improving the residual at each level of the projection , known as a v-cycle , and cycles in which the residual is interpolated only part way through the hierarchy before being reprojected and subjected to another set of improvements (a w-cycle ). The Full Multigrid Algorithm (see Brandt) introduces a further level of complexity. Instead of simply casting the problem at a single level and projecting/improving the residual on a number of grids, the whole problem is defined for all the grids. In this way the initial fine-grid approximation is obtained by interpolating from the solution to the coarsest grid problem. The solution at each level is still obtained by projecting to the finest level and reducing the residual at each projection step. The result is some sort of \"Loch Ness Monster\" cycle. One of the major problems in multigrid is knowing how best to represent material properties at different levels of grid coarseness in order to obtain the optimal convergence rate. This has a simple solution in the particle in cell finite element methods which we will discuss later. \\begin{equation*} {\\bf Gd} + {\\bf G K}^{-1}{\\bf G}^T {\\bf p} = {\\bf G K}^{-1}{\\bf f} \\end{equation*} \\begin{equation*} {\\bf Gd} + {\\bf G K}^{-1}{\\bf G}^T {\\bf p} = {\\bf G K}^{-1}{\\bf f} \\end{equation*} where the first term is zero due to the incompressibility condition. Then the equation system is in the form \\hat{\\bf K}{\\bf p} = \\hat{\\bf f} \\hat{\\bf K}{\\bf p} = \\hat{\\bf f} , where \\hat{\\bf K} \\hat{\\bf K} is now positive definite, and so a conjugate gradient method can be used: 1. $k = 0; {\\bf p}_0 = {\\bf 0}; {\\bf r}_0 = \\hat{\\bf f}$ 2. while $({\\bf r}_k \\not= {\\bf 0})$ 3. &amp; emsp; $ k = k + 1 $ 4. &amp; emsp; if $(k=1)$ 5. &amp; emsp; &amp; emsp; ${ \\ bf s } _1 = {\\bf r}_0$ 6. &amp; emsp; else 7. &amp; emsp; &amp; emsp; $\\beta = ({\\bf r} _ {k-1},{\\bf r} _ {k-1})/({\\bf r} _ {k-2},{\\bf r} _ {k-2})$ 8. &amp; emsp; &amp; emsp; ${ \\ bf s } _ k = {\\bf r} _ {k-1} + \\beta {\\bf s} _ {k-1}$ 9. &amp; emsp; end 10. &amp; emsp; ${ \\ bf w } = \\hat{\\bf K} {\\bf s}$ 11. &amp; emsp; $\\alpha = ({\\bf r} _ {k},{\\bf r} _ {k})/({\\bf s}_{k},{\\bf w})$ 12. &amp; emsp; ${ \\ bf p } _ k = {\\bf p} _ {k-1} + \\alpha {\\bf s} _ {k}$ 13. &amp; emsp; ${ \\ bf r } _ k = {\\bf r} _ {k-1} - \\alpha {\\bf w}$ 14. end 15. ${ \\ bf d } = {\\bf d}_k$ However, it is not possible to obtain \\hat{\\bf K} \\hat{\\bf K} or \\hat{\\bf f} \\hat{\\bf f} directly because the form of {\\bf K}^{-1} {\\bf K}^{-1} is not known - the reason for the use of an iterative method in the first place. In order to obtain the initial residual, therefore, perform the following operation, k = 0 k = 0 ; {\\bf p}_0 = {\\bf 0} {\\bf p}_0 = {\\bf 0} ; \\cancel{ {\\bf r}_0 = \\hat{\\bf f} } \\cancel{ {\\bf r}_0 = \\hat{\\bf f} } (1.a) \u2003 {\\bf r}_0 = {\\bf G K}^{-1} {\\bf f} {\\bf r}_0 = {\\bf G K}^{-1} {\\bf f} : (1.b) \u2003 solve: {\\bf K d} = {\\bf f} {\\bf K d} = {\\bf f} for \\bf d \\bf d (1.c) \u2003 set: {\\bf r}_0 = {\\bf G d} = {\\bf G K}^{-1} {\\bf f} {\\bf r}_0 = {\\bf G d} = {\\bf G K}^{-1} {\\bf f} and to find the \\bf w \\bf w vector: $ \\cancel{ {\\bf w} = \\hat{\\bf K} {\\bf s} }$ (10.a) \u2003 {\\bf w} = \\hat{\\bf G K}^{-1}{\\bf G}^T {\\bf s} {\\bf w} = \\hat{\\bf G K}^{-1}{\\bf G}^T {\\bf s} : (10.b) \u2003 solve {\\bf K d} = {\\bf G}^T {\\bf s} {\\bf K d} = {\\bf G}^T {\\bf s} for \\bf d \\bf d (10.c) \u2003 set {\\bf w} = {\\bf K d} = {\\bf G K}^{-1}{\\bf G}^T {\\bf s} {\\bf w} = {\\bf K d} = {\\bf G K}^{-1}{\\bf G}^T {\\bf s}","title":"Multigrid"},{"location":"Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-6.html#time","text":"We have concentrated on the solution to equilibrium equations without really touching on the way time is evolved in elasticity and slow flow where the equations themselves are independent of time. In the mantle dynamics problem, the energy equation contains time derivatives which the equation of motion lacks so it is unsurprising that these equations require entirely different computational methods. We almost always want to know the time-history of our simulations, so we generally solve this equation explicitly or semi-implicitly in time. Variational methods are not generally applicable because the energy equation with its strong advection term does not follow an extremum of some functional. In this case even the weak forms of the equation are quite similar to finite difference equations.","title":"Time"}]}