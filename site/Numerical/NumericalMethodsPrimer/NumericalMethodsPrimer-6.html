<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Louis Moresi">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>NumericalMethodsPrimer 6 - Underworld - A Geodynamics Modelling Code</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "NumericalMethodsPrimer 6";
    var mkdocs_page_input_path = "Numerical/NumericalMethodsPrimer/NumericalMethodsPrimer-6.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../../index.html" class="icon icon-home"> Underworld - A Geodynamics Modelling Code</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../index.html">Home</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Underworld</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../index.html">Introduction</a>
                    </li>
                    <li class="toctree-l1"><a class="" href="../../StartHere.md">Getting Started</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Numerical Modelling Primer</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../Introduction.html">Introduction</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Geodynamics</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../Geodynamics/Introduction.html">Introduction</a>
                    </li>
                    <li class="toctree-l1"><a class="" href="../../Geodynamics/Mathematics.md">Mathematical Background</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">Underworld - A Geodynamics Modelling Code</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
    
      
    
    <li>NumericalMethodsPrimer 6</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>\[
\require{color}
\newcommand{\dGamma}{\mathbf{d}\boldsymbol{\Gamma}}
\newcommand{\erfc}{\mbox{\rm erfc}}
\newcommand{\curly}{\sf }
\newcommand{\Red}[1]{\textcolor[rgb]{0.7,0.0,0.0}{#1}}
\newcommand{\Green}[1]{\textcolor[rgb]{0.0,0.7,0.0}{ #1}}
\newcommand{\Blue}[1]{\textcolor[rgb]{0.0,0.0,0.7}{ #1}}
\newcommand{\Emerald}[1]{\textcolor[rgb]{0.0,0.7,0.3}{ #1}}
\newcommand{\curly}
\]</p>
<h2 id="solving-the-matrix-problem">Solving the matrix problem</h2>
<p>Once the problem has been rendered into a matrix form, the finite element part is over !  The rest is matrix algebra. There are a number of standard techniques for solving such systems which we briefly outline in the context of the Earth dynamics problem. In most dynamic systems, the FEM can be formulated in a time-explicit manner (cf discrete element methods) that is robust and simple to implement, or using implicit methods which are more elaborate and are often more temperamental but which are capable of covering much larger time increments at each step. In our case, however, the fact that inertia is negligible, leaves the equation of motion independent of time and it can only be solved implicitly.</p>
<p>The original implicit solver in FEM was to build the matrix equation and solve it directly using an method such as Crout elimination. This can be done relatively efficiently by exploiting the fact that the finite element matrices are quite tightly banded.  Unfortunately, direct solution methods are limited to "small" problems since the solution-time scales very rapidly with the number of unknowns (in the worst case, as <span><span class="MathJax_Preview">N^3</span><script type="math/tex">N^3</script></span> where <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> is the number of unknowns, and even in the best case as <span><span class="MathJax_Preview">N^2</span><script type="math/tex">N^2</script></span>).</p>
<p>Iterative methods can achieve much better performance than this once the problems start to become larger. For example, preconditioned conjugate gradient methods can obtain a solution to a given accuracy in a time proportional to <span><span class="MathJax_Preview">N\log N</span><script type="math/tex">N\log N</script></span>. However, preconditioning can be highly time consuming and may need careful tailoring for different problems as the iterations may not converge (compare this to our need to carefully choose how to discretise problems in our finite difference examples earlier)</p>
<p>The optimal method, in theory, for the Stokes problem is <em>multigrid</em> which, when properly formulated, can find a solution in a time proportional to <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>.</p>
<p>An additional problem arises with direct methods. The number of unknowns in the vector <span><span class="MathJax_Preview">\mathbf{d}</span><script type="math/tex">\mathbf{d}</script></span> is slightly less than <span><span class="MathJax_Preview">n_{\rm dim}</span><script type="math/tex">n_{\rm dim}</script></span> times the number of nodal points once the prescribed boundary velocities have been included. For a well-resolved problem the stiffness matrix <span><span class="MathJax_Preview">\mathbf{K}</span><script type="math/tex">\mathbf{K}</script></span> is often too large to be stored in full, even accounting for its sparsity. Some iterative methods can operate with only the element stiffness matrices which can be built and used on the fly. Although this requires significantly more computations, it can make some large problems soluble where otherwise memory limitations would block solution. It is also possible that computation can be more efficient than retrieving carefully-packed matrix coefficents and, in this case, element-by-element methods can be competitive for speed in their own right.  </p>
<p>Some simple iterative schemes  <strong>for positive definite systems</strong> are described next. Far more that this exist and many are more efficient but this usually depends on the actual application area.</p>
<h3 id="jacobi-relaxation">Jacobi relaxation</h3>
<p>One of the simplest iterative methods of residual reduction is the Jacobi iteration. Given an approximate solution, <span><span class="MathJax_Preview">\mathbf{d}^{(0)}</span><script type="math/tex">\mathbf{d}^{(0)}</script></span>, to <span><span class="MathJax_Preview">\mathbf{Kd}=\mathbf{f}</span><script type="math/tex">\mathbf{Kd}=\mathbf{f}</script></span>, an improved solution, $ \mathbf{d}^{(1)}$ is found by</p>
<div>
<div class="MathJax_Preview">\begin{equation}
  d _ i^{(1)} = \left( f _ i - \sum^{n} _ {j=1;j\not=i} k _ {ij}d_j^{(0)} \right) / k _ {ii} \nonumber
\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}
  d _ i^{(1)} = \left( f _ i - \sum^{n} _ {j=1;j\not=i} k _ {ij}d_j^{(0)} \right) / k _ {ii} \nonumber
\end{equation}</script>
</div>
<p>It is clear that each iteration cycle requires only one matrix-vector multiplication. However, the convergence rate for this algorithm is usually poor and a large number of cycles is required.</p>
<h3 id="gauss-seidel-relaxation">Gauss-Seidel relaxation</h3>
<p>A trivial modification to the Jacobi iteration is to use updated information on the <span><span class="MathJax_Preview">\mathbf{d}</span><script type="math/tex">\mathbf{d}</script></span> componenents immediately they become available. This results in the Gauss-Seidel iteration which is considerably more efficient than Jacobi. It can, however, be harder to code efficiently because the independence of each degree of freedom during one iteration disappears.</p>
<h3 id="conjugate-gradients">Conjugate Gradients</h3>
<p>A more sophisticated iterative procedure for obtaining solutions to linear systems is the conjugate gradient method. This is a development of the method of steepest descent with better convergence properties. The 'search directions' along which the solution is improved are denoted by <span><span class="MathJax_Preview">\mathbf{s}</span><script type="math/tex">\mathbf{s}</script></span> and do not coincide with the residual vectors. In the following algorithm an inner product is denoted by <span><span class="MathJax_Preview">(\cdot,\cdot)</span><script type="math/tex">(\cdot,\cdot)</script></span>.</p>
<!-- Not ideal - especially with the spaces etc, but I don't know a latex / pseudo code mode that works -->

<ol>
<li><span><span class="MathJax_Preview">k=0; \mathbf{u}_0 = {\bf 0}; \mathbf{r}_0 = \mathbf{f}</span><script type="math/tex">k=0; \mathbf{u}_0 = {\bf 0}; \mathbf{r}_0 = \mathbf{f}</script></span>  </li>
<li>while <span><span class="MathJax_Preview">(\mathbf{r}_k \not= {\bf 0})</span><script type="math/tex">(\mathbf{r}_k \not= {\bf 0})</script></span></li>
<li>&emsp;  <span><span class="MathJax_Preview">k = k + 1</span><script type="math/tex">k = k + 1</script></span></li>
<li>&emsp;  if <span><span class="MathJax_Preview">(k=1)</span><script type="math/tex">(k=1)</script></span>  </li>
<li>&emsp;  &emsp; <span><span class="MathJax_Preview">\mathbf{s}_1 = \mathbf{r}_0</span><script type="math/tex">\mathbf{s}_1 = \mathbf{r}_0</script></span></li>
<li>&emsp;  else  </li>
<li>&emsp;  &emsp;   <span><span class="MathJax_Preview">\beta = (\mathbf{r} _ {k-1},\mathbf{r} _ {k-1})/(\mathbf{r} _ {k-2},\mathbf{r} _ {k-2})</span><script type="math/tex">\beta = (\mathbf{r} _ {k-1},\mathbf{r} _ {k-1})/(\mathbf{r} _ {k-2},\mathbf{r} _ {k-2})</script></span></li>
<li>&emsp;  &emsp;   <span><span class="MathJax_Preview">\mathbf{s} _ k = \mathbf{r} _ {k-1} + \beta \mathbf{s} _ {k-1}</span><script type="math/tex">\mathbf{s} _ k = \mathbf{r} _ {k-1} + \beta \mathbf{s} _ {k-1}</script></span></li>
<li>&emsp;  end</li>
<li>&emsp;  <span><span class="MathJax_Preview">\alpha = (\mathbf{r} _ {k},\mathbf{r} _ {k})/(\mathbf{s} _ {k},{\bf A s} _ {k})</span><script type="math/tex">\alpha = (\mathbf{r} _ {k},\mathbf{r} _ {k})/(\mathbf{s} _ {k},{\bf A s} _ {k})</script></span></li>
<li>&emsp;  <span><span class="MathJax_Preview">\mathbf{u} _ k = \mathbf{u} _ {k-1} + \alpha \mathbf{s} _ {k}</span><script type="math/tex">\mathbf{u} _ k = \mathbf{u} _ {k-1} + \alpha \mathbf{s} _ {k}</script></span></li>
<li>&emsp;  <span><span class="MathJax_Preview">\mathbf{r} _ k = \mathbf{r} _ {k-1} - \alpha {\bf As} _ {k}</span><script type="math/tex">\mathbf{r} _ k = \mathbf{r} _ {k-1} - \alpha {\bf As} _ {k}</script></span></li>
<li>end</li>
<li><span><span class="MathJax_Preview">\mathbf{u} = \mathbf{u} _ k</span><script type="math/tex">\mathbf{u} = \mathbf{u} _ k</script></span></li>
</ol>
<p>In this case the use of residual vectors from previous iteration steps is necessary to ensure that there is only one matrix-vector multiplication. The convergence of the conjugate gradient algorithm is fastest when <span><span class="MathJax_Preview">\mathbf{K}</span><script type="math/tex">\mathbf{K}</script></span> is close to the identity matrix. For a general matrix the reduction of the residual may be  slow through the first few iterations then picks up speed as the procedure progresses. This property makes this iterative loop inefficient for relatively small numbers of iterations.</p>
<p>Note that the reduction of the residual is a measure of "improvement" of the solution which is internal to the iteration. Although the magnitude of the residual is usually reduced monotonically, the true error, when it can be calculated, may increase as well as decrease. In order to improve the rate of convergence it is necessary to reformulate the problem such that the solution to a <em>nearly diagonal</em> system is sought. This procedure is known as preconditioning. The system <span><span class="MathJax_Preview">\mathbf{Kd} = \mathbf{f}</span><script type="math/tex">\mathbf{Kd} = \mathbf{f}</script></span> is transformed to a case which can be solved more rapidly by the conjugate gradient algorithm:</p>
<div>
<div class="MathJax_Preview">\begin{equation}
    \tilde{\mathbf{K}}\tilde{\mathbf{u}} = \tilde{\mathbf{f}} \nonumber
  \end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}
    \tilde{\mathbf{K}}\tilde{\mathbf{u}} = \tilde{\mathbf{f}} \nonumber
  \end{equation}</script>
</div>
<p>where</p>
<p>\begin{align*}
      \tilde{\mathbf{K}} &amp;= {\bf C}^{-1}\mathbf{K} {\bf C}^{-1} \\
      \tilde{\mathbf{u}} &amp;= {\bf Cu}   \\
      \tilde{\mathbf{f}} &amp;= {\bf C}^{-1} {\bf b}
\end{align*}</p>
<p>The preconditioning matrix <span><span class="MathJax_Preview">\bf M</span><script type="math/tex">\bf M</script></span> is defined by <span><span class="MathJax_Preview">{\bf M} = {\bf C}^2</span><script type="math/tex">{\bf M} = {\bf C}^2</script></span>, and  the preconditioned
conjugate gradient algorithm is then:</p>
<ol>
<li>$ k = 0; \mathbf{u}_0 = {\bf 0}; \mathbf{r}_0 = \mathbf{f}$  </li>
<li>while <span><span class="MathJax_Preview">(\mathbf{r}_k \not= {\bf 0})</span><script type="math/tex">(\mathbf{r}_k \not= {\bf 0})</script></span>  </li>
<li>&emsp;  <em>solve</em> <span><span class="MathJax_Preview">{\bf M z}_k = \mathbf{r}_k</span><script type="math/tex">{\bf M z}_k = \mathbf{r}_k</script></span></li>
<li>&emsp;  $ k = k + 1 $</li>
<li>&emsp;  if <span><span class="MathJax_Preview">(k=1)</span><script type="math/tex">(k=1)</script></span>  </li>
<li>&emsp;  &emsp; $ \mathbf{s}_1 = {\bf z}_0$</li>
<li>&emsp;  else</li>
<li>&emsp;  &emsp;  <span><span class="MathJax_Preview">\beta = (\mathbf{r} _ {k-1},{\bf z} _ {k-1})/(\mathbf{r} _ {k-2},{\bf z} _ {k-2})</span><script type="math/tex">\beta = (\mathbf{r} _ {k-1},{\bf z} _ {k-1})/(\mathbf{r} _ {k-2},{\bf z} _ {k-2})</script></span></li>
<li>&emsp;  &emsp;  <span><span class="MathJax_Preview">\mathbf{s} _ k = {\bf z} _ {k-1} + \beta \mathbf{s} _ {k-1}</span><script type="math/tex">\mathbf{s} _ k = {\bf z} _ {k-1} + \beta \mathbf{s} _ {k-1}</script></span></li>
<li>&emsp;  end</li>
<li>&emsp;  <span><span class="MathJax_Preview">\alpha = (\mathbf{r} _ {k},{\bf z} _ {k})/(\mathbf{s} _ {k},{\bf A s} _ {k})</span><script type="math/tex">\alpha = (\mathbf{r} _ {k},{\bf z} _ {k})/(\mathbf{s} _ {k},{\bf A s} _ {k})</script></span></li>
<li>&emsp;  <span><span class="MathJax_Preview">\mathbf{d} _ k = \mathbf{d} _ {k-1} + \alpha \mathbf{s} _ {k}</span><script type="math/tex">\mathbf{d} _ k = \mathbf{d} _ {k-1} + \alpha \mathbf{s} _ {k}</script></span></li>
<li>&emsp;  <span><span class="MathJax_Preview">\mathbf{r} _ k = \mathbf{r} _ {k-1} - \alpha \mathbf{Ks} _ {k}</span><script type="math/tex">\mathbf{r} _ k = \mathbf{r} _ {k-1} - \alpha \mathbf{Ks} _ {k}</script></span></li>
<li>end</li>
<li><span><span class="MathJax_Preview">\mathbf{d} = \mathbf{d}_k</span><script type="math/tex">\mathbf{d} = \mathbf{d}_k</script></span></li>
</ol>
<p>The choice of <span><span class="MathJax_Preview">\bf M</span><script type="math/tex">\bf M</script></span> has a dramatic effect on the rate of convergence of the method. Not only must the preconditioner improve the convergence properties of the system, but the solution to <span><span class="MathJax_Preview">{\bf Mz} = \mathbf{r}</span><script type="math/tex">{\bf Mz} = \mathbf{r}</script></span> must be inexpensive. Clearly if <span><span class="MathJax_Preview">{\bf M} = \mathbf{K}</span><script type="math/tex">{\bf M} = \mathbf{K}</script></span> then exact convergence is obtained in one iteration but the solution of the preconditioning step becomes impossible. The simplest preconditioner, and the one employed here, is to set <span><span class="MathJax_Preview">{\bf M} = \mathrm{diag}(\mathbf{K})</span><script type="math/tex">{\bf M} = \mathrm{diag}(\mathbf{K})</script></span>. This is not only simple to calculate (and invert) but requires very little storage. For simple systems the reduction of the residual is noticeably improved during the initial iterations. However, it is not clear how well this simple preconditioner will work when strong viscosity contrasts are present and off-diagonal terms in <span><span class="MathJax_Preview">\mathbf{K}</span><script type="math/tex">\mathbf{K}</script></span> may be large.</p>
<h3 id="multigrid">Multigrid</h3>
<hr />
<p><img alt="" src="../Diagrams/mg.png" />{. width="50%" id="multigrid-diagram"}</p>
<p><em>Multigrid: (a) Error reduction during simple two level V cycle, (b) Shape functions on different grid scales, (c) V cycle on four grids, (d) W cycle on four grids, (e) Schematic of Full Multigrid V cycles</em></p>
<hr />
<p>The multigrid method works by formulating the finite element problem on a number of different scales - usually a set of grids which are nested one within the other sharing common nodes.  The solution progresses on all of the grids at the same time with each grid eliminating errors at a different scale. The effect is to propagate information very rapidly between different nodes in the grid which would otherwise be prevented by the local support of the element shape functions. In fact, by a single traverse from fine to coarse grid and back, all nodes in the mesh can be directly connected to every other - allowing nodes which are physically coupled but remote in the mesh to communicate directly during each iteration cycle.</p>
<p>The multigrid effect relies upon using an iterative solver on each of the grid resolutions which acts like a smoother on the residual error at the characteristic scale of that particular grid. People commonly use Gauss-Seidel iteration because it has exactly this property. On the coarsest grid it is possible to use a direct solver because the number of elements is usually very small.</p>
<p>For an elliptic operator such as the Laplacian of the Stokes' problem encountered in the algorithm above the discretized problem is now written</p>
<p>\begin{equation*}
  {\bf K}_h {\bf d}_h = {\bf f}_h
\end{equation*}</p>
<p>where  the <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> subscript indicates that the problem has been discretized to a mesh of fineness <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>.  As before an initial estimate of the velocity can be improved by determining the solution to</p>
<p>\begin{equation*}
  {\bf K}_h \delta {\bf d}_h = {\bf r}_h
\end{equation*}</p>
<p>where <span><span class="MathJax_Preview">{\bf r}_h</span><script type="math/tex">{\bf r}_h</script></span> is the residual on this mesh, and <span><span class="MathJax_Preview">\delta {\bf d}</span><script type="math/tex">\delta {\bf d}</script></span> is a correction to <span><span class="MathJax_Preview">\bf d</span><script type="math/tex">\bf d</script></span> which reduces <span><span class="MathJax_Preview">\bf r</span><script type="math/tex">\bf r</script></span>. In the iterative methods described above, the initial approximation and the correction are found by solving a simplified version of the problem at the same gridpoints in such a way that computation time is reduced dramatically. However, another approach to the problem is to obtain an approximate solution by solving the problem on a more coarse grid. The reduction of the number of degrees of freedom also leads to a more manageable problem which can be solved fast. The correction term is therefore:</p>
<p>\begin{equation*}
  {\bf K}_H \delta{\bf d}_H = {\bf r}_H
\end{equation*}</p>
<p>where <span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> indicates a coarser level of discretization. The residual on the coarser mesh is determined by the use of a projection (restriction) operator:</p>
<p>\begin{equation*}
  {\bf r}_H = {\bf P}_H^h {\bf r}_h
\end{equation*}</p>
<p>and the approximate solution is then interpolated from the coarse to fine grid using an interpolation operator:</p>
<p>\begin{equation*}
  \delta{\bf d}_h = {\bf I}_H^h \delta{\bf d}_H
\end{equation*}</p>
<p>The power of the algorithm is in a recursive application. The coarse grid correction is also calculated through the use of a still-coarser grid and so on, until the problem is so small that an exact solution can be obtained very rapidly. One very simple, but instructive, algorithm for hierarchical residual reduction is the sawtooth cycle (the same logical layout as the <a href="#multigrid-diagram">multigrid V cycle</a>).</p>
<div class="codehilite"><pre><span></span>  1. Obtain approximate solution, <span class="cp">${</span>\<span class="n">bf</span> <span class="n">d</span><span class="cp">}</span>_h$ at highest level <span class="nv">$h</span>$
  2. Calculate residual: <span class="cp">${</span>\<span class="n">bf</span> <span class="n">r</span><span class="cp">}</span>_h = {\bf f}_h - {\bf K}_h {\bf d}_h$
  3. Project residual by N levels to level <span class="nv">$h</span>-N$
  4. <span class="ni">&amp;amp;</span>emsp;  <span class="cp">${</span>\<span class="n">bf</span> <span class="n">r</span><span class="cp">}</span> _ {h-i} = {\bf  P} _ {h-i}^{h-i+1} {\bf r} _ {h-i+1}$
  5. Solve exactly: $\delta {\bf u} _ {h-N} = {\bf A} _ {h-N} {\bf r} _ {h-N}$
  6. Interpolation steps:
  7. <span class="ni">&amp;amp;</span>emsp;  <span class="cp">${</span>\<span class="n">bf</span> <span class="n">r</span><span class="cp">}</span> _ {h-i+1} += {\bf I} _ {h-i}^{h-i+1} {\bf K} _ {h-i} \delta {\bf d} _ {h-i}$  
  8. <span class="ni">&amp;amp;</span>emsp;  Improve $\delta {\bf d} _ {h-i+1}$  
  9.  <span class="cp">${</span>\<span class="n">bf</span> <span class="n">d</span><span class="cp">}</span> _ h += \delta {\bf d} _ h$
</pre></div>


<p>The step in which the velocity correction is "improved" is an iterative method for reducing the residual at the current level such as those described above. Although many methods of residual reduction are available, the class of methods which often work best with the multigrid approach are relaxation iterations which are also effective <em>smoothing</em> operators. At each level the smoothing operators reduce the residual most strongly on the scale  of the discretization - the hierarchical nesting of different mesh sizes allows the residual to be reduced at each scale very efficiently. (see Parsons and Hall).  The Jacobi relaxation above is a suitable algorithm for multigrid enhancement but still converges too slowly to build into an efficient code. Preconditioned conjugate gradient methods can better reduce the residual for the same number of operations but may not possess the smoothing properties which benefit the multigrid approach. The local inverse method appears to have both the smoothing and rapid convergence properties for the Stokes' problem which are required for effective multigridding.</p>
<p>The projection and interpolation operators have to be chosen fairly carefully to avoid poor approximations to the problem at the coarse levels and ineffectual corrections propogated to the fine levels. The interpolation operator is defined naturally from the shape functions at the coarse levels. The projection operator is then defined to complement this choice (the operators should be adjoint).</p>
<p>The sawtooth cycle given in this section is the simplest multigrid algorithm. Developments include improving the residual at each level of the <em>projection</em>, known as a <em>v-cycle</em>, and cycles in which the residual is interpolated only part way through the hierarchy before being reprojected and subjected to another set of improvements (a <em>w-cycle</em>).</p>
<p>The Full Multigrid Algorithm (see Brandt) introduces a further level of complexity. Instead of simply casting the problem at a single level and projecting/improving the residual on a number of grids, the whole problem is defined for all the grids. In this way the initial fine-grid approximation is obtained by interpolating from the solution to the coarsest grid problem. The solution at each level is still obtained by projecting to the finest level and reducing the residual at each projection step. The result is some sort of "Loch Ness Monster" cycle.</p>
<p>One of the major problems in multigrid is knowing how best to represent material properties at different levels of grid coarseness in order to obtain the optimal convergence rate. This has a simple solution in the particle in cell finite element methods which we will discuss later.</p>
<div>
<div class="MathJax_Preview">\begin{equation*}
{\bf Gd} + {\bf G K}^{-1}{\bf G}^T {\bf p} = {\bf G K}^{-1}{\bf  f}
\end{equation*}</div>
<script type="math/tex; mode=display">\begin{equation*}
{\bf Gd} + {\bf G K}^{-1}{\bf G}^T {\bf p} = {\bf G K}^{-1}{\bf  f}
\end{equation*}</script>
</div>
<p>where the first term is zero due to the incompressibility condition. Then the equation system is in the form <span><span class="MathJax_Preview">\hat{\bf K}{\bf p} = \hat{\bf f}</span><script type="math/tex">\hat{\bf K}{\bf p} = \hat{\bf f}</script></span>, where <span><span class="MathJax_Preview">\hat{\bf K}</span><script type="math/tex">\hat{\bf K}</script></span> is  now positive definite, and so a conjugate gradient method can be used:</p>
<div class="codehilite"><pre><span></span>1.      <span class="nv">$k</span> = 0; {\bf p}_0 = {\bf 0}; {\bf r}_0 = \hat{\bf f}$  
2.      while $({\bf r}_k \not= {\bf 0})$  
3.      <span class="ni">&amp;amp;</span>emsp;  $ k = k + 1 $
4.      <span class="ni">&amp;amp;</span>emsp;  if $(k=1)$  
5.      <span class="ni">&amp;amp;</span>emsp;  <span class="ni">&amp;amp;</span>emsp; <span class="cp">${</span>\<span class="n">bf</span> <span class="n">s</span><span class="cp">}</span>_1 = {\bf r}_0$
6.      <span class="ni">&amp;amp;</span>emsp;  else  
7.      <span class="ni">&amp;amp;</span>emsp;  <span class="ni">&amp;amp;</span>emsp;  $\beta = ({\bf r} _ {k-1},{\bf r} _ {k-1})/({\bf r} _ {k-2},{\bf r} _ {k-2})$
8.      <span class="ni">&amp;amp;</span>emsp;  <span class="ni">&amp;amp;</span>emsp;  <span class="cp">${</span>\<span class="n">bf</span> <span class="n">s</span><span class="cp">}</span> _ k = {\bf r} _ {k-1} + \beta {\bf s} _ {k-1}$
9.      <span class="ni">&amp;amp;</span>emsp;  end
10.     <span class="ni">&amp;amp;</span>emsp;  <span class="cp">${</span>\<span class="n">bf</span> <span class="n">w</span><span class="cp">}</span> = \hat{\bf K} {\bf s}$
11.     <span class="ni">&amp;amp;</span>emsp;  $\alpha = ({\bf r} _ {k},{\bf r} _ {k})/({\bf s}_{k},{\bf w})$
12.     <span class="ni">&amp;amp;</span>emsp;  <span class="cp">${</span>\<span class="n">bf</span> <span class="n">p</span><span class="cp">}</span> _ k = {\bf p} _ {k-1} + \alpha {\bf s} _ {k}$
13.     <span class="ni">&amp;amp;</span>emsp;  <span class="cp">${</span>\<span class="n">bf</span> <span class="n">r</span><span class="cp">}</span> _ k = {\bf r} _ {k-1} - \alpha {\bf w}$
14.     end
15.     <span class="cp">${</span>\<span class="n">bf</span> <span class="n">d</span><span class="cp">}</span> = {\bf d}_k$
</pre></div>


<p>However, it is not possible to obtain <span><span class="MathJax_Preview">\hat{\bf K}</span><script type="math/tex">\hat{\bf K}</script></span> or <span><span class="MathJax_Preview">\hat{\bf f}</span><script type="math/tex">\hat{\bf f}</script></span> directly because the form of <span><span class="MathJax_Preview">{\bf K}^{-1}</span><script type="math/tex">{\bf K}^{-1}</script></span> is not known - the reason for the use of an iterative method in the first place. In order to obtain the initial residual, therefore, perform the following operation,</p>
<ol>
<li>
<p><span><span class="MathJax_Preview">k = 0</span><script type="math/tex">k = 0</script></span>; <span><span class="MathJax_Preview">{\bf p}_0 = {\bf 0}</span><script type="math/tex">{\bf p}_0 = {\bf 0}</script></span> ; <span><span class="MathJax_Preview">\cancel{ {\bf r}_0 = \hat{\bf f} }</span><script type="math/tex">\cancel{ {\bf r}_0 = \hat{\bf f} }</script></span>  </p>
<ul>
<li>(1.a) &emsp;  <span><span class="MathJax_Preview">{\bf r}_0 = {\bf G K}^{-1} {\bf f}</span><script type="math/tex">{\bf r}_0 = {\bf G K}^{-1} {\bf f}</script></span>:</li>
<li>(1.b) &emsp;  solve: <span><span class="MathJax_Preview">{\bf K d} = {\bf f}</span><script type="math/tex">{\bf K d} = {\bf f}</script></span> for <span><span class="MathJax_Preview">\bf d</span><script type="math/tex">\bf d</script></span></li>
<li>(1.c) &emsp;  set: <span><span class="MathJax_Preview">{\bf r}_0 = {\bf G d} = {\bf G K}^{-1} {\bf f}</span><script type="math/tex">{\bf r}_0 = {\bf G d} = {\bf G K}^{-1} {\bf f}</script></span></li>
</ul>
</li>
</ol>
<p>and to find the <span><span class="MathJax_Preview">\bf w</span><script type="math/tex">\bf w</script></span> vector:</p>
<ol>
<li>$ \cancel{ {\bf w} = \hat{\bf K} {\bf s} }$  <ul>
<li>(10.a) &emsp; <span><span class="MathJax_Preview">{\bf w} = \hat{\bf G K}^{-1}{\bf G}^T {\bf s}</span><script type="math/tex">{\bf w} = \hat{\bf G K}^{-1}{\bf G}^T {\bf s}</script></span>:</li>
<li>(10.b) &emsp;  solve <span><span class="MathJax_Preview">{\bf K d} = {\bf G}^T {\bf s}</span><script type="math/tex">{\bf K d} = {\bf G}^T {\bf s}</script></span> for <span><span class="MathJax_Preview">\bf d</span><script type="math/tex">\bf d</script></span></li>
<li>(10.c) &emsp;  set <span><span class="MathJax_Preview">{\bf w} = {\bf K d} = {\bf G K}^{-1}{\bf G}^T {\bf s}</span><script type="math/tex">{\bf w} = {\bf K d} = {\bf G K}^{-1}{\bf G}^T {\bf s}</script></span></li>
</ul>
</li>
</ol>
<h2 id="time">Time</h2>
<p>We have concentrated on the solution to equilibrium equations without really touching on the way time is evolved in elasticity and slow flow where the equations themselves are independent of time. In the mantle dynamics problem, the energy equation contains time derivatives which the equation of motion lacks so it is unsurprising that these equations require entirely different computational methods.</p>
<p>We almost always want to know the time-history of our simulations, so we generally solve this equation explicitly or semi-implicitly in time.</p>
<p>Variational methods are not generally applicable because the energy equation with its strong advection term does not follow an extremum of some functional. In this case even the weak forms of the equation are quite similar to finite difference equations.</p>
<!-- MORE - perhaps take some of the treatise article here and expand it -->

<!-- %## Ideas on a Particle in Cell Finite Element Code}
%   
%   The extension of standard finite element
%   methods to include a basic Lagrangian Particle reference
%   frame.
%   
%   Adapting integration schemes.
%   
%   Problems for highly active flow regimes.
%
%   Discussion: as this is ongoing, research work with
%   potentially exciting commercial applications, it will be discussed but
%   not written out here !!
%   
%### Geological Examples from a PIC / FE method}
%
%   To wind down the course, we will watch some movies
%   of finite elements in action. -->
              
            </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../js/mathjax-local-config.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
      <script src="../../search/main.js" defer></script>
    <script type="text/javascript" defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
